{
  "cwe_type": "Uncontrolled Resource Consumption ('Resource Exhaustion')",
  "cve_id": "CVE-2025-22010",
  "supplementary_code": "```c\nstruct hns_roce_dev {\nstruct ib_device ib_dev;\nstruct pci_dev *pci_dev;\nstruct device *dev;\nstruct hns_roce_uar priv_uar;\nconst char *irq_names[HNS_ROCE_MAX_IRQ_NUM];\nspinlock_t sm_lock;\nbool active;\nbool is_reset;\nbool dis_db;\nunsigned long reset_cnt;\nstruct hns_roce_ib_iboe iboe;\nenum hns_roce_device_state state;\nstruct list_head qp_list; /* list of all qps on this dev */\nspinlock_t qp_list_lock; /* protect qp_list */\nstruct list_head pgdir_list;\nstruct mutex pgdir_mutex;\nint irq[HNS_ROCE_MAX_IRQ_NUM];\nu8 __iomem *reg_base;\nvoid __iomem *mem_base;\nstruct hns_roce_caps caps;\nstruct xarray qp_table_xa;\nunsigned char dev_addr[HNS_ROCE_MAX_PORTS][ETH_ALEN];\nu64 sys_image_guid;\nu32 vendor_id;\nu32 vendor_part_id;\nu32 hw_rev;\nvoid __iomem *priv_addr;\nstruct hns_roce_cmdq cmd;\nstruct hns_roce_ida pd_ida;\nstruct hns_roce_ida xrcd_ida;\nstruct hns_roce_ida uar_ida;\nstruct hns_roce_mr_table mr_table;\nstruct hns_roce_cq_table cq_table;\nstruct hns_roce_srq_table srq_table;\nstruct hns_roce_qp_table qp_table;\nstruct hns_roce_eq_table eq_table;\nstruct hns_roce_hem_table qpc_timer_table;\nstruct hns_roce_hem_table cqc_timer_table;\n/* GMV is the memory area that the driver allocates for the hardware\n* to store SGID, SMAC and VLAN information.\n*/\nstruct hns_roce_hem_table gmv_table;\nint cmd_mod;\nint loop_idc;\nu32 sdb_offset;\nu32 odb_offset;\nconst struct hns_roce_hw *hw;\nvoid *priv;\nstruct workqueue_struct *irq_workq;\nstruct work_struct ecc_work;\nu32 func_num;\nu32 is_vf;\nu32 cong_algo_tmpl_id;\nu64 dwqe_page;\nstruct hns_roce_dev_debugfs dbgfs;\natomic64_t *dfx_cnt;\n};\n```\n```c\nstruct hns_roce_hem_list {\nstruct list_head root_bt;\n/* link all bt dma mem by hop config */\nstruct list_head mid_bt[HNS_ROCE_MAX_BT_REGION][HNS_ROCE_MAX_BT_LEVEL];\nstruct list_head btm_bt; /* link all bottom bt in @mid_bt */\ndma_addr_t root_ba; /* pointer to the root ba table */\n};\n```\n```c\nstruct hns_roce_buf_region {\nu32 offset; /* page offset */\nu32 count; /* page count */\nint hopnum; /* addressing hop num */\n};\n```\n```c\n#define HNS_ROCE_MAX_BT_REGION 3\n```\n```c\n#define dev_err(dev, fmt, ...) \\\ndev_printk_index_wrap(_dev_err, KERN_ERR, dev, dev_fmt(fmt), ##__VA_ARGS__)\n```\n```c\n#define EINVAL 22 /* Invalid argument */\n```\n```c\n#define BA_BYTE_LEN 8\n```\n```c\nstatic int hem_list_alloc_mid_bt(struct hns_roce_dev *hr_dev,\nconst struct hns_roce_buf_region *r, int unit,\nint offset, struct list_head *mid_bt,\nstruct list_head *btm_bt)\n{\nstruct hns_roce_hem_item *hem_ptrs[HNS_ROCE_MAX_BT_LEVEL] = { NULL };\nstruct list_head temp_list[HNS_ROCE_MAX_BT_LEVEL];\nstruct hns_roce_hem_item *cur, *pre;\nconst int hopnum = r->hopnum;\nint start_aligned;\nint distance;\nint ret = 0;\nint max_ofs;\nint level;\nu64 step;\nint end;\nif (hopnum <= 1)\nreturn 0;\nif (hopnum > HNS_ROCE_MAX_BT_LEVEL) {\ndev_err(hr_dev->dev, \"invalid hopnum %d!\\n\", hopnum);\nreturn -EINVAL;\n}\nif (offset < r->offset) {\ndev_err(hr_dev->dev, \"invalid offset %d, min %u!\\n\",\noffset, r->offset);\nreturn -EINVAL;\n}\ndistance = offset - r->offset;\nmax_ofs = r->offset + r->count - 1;\nfor (level = 0; level < hopnum; level++)\nINIT_LIST_HEAD(&temp_list[level]);\n/* config L1 bt to last bt and link them to corresponding parent */\nfor (level = 1; level < hopnum; level++) {\nif (!hem_list_is_bottom_bt(hopnum, level)) {\ncur = hem_list_search_item(&mid_bt[level], offset);\nif (cur) {\nhem_ptrs[level] = cur;\ncontinue;\n}\n}\nstep = hem_list_calc_ba_range(hopnum, level, unit);\nif (step < 1) {\nret = -EINVAL;\ngoto err_exit;\n}\nstart_aligned = (distance / step) * step + r->offset;\nend = min_t(u64, start_aligned + step - 1, max_ofs);\ncur = hem_list_alloc_item(hr_dev, start_aligned, end, unit,\ntrue);\nif (!cur) {\nret = -ENOMEM;\ngoto err_exit;\n}\nhem_ptrs[level] = cur;\nlist_add(&cur->list, &temp_list[level]);\nif (hem_list_is_bottom_bt(hopnum, level))\nlist_add(&cur->sibling, &temp_list[0]);\n/* link bt to parent bt */\nif (level > 1) {\npre = hem_ptrs[level - 1];\nstep = (cur->start - pre->start) / step * BA_BYTE_LEN;\nhem_list_link_bt(pre->addr + step, cur->dma_addr);\n}\n}\nlist_splice(&temp_list[0], btm_bt);\nfor (level = 1; level < hopnum; level++)\nlist_splice(&temp_list[level], &mid_bt[level]);\nreturn 0;\nerr_exit:\nfor (level = 1; level < hopnum; level++)\nhem_list_free_all(hr_dev, &temp_list[level], true);\nreturn ret;\n}\n```\n```c\nstatic int hem_list_alloc_root_bt(struct hns_roce_dev *hr_dev,\nstruct hns_roce_hem_list *hem_list, int unit,\nconst struct hns_roce_buf_region *regions,\nint region_cnt)\n{\nstruct hns_roce_hem_item *root_hem;\nstruct hns_roce_hem_head head;\nint max_ba_num;\nint ret;\nint i;\nroot_hem = hem_list_search_item(&hem_list->root_bt, regions[0].offset);\nif (root_hem)\nreturn 0;\nmax_ba_num = 0;\nroot_hem = alloc_root_hem(hr_dev, unit, &max_ba_num, regions,\nregion_cnt);\nif (IS_ERR(root_hem))\nreturn PTR_ERR(root_hem);\n/* List head for storing all allocated HEM items */\nINIT_LIST_HEAD(&head.root);\nINIT_LIST_HEAD(&head.leaf);\nfor (i = 0; i < region_cnt; i++)\nINIT_LIST_HEAD(&head.branch[i]);\nhem_list->root_ba = root_hem->dma_addr;\nlist_add(&root_hem->list, &head.root);\nret = setup_root_hem(hr_dev, hem_list, unit, max_ba_num, &head, regions,\nregion_cnt);\nif (ret) {\nfor (i = 0; i < region_cnt; i++)\nhem_list_free_all(hr_dev, &head.branch[i], false);\nhem_list_free_all(hr_dev, &head.root, true);\n}\nreturn ret;\n}\n```\n```c\nvoid hns_roce_hem_list_release(struct hns_roce_dev *hr_dev,\nstruct hns_roce_hem_list *hem_list)\n{\nint i, j;\nfor (i = 0; i < HNS_ROCE_MAX_BT_REGION; i++)\nfor (j = 0; j < HNS_ROCE_MAX_BT_LEVEL; j++)\nhem_list_free_all(hr_dev, &hem_list->mid_bt[i][j],\nj != 0);\nhem_list_free_all(hr_dev, &hem_list->root_bt, true);\nINIT_LIST_HEAD(&hem_list->btm_bt);\nhem_list->root_ba = 0;\n}\n```",
  "original_code": "```c\nint hns_roce_hem_list_request(struct hns_roce_dev *hr_dev, struct hns_roce_hem_list *hem_list, const struct hns_roce_buf_region *regions, int region_cnt, unsigned int bt_pg_shift)\n{\nconst struct hns_roce_buf_region *r;\nint ofs, end;\nint unit;\nint ret;\nint i;\nif (region_cnt > HNS_ROCE_MAX_BT_REGION) {\ndev_err(hr_dev->dev, \"invalid region region_cnt %d!\\n\",\nregion_cnt);\nreturn -EINVAL;\n}\nunit = (1 << bt_pg_shift) / BA_BYTE_LEN;\nfor (i = 0; i < region_cnt; i++) {\nr = &regions[i];\nif (!r->count)\ncontinue;\nend = r->offset + r->count;\nfor (ofs = r->offset; ofs < end; ofs += unit) {\nret = hem_list_alloc_mid_bt(hr_dev, r, unit, ofs,\nhem_list->mid_bt[i],\n&hem_list->btm_bt);\nif (ret) {\ndev_err(hr_dev->dev,\n\"alloc hem trunk fail ret = %d!\\n\", ret);\ngoto err_alloc;\n}\n}\n}\nret = hem_list_alloc_root_bt(hr_dev, hem_list, unit, regions,\nregion_cnt);\nif (ret)\ndev_err(hr_dev->dev, \"alloc hem root fail ret = %d!\\n\", ret);\nelse\nreturn 0;\nerr_alloc:\nhns_roce_hem_list_release(hr_dev, hem_list);\nreturn ret;\n}\n```",
  "vuln_patch": "```c\nint hns_roce_hem_list_request(struct hns_roce_dev *hr_dev, struct hns_roce_hem_list *hem_list, const struct hns_roce_buf_region *regions, int region_cnt, unsigned int bt_pg_shift)\n{\nconst struct hns_roce_buf_region *r;\nint ofs, end;\nint loop;\nint unit;\nint ret;\nint i;\nif (region_cnt > HNS_ROCE_MAX_BT_REGION) {\ndev_err(hr_dev->dev, \"invalid region region_cnt %d!\\n\",\nregion_cnt);\nreturn -EINVAL;\n}\nunit = (1 << bt_pg_shift) / BA_BYTE_LEN;\nfor (i = 0; i < region_cnt; i++) {\nr = &regions[i];\nif (!r->count)\ncontinue;\nend = r->offset + r->count;\nfor (ofs = r->offset, loop = 1; ofs < end; ofs += unit, loop++) {\nif (!(loop % RESCHED_LOOP_CNT_THRESHOLD_ON_4K))\ncond_resched();\nret = hem_list_alloc_mid_bt(hr_dev, r, unit, ofs,\nhem_list->mid_bt[i],\n&hem_list->btm_bt);\nif (ret) {\ndev_err(hr_dev->dev,\n\"alloc hem trunk fail ret = %d!\\n\", ret);\ngoto err_alloc;\n}\n}\n}\nret = hem_list_alloc_root_bt(hr_dev, hem_list, unit, regions,\nregion_cnt);\nif (ret)\ndev_err(hr_dev->dev, \"alloc hem root fail ret = %d!\\n\", ret);\nelse\nreturn 0;\nerr_alloc:\nhns_roce_hem_list_release(hr_dev, hem_list);\nreturn ret;\n}\n```",
  "function_name": "hns_roce_hem_list_request",
  "function_prototype": "int hns_roce_hem_list_request(struct hns_roce_dev *hr_dev, struct hns_roce_hem_list *hem_list, const struct hns_roce_buf_region *regions, int region_cnt, unsigned int bt_pg_shift)",
  "code_semantics": "The target code is a function that manages a hierarchical memory structure for a device. It checks if the number of memory regions is within a valid range. It calculates a unit size based on a given shift value. For each memory region, it allocates intermediate memory blocks at specified offsets. If any allocation fails, it logs an error and releases all allocated resources. After allocating intermediate blocks, it attempts to allocate a root memory block. If the root allocation fails, it logs an error and releases all resources. If all allocations are successful, it returns success.",
  "vulnerability_checklist": "Check if the loop iterating over ofs from r->offset to end is potentially long-running. Verify that the loop includes a mechanism to yield the CPU, such as calling cond_resched(). Ensure that hem_list_alloc_mid_bt is not called in a tight loop without yielding the CPU. Confirm that the loop counter or a similar mechanism is used to periodically yield the CPU.",
  "safe_verification_cot": "The loop iterates over ofs from r->offset to end, but now includes a loop counter. The cond_resched() function is called periodically based on the loop counter, allowing the CPU to yield. This prevents the function hem_list_alloc_mid_bt from causing resource exhaustion by ensuring the CPU is not monopolized.",
  "verification_cot": "The loop iterates over ofs from r->offset to end without any mechanism to yield the CPU. The function hem_list_alloc_mid_bt is called in each iteration, potentially leading to a long-running operation. Without yielding the CPU, the system can experience uncontrolled resource consumption, leading to resource exhaustion.",
  "vulnerability_related_variables": {
    "ofs": "Represents a starting point for a sequence of operations, incremented by a fixed size until a boundary is reached.",
    "end": "Represents a boundary calculated from a starting point and a size, used to determine the limit for a sequence of operations.",
    "unit": "Represents a fixed size derived from a shift value, used to determine the increment step for a sequence of operations."
  },
  "vulnerability_related_functions": {
    "hem_list_alloc_mid_bt": "The function manages a hierarchical memory structure by allocating and linking intermediate nodes. It checks for existing nodes at each level, calculates the required memory range, and allocates new nodes if necessary. It ensures that nodes are linked correctly to form a tree structure, handling errors by cleaning up partially allocated resources.",
    "cond_resched": "This function is not present in the provided code, so we cannot provide a self-contained explanation for it."
  },
  "root_cause": "Lack of CPU yielding mechanism in long-running loop leading to uncontrolled resource consumption.",
  "patch_cot": "To patch the vulnerability, first introduce a loop counter 'loop' and initialize it at the start of the loop. Modify the loop to increment the 'loop' counter with each iteration. Add a condition within the loop to check if the 'loop' counter has reached a threshold, and if so, call 'cond_resched()' to yield the CPU. Finally, ensure that the loop terminates correctly by verifying the condition involving 'ofs' and 'end'.",
  "fix_list": "1. Check if function hem_list_alloc_mid_bt is safely handling variable ofs. 2. Verify that variable end is properly calculated and used to terminate the loop. 3. Introduce a loop counter loop to track iterations. 4. Use cond_resched() to yield the CPU periodically."
}