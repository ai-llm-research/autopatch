{
 "supplementary_code": "```c\nstruct sock {\n/*\n* Now struct inet_timewait_sock also uses sock_common, so please just\n* don't add nothing before this first member (__sk_common) --acme\n*/\nstruct sock_common __sk_common;\n#define sk_node __sk_common.skc_node\n#define sk_nulls_node __sk_common.skc_nulls_node\n#define sk_refcnt __sk_common.skc_refcnt\n#define sk_tx_queue_mapping __sk_common.skc_tx_queue_mapping\n#ifdef CONFIG_SOCK_RX_QUEUE_MAPPING\n#define sk_rx_queue_mapping __sk_common.skc_rx_queue_mapping\n#endif\n#define sk_dontcopy_begin __sk_common.skc_dontcopy_begin\n#define sk_dontcopy_end __sk_common.skc_dontcopy_end\n#define sk_hash __sk_common.skc_hash\n#define sk_portpair __sk_common.skc_portpair\n#define sk_num __sk_common.skc_num\n#define sk_dport __sk_common.skc_dport\n#define sk_addrpair __sk_common.skc_addrpair\n#define sk_daddr __sk_common.skc_daddr\n#define sk_rcv_saddr __sk_common.skc_rcv_saddr\n#define sk_family __sk_common.skc_family\n#define sk_state __sk_common.skc_state\n#define sk_reuse __sk_common.skc_reuse\n#define sk_reuseport __sk_common.skc_reuseport\n#define sk_ipv6only __sk_common.skc_ipv6only\n#define sk_net_refcnt __sk_common.skc_net_refcnt\n#define sk_bound_dev_if __sk_common.skc_bound_dev_if\n#define sk_bind_node __sk_common.skc_bind_node\n#define sk_prot __sk_common.skc_prot\n#define sk_net __sk_common.skc_net\n#define sk_v6_daddr __sk_common.skc_v6_daddr\n#define sk_v6_rcv_saddr __sk_common.skc_v6_rcv_saddr\n#define sk_cookie __sk_common.skc_cookie\n#define sk_incoming_cpu __sk_common.skc_incoming_cpu\n#define sk_flags __sk_common.skc_flags\n#define sk_rxhash __sk_common.skc_rxhash\n__cacheline_group_begin(sock_write_rx);\natomic_t sk_drops;\n__s32 sk_peek_off;\nstruct sk_buff_head sk_error_queue;\nstruct sk_buff_head sk_receive_queue;\n/*\n* The backlog queue is special, it is always used with\n* the per-socket spinlock held and requires low latency\n* access. Therefore we special case it's implementation.\n* Note : rmem_alloc is in this structure to fill a hole\n* on 64bit arches, not because its logically part of\n* backlog.\n*/\nstruct {\natomic_t rmem_alloc;\nint len;\nstruct sk_buff *head;\nstruct sk_buff *tail;\n} sk_backlog;\n#define sk_rmem_alloc sk_backlog.rmem_alloc\n__cacheline_group_end(sock_write_rx);\n__cacheline_group_begin(sock_read_rx);\n/* early demux fields */\nstruct dst_entry __rcu *sk_rx_dst;\nint sk_rx_dst_ifindex;\nu32 sk_rx_dst_cookie;\n#ifdef CONFIG_NET_RX_BUSY_POLL\nunsigned int sk_ll_usec;\nunsigned int sk_napi_id;\nu16 sk_busy_poll_budget;\nu8 sk_prefer_busy_poll;\n#endif\nu8 sk_userlocks;\nint sk_rcvbuf;\nstruct sk_filter __rcu *sk_filter;\nunion {\nstruct socket_wq __rcu *sk_wq;\n/* private: */\nstruct socket_wq *sk_wq_raw;\n/* public: */\n};\nvoid (*sk_data_ready)(struct sock *sk);\nlong sk_rcvtimeo;\nint sk_rcvlowat;\n__cacheline_group_end(sock_read_rx);\n__cacheline_group_begin(sock_read_rxtx);\nint sk_err;\nstruct socket *sk_socket;\nstruct mem_cgroup *sk_memcg;\n#ifdef CONFIG_XFRM\nstruct xfrm_policy __rcu *sk_policy[2];\n#endif\n__cacheline_group_end(sock_read_rxtx);\n__cacheline_group_begin(sock_write_rxtx);\nsocket_lock_t sk_lock;\nu32 sk_reserved_mem;\nint sk_forward_alloc;\nu32 sk_tsflags;\n__cacheline_group_end(sock_write_rxtx);\n__cacheline_group_begin(sock_write_tx);\nint sk_write_pending;\natomic_t sk_omem_alloc;\nint sk_sndbuf;\nint sk_wmem_queued;\nrefcount_t sk_wmem_alloc;\nunsigned long sk_tsq_flags;\nunion {\nstruct sk_buff *sk_send_head;\nstruct rb_root tcp_rtx_queue;\n};\nstruct sk_buff_head sk_write_queue;\nu32 sk_dst_pending_confirm;\nu32 sk_pacing_status; /* see enum sk_pacing */\nstruct page_frag sk_frag;\nstruct timer_list sk_timer;\nunsigned long sk_pacing_rate; /* bytes per second */\natomic_t sk_zckey;\natomic_t sk_tskey;\n__cacheline_group_end(sock_write_tx);\n__cacheline_group_begin(sock_read_tx);\nunsigned long sk_max_pacing_rate;\nlong sk_sndtimeo;\nu32 sk_priority;\nu32 sk_mark;\nstruct dst_entry __rcu *sk_dst_cache;\nnetdev_features_t sk_route_caps;\n#ifdef CONFIG_SOCK_VALIDATE_XMIT\nstruct sk_buff* (*sk_validate_xmit_skb)(struct sock *sk,\nstruct net_device *dev,\nstruct sk_buff *skb);\n#endif\nu16 sk_gso_type;\nu16 sk_gso_max_segs;\nunsigned int sk_gso_max_size;\ngfp_t sk_allocation;\nu32 sk_txhash;\nu8 sk_pacing_shift;\nbool sk_use_task_frag;\n__cacheline_group_end(sock_read_tx);\n/*\n* Because of non atomicity rules, all\n* changes are protected by socket lock.\n*/\nu8 sk_gso_disabled : 1,\nsk_kern_sock : 1,\nsk_no_check_tx : 1,\nsk_no_check_rx : 1;\nu8 sk_shutdown;\nu16 sk_type;\nu16 sk_protocol;\nunsigned long sk_lingertime;\nstruct proto *sk_prot_creator;\nrwlock_t sk_callback_lock;\nint sk_err_soft;\nu32 sk_ack_backlog;\nu32 sk_max_ack_backlog;\nkuid_t sk_uid;\nspinlock_t sk_peer_lock;\nint sk_bind_phc;\nstruct pid *sk_peer_pid;\nconst struct cred *sk_peer_cred;\nktime_t sk_stamp;\n#if BITS_PER_LONG==32\nseqlock_t sk_stamp_seq;\n#endif\nint sk_disconnects;\nu8 sk_txrehash;\nu8 sk_clockid;\nu8 sk_txtime_deadline_mode : 1,\nsk_txtime_report_errors : 1,\nsk_txtime_unused : 6;\nvoid *sk_user_data;\n#ifdef CONFIG_SECURITY\nvoid *sk_security;\n#endif\nstruct sock_cgroup_data sk_cgrp_data;\nvoid (*sk_state_change)(struct sock *sk);\nvoid (*sk_write_space)(struct sock *sk);\nvoid (*sk_error_report)(struct sock *sk);\nint (*sk_backlog_rcv)(struct sock *sk,\nstruct sk_buff *skb);\nvoid (*sk_destruct)(struct sock *sk);\nstruct sock_reuseport __rcu *sk_reuseport_cb;\n#ifdef CONFIG_BPF_SYSCALL\nstruct bpf_local_storage __rcu *sk_bpf_storage;\n#endif\nstruct rcu_head sk_rcu;\nnetns_tracker ns_tracker;\nstruct xarray sk_user_frags;\n};\n```\n```c\nstatic inline bool sk_is_tcp(const struct sock *sk)\n{\nreturn sk_is_inet(sk) &&\nsk->sk_type == SOCK_STREAM &&\nsk->sk_protocol == IPPROTO_TCP;\n}\n```\n```c\nstatic inline bool sk_is_stream_unix(const struct sock *sk)\n{\nreturn sk->sk_family == AF_UNIX && sk->sk_type == SOCK_STREAM;\n}\n```\n",
 "function_prototype": "static bool sock_map_sk_state_allowed(const struct sock *sk)",
 "re_implemented_code": "\n```c\nstatic bool sock_map_sk_state_allowed(const struct sock *sk) {\n// Check if the socket is a TCP socket\nif (sk_isTcp(sk)) { // Assuming sk_isTcp is defined elsewhere\n// Return true if the socket state is either ESTABLISHED or LISTEN\nreturn sk->state == SS_ESTABLISHED || sk->state == SS_LISTEN;\n}\n// Check if the socket is a UNIX stream socket\nif (sk->family == PF_UNIX && sk->type == SOCK_STREAM) {\n// Return true if the socket state is ESTABLISHED\nreturn sk->state == SS_ESTABLISHED;\n}\n// For all other socket types, return true by default\nreturn true;\n}\n```\n",
 "is_vulnerable": "N/A"
}