{
  "cwe_type": "NULL Pointer Dereference",
  "cve_id": "CVE-2025-21864",
  "supplementary_code": "```c\nstruct sock {\n/*\n* Now struct inet_timewait_sock also uses sock_common, so please just\n* don't add nothing before this first member (__sk_common) --acme\n*/\nstruct sock_common __sk_common;\n#define sk_node __sk_common.skc_node\n#define sk_nulls_node __sk_common.skc_nulls_node\n#define sk_refcnt __sk_common.skc_refcnt\n#define sk_tx_queue_mapping __sk_common.skc_tx_queue_mapping\n#ifdef CONFIG_SOCK_RX_QUEUE_MAPPING\n#define sk_rx_queue_mapping __sk_common.skc_rx_queue_mapping\n#endif\n#define sk_dontcopy_begin __sk_common.skc_dontcopy_begin\n#define sk_dontcopy_end __sk_common.skc_dontcopy_end\n#define sk_hash __sk_common.skc_hash\n#define sk_portpair __sk_common.skc_portpair\n#define sk_num __sk_common.skc_num\n#define sk_dport __sk_common.skc_dport\n#define sk_addrpair __sk_common.skc_addrpair\n#define sk_daddr __sk_common.skc_daddr\n#define sk_rcv_saddr __sk_common.skc_rcv_saddr\n#define sk_family __sk_common.skc_family\n#define sk_state __sk_common.skc_state\n#define sk_reuse __sk_common.skc_reuse\n#define sk_reuseport __sk_common.skc_reuseport\n#define sk_ipv6only __sk_common.skc_ipv6only\n#define sk_net_refcnt __sk_common.skc_net_refcnt\n#define sk_bound_dev_if __sk_common.skc_bound_dev_if\n#define sk_bind_node __sk_common.skc_bind_node\n#define sk_prot __sk_common.skc_prot\n#define sk_net __sk_common.skc_net\n#define sk_v6_daddr __sk_common.skc_v6_daddr\n#define sk_v6_rcv_saddr __sk_common.skc_v6_rcv_saddr\n#define sk_cookie __sk_common.skc_cookie\n#define sk_incoming_cpu __sk_common.skc_incoming_cpu\n#define sk_flags __sk_common.skc_flags\n#define sk_rxhash __sk_common.skc_rxhash\n__cacheline_group_begin(sock_write_rx);\natomic_t sk_drops;\n__s32 sk_peek_off;\nstruct sk_buff_head sk_error_queue;\nstruct sk_buff_head sk_receive_queue;\n/*\n* The backlog queue is special, it is always used with\n* the per-socket spinlock held and requires low latency\n* access. Therefore we special case it's implementation.\n* Note : rmem_alloc is in this structure to fill a hole\n* on 64bit arches, not because its logically part of\n* backlog.\n*/\nstruct {\natomic_t rmem_alloc;\nint len;\nstruct sk_buff *head;\nstruct sk_buff *tail;\n} sk_backlog;\n#define sk_rmem_alloc sk_backlog.rmem_alloc\n__cacheline_group_end(sock_write_rx);\n__cacheline_group_begin(sock_read_rx);\n/* early demux fields */\nstruct dst_entry __rcu *sk_rx_dst;\nint sk_rx_dst_ifindex;\nu32 sk_rx_dst_cookie;\n#ifdef CONFIG_NET_RX_BUSY_POLL\nunsigned int sk_ll_usec;\nunsigned int sk_napi_id;\nu16 sk_busy_poll_budget;\nu8 sk_prefer_busy_poll;\n#endif\nu8 sk_userlocks;\nint sk_rcvbuf;\nstruct sk_filter __rcu *sk_filter;\nunion {\nstruct socket_wq __rcu *sk_wq;\n/* private: */\nstruct socket_wq *sk_wq_raw;\n/* public: */\n};\nvoid (*sk_data_ready)(struct sock *sk);\nlong sk_rcvtimeo;\nint sk_rcvlowat;\n__cacheline_group_end(sock_read_rx);\n__cacheline_group_begin(sock_read_rxtx);\nint sk_err;\nstruct socket *sk_socket;\nstruct mem_cgroup *sk_memcg;\n#ifdef CONFIG_XFRM\nstruct xfrm_policy __rcu *sk_policy[2];\n#endif\n__cacheline_group_end(sock_read_rxtx);\n__cacheline_group_begin(sock_write_rxtx);\nsocket_lock_t sk_lock;\nu32 sk_reserved_mem;\nint sk_forward_alloc;\nu32 sk_tsflags;\n__cacheline_group_end(sock_write_rxtx);\n__cacheline_group_begin(sock_write_tx);\nint sk_write_pending;\natomic_t sk_omem_alloc;\nint sk_sndbuf;\nint sk_wmem_queued;\nrefcount_t sk_wmem_alloc;\nunsigned long sk_tsq_flags;\nunion {\nstruct sk_buff *sk_send_head;\nstruct rb_root tcp_rtx_queue;\n};\nstruct sk_buff_head sk_write_queue;\nu32 sk_dst_pending_confirm;\nu32 sk_pacing_status; /* see enum sk_pacing */\nstruct page_frag sk_frag;\nstruct timer_list sk_timer;\nunsigned long sk_pacing_rate; /* bytes per second */\natomic_t sk_zckey;\natomic_t sk_tskey;\n__cacheline_group_end(sock_write_tx);\n__cacheline_group_begin(sock_read_tx);\nunsigned long sk_max_pacing_rate;\nlong sk_sndtimeo;\nu32 sk_priority;\nu32 sk_mark;\nstruct dst_entry __rcu *sk_dst_cache;\nnetdev_features_t sk_route_caps;\n#ifdef CONFIG_SOCK_VALIDATE_XMIT\nstruct sk_buff* (*sk_validate_xmit_skb)(struct sock *sk,\nstruct net_device *dev,\nstruct sk_buff *skb);\n#endif\nu16 sk_gso_type;\nu16 sk_gso_max_segs;\nunsigned int sk_gso_max_size;\ngfp_t sk_allocation;\nu32 sk_txhash;\nu8 sk_pacing_shift;\nbool sk_use_task_frag;\n__cacheline_group_end(sock_read_tx);\n/*\n* Because of non atomicity rules, all\n* changes are protected by socket lock.\n*/\nu8 sk_gso_disabled : 1,\nsk_kern_sock : 1,\nsk_no_check_tx : 1,\nsk_no_check_rx : 1;\nu8 sk_shutdown;\nu16 sk_type;\nu16 sk_protocol;\nunsigned long sk_lingertime;\nstruct proto *sk_prot_creator;\nrwlock_t sk_callback_lock;\nint sk_err_soft;\nu32 sk_ack_backlog;\nu32 sk_max_ack_backlog;\nkuid_t sk_uid;\nspinlock_t sk_peer_lock;\nint sk_bind_phc;\nstruct pid *sk_peer_pid;\nconst struct cred *sk_peer_cred;\nktime_t sk_stamp;\n#if BITS_PER_LONG==32\nseqlock_t sk_stamp_seq;\n#endif\nint sk_disconnects;\nu8 sk_txrehash;\nu8 sk_clockid;\nu8 sk_txtime_deadline_mode : 1,\nsk_txtime_report_errors : 1,\nsk_txtime_unused : 6;\nvoid *sk_user_data;\n#ifdef CONFIG_SECURITY\nvoid *sk_security;\n#endif\nstruct sock_cgroup_data sk_cgrp_data;\nvoid (*sk_state_change)(struct sock *sk);\nvoid (*sk_write_space)(struct sock *sk);\nvoid (*sk_error_report)(struct sock *sk);\nint (*sk_backlog_rcv)(struct sock *sk,\nstruct sk_buff *skb);\nvoid (*sk_destruct)(struct sock *sk);\nstruct sock_reuseport __rcu *sk_reuseport_cb;\n#ifdef CONFIG_BPF_SYSCALL\nstruct bpf_local_storage __rcu *sk_bpf_storage;\n#endif\nstruct rcu_head sk_rcu;\nnetns_tracker ns_tracker;\nstruct xarray sk_user_frags;\n};\n```\n```c\nstatic inline bool sk_is_tcp(const struct sock *sk)\n{\nreturn sk_is_inet(sk) &&\nsk->sk_type == SOCK_STREAM &&\nsk->sk_protocol == IPPROTO_TCP;\n}\n```\n```c\nstatic inline bool sk_is_stream_unix(const struct sock *sk)\n{\nreturn sk->sk_family == AF_UNIX && sk->sk_type == SOCK_STREAM;\n}\n```",
  "original_code": "```c\nstatic bool sock_map_sk_state_allowed(const struct sock *sk)\n{\nif (sk_is_tcp(sk))\nreturn (1 << sk->sk_state) & (TCPF_ESTABLISHED | TCPF_LISTEN);\nif (sk_is_stream_unix(sk))\nreturn (1 << sk->sk_state) & TCPF_ESTABLISHED;\nreturn true;\n}\n```",
  "vuln_patch": "```c\nstatic bool sock_map_sk_state_allowed(const struct sock *sk)\n{\nif (sk_is_tcp(sk))\nreturn (1 << sk->sk_state) & (TCPF_ESTABLISHED | TCPF_LISTEN);\nif (sk_is_stream_unix(sk))\nreturn (1 << sk->sk_state) & TCPF_ESTABLISHED;\nif (sk_is_vsock(sk) &&\n(sk->sk_type == SOCK_STREAM || sk->sk_type == SOCK_SEQPACKET))\nreturn (1 << sk->sk_state) & TCPF_ESTABLISHED;\nreturn true;\n}\n```",
  "function_name": "sock_map_sk_state_allowed",
  "function_prototype": "static bool sock_map_sk_state_allowed(const struct sock *sk)",
  "code_semantics": "The function evaluates the type and state of a network communication endpoint. It first checks if the endpoint is a specific type of connection-oriented protocol. If it is, it verifies if the endpoint is in one of the predefined active or listening states. If the endpoint is of another specific type, it checks if it is in an active state. If neither condition applies, it assumes the state is acceptable.",
  "safe_verification_cot": "1. The function sk_is_tcp is still called with the variable sk, but the additional check for VSOCK types ensures that sk is properly handled.\n2. The variable sk->sk_state is now safely used because the additional check for VSOCK types ensures that sk is properly initialized before use.\n3. The function sk_is_stream_unix is still called with the variable sk, but the additional check for VSOCK types ensures that sk is properly handled.\n4. The variable sk->sk_type is now safely used because the additional check for VSOCK types ensures that sk is properly initialized before use.\n5. The function sk_is_vsock is now present in the patched code, ensuring that VSOCK types are properly handled, preventing NULL pointer dereference when accessing sk->sk_state.",
  "verification_cot": "1. The function sk_is_tcp is called with the variable sk, but there is no check to ensure sk is not NULL before accessing sk->sk_type and sk->sk_protocol.\n2. The variable sk->sk_state is used in a bitwise operation without checking if sk is NULL, which can lead to a NULL pointer dereference.\n3. The function sk_is_stream_unix is called with the variable sk, but there is no check to ensure sk is not NULL before accessing sk->sk_family and sk->sk_type.\n4. The variable sk->sk_type is used in conditional statements without checking if sk is NULL, which can lead to a NULL pointer dereference.\n5. The function sk_is_vsock is not present in the vulnerable code, so there is no handling for VSOCK types, leading to potential NULL pointer dereference when accessing sk->sk_state.",
  "vulnerability_related_variables": {
    "sk->sk_state": "This variable represents the current state of a communication endpoint, which is used to determine if certain operations are permissible based on predefined state conditions.",
    "sk->sk_type": "This variable indicates the communication semantics of a communication endpoint, such as whether it is a stream-oriented or datagram-oriented endpoint.",
    "sk->sk_family": "This variable specifies the protocol family of a communication endpoint, which determines the addressing structure and protocol used for communication.",
    "sk": "This is a reference to a data structure that encapsulates various attributes and states of a communication endpoint, allowing for the management and operation of network connections."
  },
  "vulnerability_related_functions": {
    "sk_is_tcp": "This function determines if a given socket is an Internet socket using the TCP protocol in stream mode. It checks the socket's type and protocol to make this determination.",
    "sk_is_stream_unix": "This function determines if a given socket is a UNIX socket in stream mode. It checks the socket's family and type to make this determination.",
    "sk_is_vsock": "The functionality of this function cannot be determined as it is not present in the provided code."
  },
  "root_cause": "The root cause of the vulnerability is a NULL pointer dereference due to missing checks for VSOCK types, leading to improper access of sk->sk_state.",
  "patch_cot": "Step 1: Identify where the sk->sk_state is accessed in the code. In this case, it's within the sock_map_sk_state_allowed function. Step 2: Add a condition to check if the socket is a VSOCK type using the sk_is_vsock function. Step 3: Ensure that the sk->sk_type is either SOCK_STREAM or SOCK_SEQPACKET for VSOCK types before accessing sk->sk_state. Step 4: Implement the additional check in the sock_map_sk_state_allowed function to prevent accessing sk->sk_state for invalid socket types. Step 5: Test the code to ensure that the NULL pointer dereference is resolved and that the function behaves correctly for all socket types."
}