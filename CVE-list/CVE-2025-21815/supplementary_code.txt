```c
struct compact_control {
    struct list_head freepages[NR_PAGE_ORDERS]; /* List of free pages to migrate to */
    struct list_head migratepages;  /* List of pages being migrated */
    unsigned int nr_freepages;  /* Number of isolated free pages */
    unsigned int nr_migratepages;   /* Number of pages to migrate */
    unsigned long free_pfn;     /* isolate_freepages search base */
    /*
     * Acts as an in/out parameter to page isolation for migration.
     * isolate_migratepages uses it as a search base.
     * isolate_migratepages_block will update the value to the next pfn
     * after the last isolated one.
     */
    unsigned long migrate_pfn;
    unsigned long fast_start_pfn;   /* a pfn to start linear scan from */
    struct zone *zone;
    unsigned long total_migrate_scanned;
    unsigned long total_free_scanned;
    unsigned short fast_search_fail;/* failures to use free list searches */
    short search_order;     /* order to start a fast search at */
    const gfp_t gfp_mask;       /* gfp mask of a direct compactor */
    int order;          /* order a direct compactor needs */
    int migratetype;        /* migratetype of direct compactor */
    const unsigned int alloc_flags; /* alloc flags of a direct compactor */
    const int highest_zoneidx;  /* zone index of a direct compactor */
    enum migrate_mode mode;     /* Async or sync migration mode */
    bool ignore_skip_hint;      /* Scan blocks even if marked skip */
    bool no_set_skip_hint;      /* Don't mark blocks for skipping */
    bool ignore_block_suitable; /* Scan blocks considered unsuitable */
    bool direct_compaction;     /* False from kcompactd or /proc/... */
    bool proactive_compaction;  /* kcompactd proactive compaction */
    bool whole_zone;        /* Whole zone should/has been scanned */
    bool contended;         /* Signal lock contention */
    bool finish_pageblock;      /* Scan the remainder of a pageblock. Used
                     * when there are potentially transient
                     * isolation or migration failures to
                     * ensure forward progress.
                     */
    bool alloc_contig;      /* alloc_contig_range allocation */
};
```

```c
struct list_head {
    struct list_head *next, *prev;
};
```

```c
#define pfn_to_page __pfn_to_page
```

```c
static bool compact_unlock_should_abort(spinlock_t *lock,
        unsigned long flags, bool *locked, struct compact_control *cc)
{
    if (*locked) {
        spin_unlock_irqrestore(lock, flags);
        *locked = false;
    }

    if (fatal_signal_pending(current)) {
        cc->contended = true;
        return true;
    }

    cond_resched();

    return false;
}
```

```c
static __always_inline int PageCompound(const struct page *page)
{
    return test_bit(PG_head, &page->flags) ||
           READ_ONCE(page->compound_head) & 1;
}
```

```c
static inline unsigned int compound_order(struct page *page)
{
    struct folio *folio = (struct folio *)page;

    if (!test_bit(PG_head, &folio->flags))
        return 0;
    return folio->_flags_1 & 0xff;
}
```

```c
static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,
                        struct compact_control *cc)
    __acquires(lock)
{
    /* Track if the lock is contended in async mode */
    if (cc->mode == MIGRATE_ASYNC && !cc->contended) {
        if (spin_trylock_irqsave(lock, *flags))
            return true;

        cc->contended = true;
    }

    spin_lock_irqsave(lock, *flags);
    return true;
}
```

```c
int __isolate_free_page(struct page *page, unsigned int order)
{
    struct zone *zone = page_zone(page);
    int mt = get_pageblock_migratetype(page);

    if (!is_migrate_isolate(mt)) {
        unsigned long watermark;
        /*
         * Obey watermarks as if the page was being allocated. We can
         * emulate a high-order watermark check with a raised order-0
         * watermark, because we already know our high-order page
         * exists.
         */
        watermark = zone->_watermark[WMARK_MIN] + (1UL << order);
        if (!zone_watermark_ok(zone, 0, watermark, 0, ALLOC_CMA))
            return 0;
    }

    del_page_from_free_list(page, zone, order, mt);

    /*
     * Set the pageblock if the isolated page is at least half of a
     * pageblock
     */
    if (order >= pageblock_order - 1) {
        struct page *endpage = page + (1 << order) - 1;
        for (; page < endpage; page += pageblock_nr_pages) {
            int mt = get_pageblock_migratetype(page);
            /*
             * Only change normal pageblocks (i.e., they can merge
             * with others)
             */
            if (migratetype_is_mergeable(mt))
                move_freepages_block(zone, page, mt,
                             MIGRATE_MOVABLE);
        }
    }

    return 1UL << order;
}
```

```c
static inline unsigned int buddy_order(struct page *page)
{
    /* PageBuddy() must be checked by the caller */
    return page_private(page);
}
```

```c
static inline void set_page_private(struct page *page, unsigned long private)
{
    page->private = private;
}
```

```c
static inline void list_add_tail(struct list_head *new, struct list_head *head)
{
    __list_add(new, head->prev, head);
}
```

```c
static __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
{
    raw_spin_unlock_irqrestore(&lock->rlock, flags);
}
```

```c
#define unlikely(x) __builtin_expect(!!(x), 0)
```

```c
#define count_compact_events(item, delta) do { } while (0)
```
