{
  "cwe_type": "Integer Underflow",
  "cve_id": "CVE-2025-21815",
  "supplementary_code": "```c\nstruct compact_control {\nstruct list_head freepages[NR_PAGE_ORDERS]; /* List of free pages to migrate to */\nstruct list_head migratepages; /* List of pages being migrated */\nunsigned int nr_freepages; /* Number of isolated free pages */\nunsigned int nr_migratepages; /* Number of pages to migrate */\nunsigned long free_pfn; /* isolate_freepages search base */\n/*\n* Acts as an in/out parameter to page isolation for migration.\n* isolate_migratepages uses it as a search base.\n* isolate_migratepages_block will update the value to the next pfn\n* after the last isolated one.\n*/\nunsigned long migrate_pfn;\nunsigned long fast_start_pfn; /* a pfn to start linear scan from */\nstruct zone *zone;\nunsigned long total_migrate_scanned;\nunsigned long total_free_scanned;\nunsigned short fast_search_fail;/* failures to use free list searches */\nshort search_order; /* order to start a fast search at */\nconst gfp_t gfp_mask; /* gfp mask of a direct compactor */\nint order; /* order a direct compactor needs */\nint migratetype; /* migratetype of direct compactor */\nconst unsigned int alloc_flags; /* alloc flags of a direct compactor */\nconst int highest_zoneidx; /* zone index of a direct compactor */\nenum migrate_mode mode; /* Async or sync migration mode */\nbool ignore_skip_hint; /* Scan blocks even if marked skip */\nbool no_set_skip_hint; /* Don't mark blocks for skipping */\nbool ignore_block_suitable; /* Scan blocks considered unsuitable */\nbool direct_compaction; /* False from kcompactd or /proc/... */\nbool proactive_compaction; /* kcompactd proactive compaction */\nbool whole_zone; /* Whole zone should/has been scanned */\nbool contended; /* Signal lock contention */\nbool finish_pageblock; /* Scan the remainder of a pageblock. Used\n* when there are potentially transient\n* isolation or migration failures to\n* ensure forward progress.\n*/\nbool alloc_contig; /* alloc_contig_range allocation */\n};\n```\n```c\nstruct list_head {\nstruct list_head *next, *prev;\n};\n```\n```c\n#define pfn_to_page __pfn_to_page\n```\n```c\nstatic bool compact_unlock_should_abort(spinlock_t *lock,\nunsigned long flags, bool *locked, struct compact_control *cc)\n{\nif (*locked) {\nspin_unlock_irqrestore(lock, flags);\n*locked = false;\n}\nif (fatal_signal_pending(current)) {\ncc->contended = true;\nreturn true;\n}\ncond_resched();\nreturn false;\n}\n```\n```c\nstatic __always_inline int PageCompound(const struct page *page)\n{\nreturn test_bit(PG_head, &page->flags) ||\nREAD_ONCE(page->compound_head) & 1;\n}\n```\n```c\nstatic inline unsigned int compound_order(struct page *page)\n{\nstruct folio *folio = (struct folio *)page;\nif (!test_bit(PG_head, &folio->flags))\nreturn 0;\nreturn folio->_flags_1 & 0xff;\n}\n```\n```c\nstatic bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\nstruct compact_control *cc)\n__acquires(lock)\n{\n/* Track if the lock is contended in async mode */\nif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\nif (spin_trylock_irqsave(lock, *flags))\nreturn true;\ncc->contended = true;\n}\nspin_lock_irqsave(lock, *flags);\nreturn true;\n}\n```\n```c\nint __isolate_free_page(struct page *page, unsigned int order)\n{\nstruct zone *zone = page_zone(page);\nint mt = get_pageblock_migratetype(page);\nif (!is_migrate_isolate(mt)) {\nunsigned long watermark;\n/*\n* Obey watermarks as if the page was being allocated. We can\n* emulate a high-order watermark check with a raised order-0\n* watermark, because we already know our high-order page\n* exists.\n*/\nwatermark = zone->_watermark[WMARK_MIN] + (1UL << order);\nif (!zone_watermark_ok(zone, 0, watermark, 0, ALLOC_CMA))\nreturn 0;\n}\ndel_page_from_free_list(page, zone, order, mt);\n/*\n* Set the pageblock if the isolated page is at least half of a\n* pageblock\n*/\nif (order >= pageblock_order - 1) {\nstruct page *endpage = page + (1 << order) - 1;\nfor (; page < endpage; page += pageblock_nr_pages) {\nint mt = get_pageblock_migratetype(page);\n/*\n* Only change normal pageblocks (i.e., they can merge\n* with others)\n*/\nif (migratetype_is_mergeable(mt))\nmove_freepages_block(zone, page, mt,\nMIGRATE_MOVABLE);\n}\n}\nreturn 1UL << order;\n}\n```\n```c\nstatic inline unsigned int buddy_order(struct page *page)\n{\n/* PageBuddy() must be checked by the caller */\nreturn page_private(page);\n}\n```\n```c\nstatic inline void set_page_private(struct page *page, unsigned long private)\n{\npage->private = private;\n}\n```\n```c\nstatic inline void list_add_tail(struct list_head *new, struct list_head *head)\n{\n__list_add(new, head->prev, head);\n}\n```\n```c\nstatic __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)\n{\nraw_spin_unlock_irqrestore(&lock->rlock, flags);\n}\n```\n```c\n#define unlikely(x) __builtin_expect(!!(x), 0)\n```\n```c\n#define count_compact_events(item, delta) do { } while (0)\n```",
  "original_code": "```c\nstatic unsigned long isolate_freepages_block(struct compact_control *cc,\nunsigned long *start_pfn,\nunsigned long end_pfn,\nstruct list_head *freelist,\nunsigned int stride,\nbool strict)\n{\nint nr_scanned = 0, total_isolated = 0;\nstruct page *page;\nunsigned long flags = 0;\nbool locked = false;\nunsigned long blockpfn = *start_pfn;\nunsigned int order;\n/* Strict mode is for isolation, speed is secondary */\nif (strict)\nstride = 1;\npage = pfn_to_page(blockpfn);\n/* Isolate free pages. */\nfor (; blockpfn < end_pfn; blockpfn += stride, page += stride) {\nint isolated;\n/*\n* Periodically drop the lock (if held) regardless of its\n* contention, to give chance to IRQs. Abort if fatal signal\n* pending.\n*/\nif (!(blockpfn % COMPACT_CLUSTER_MAX)\n&& compact_unlock_should_abort(&cc->zone->lock, flags,\n&locked, cc))\nbreak;\nnr_scanned++;\n/*\n* For compound pages such as THP and hugetlbfs, we can save\n* potentially a lot of iterations if we skip them at once.\n* The check is racy, but we can consider only valid values\n* and the only danger is skipping too much.\n*/\nif (PageCompound(page)) {\nconst unsigned int order = compound_order(page);\nif (blockpfn + (1UL << order) <= end_pfn) {\nblockpfn += (1UL << order) - 1;\npage += (1UL << order) - 1;\nnr_scanned += (1UL << order) - 1;\n}\ngoto isolate_fail;\n}\nif (!PageBuddy(page))\ngoto isolate_fail;\n/* If we already hold the lock, we can skip some rechecking. */\nif (!locked) {\nlocked = compact_lock_irqsave(&cc->zone->lock,\n&flags, cc);\n/* Recheck this is a buddy page under lock */\nif (!PageBuddy(page))\ngoto isolate_fail;\n}\n/* Found a free page, will break it into order-0 pages */\norder = buddy_order(page);\nisolated = __isolate_free_page(page, order);\nif (!isolated)\nbreak;\nset_page_private(page, order);\nnr_scanned += isolated - 1;\ntotal_isolated += isolated;\ncc->nr_freepages += isolated;\nlist_add_tail(&page->lru, &freelist[order]);\nif (!strict && cc->nr_migratepages <= cc->nr_freepages) {\nblockpfn += isolated;\nbreak;\n}\n/* Advance to the end of split page */\nblockpfn += isolated - 1;\npage += isolated - 1;\ncontinue;\nisolate_fail:\nif (strict)\nbreak;\n}\nif (locked)\nspin_unlock_irqrestore(&cc->zone->lock, flags);\n/*\n* Be careful to not go outside of the pageblock.\n*/\nif (unlikely(blockpfn > end_pfn))\nblockpfn = end_pfn;\ntrace_mm_compaction_isolate_freepages(*start_pfn, blockpfn,\nnr_scanned, total_isolated);\n/* Record how far we have got within the block */\n*start_pfn = blockpfn;\n/*\n* If strict isolation is requested by CMA then check that all the\n* pages requested were isolated. If there were any failures, 0 is\n* returned and CMA will fail.\n*/\nif (strict && blockpfn < end_pfn)\ntotal_isolated = 0;\ncc->total_free_scanned += nr_scanned;\nif (total_isolated)\ncount_compact_events(COMPACTISOLATED, total_isolated);\nreturn total_isolated;\n}\n```",
  "vuln_patch": "```c\nstatic unsigned long isolate_freepages_block(struct compact_control *cc,\nunsigned long *start_pfn,\nunsigned long end_pfn,\nstruct list_head *freelist,\nunsigned int stride,\nbool strict)\n{\nint nr_scanned = 0, total_isolated = 0;\nstruct page *page;\nunsigned long flags = 0;\nbool locked = false;\nunsigned long blockpfn = *start_pfn;\nunsigned int order;\n/* Strict mode is for isolation, speed is secondary */\nif (strict)\nstride = 1;\npage = pfn_to_page(blockpfn);\n/* Isolate free pages. */\nfor (; blockpfn < end_pfn; blockpfn += stride, page += stride) {\nint isolated;\n/*\n* Periodically drop the lock (if held) regardless of its\n* contention, to give chance to IRQs. Abort if fatal signal\n* pending.\n*/\nif (!(blockpfn % COMPACT_CLUSTER_MAX)\n&& compact_unlock_should_abort(&cc->zone->lock, flags,\n&locked, cc))\nbreak;\nnr_scanned++;\n/*\n* For compound pages such as THP and hugetlbfs, we can save\n* potentially a lot of iterations if we skip them at once.\n* The check is racy, but we can consider only valid values\n* and the only danger is skipping too much.\n*/\nif (PageCompound(page)) {\nconst unsigned int order = compound_order(page);\nif ((order <= MAX_PAGE_ORDER) &&\n(blockpfn + (1UL << order) <= end_pfn)) {\nblockpfn += (1UL << order) - 1;\npage += (1UL << order) - 1;\nnr_scanned += (1UL << order) - 1;\n}\ngoto isolate_fail;\n}\nif (!PageBuddy(page))\ngoto isolate_fail;\n/* If we already hold the lock, we can skip some rechecking. */\nif (!locked) {\nlocked = compact_lock_irqsave(&cc->zone->lock,\n&flags, cc);\n/* Recheck this is a buddy page under lock */\nif (!PageBuddy(page))\ngoto isolate_fail;\n}\n/* Found a free page, will break it into order-0 pages */\norder = buddy_order(page);\nisolated = __isolate_free_page(page, order);\nif (!isolated)\nbreak;\nset_page_private(page, order);\nnr_scanned += isolated - 1;\ntotal_isolated += isolated;\ncc->nr_freepages += isolated;\nlist_add_tail(&page->lru, &freelist[order]);\nif (!strict && cc->nr_migratepages <= cc->nr_freepages) {\nblockpfn += isolated;\nbreak;\n}\n/* Advance to the end of split page */\nblockpfn += isolated - 1;\npage += isolated - 1;\ncontinue;\nisolate_fail:\nif (strict)\nbreak;\n}\nif (locked)\nspin_unlock_irqrestore(&cc->zone->lock, flags);\n/*\n* Be careful to not go outside of the pageblock.\n*/\nif (unlikely(blockpfn > end_pfn))\nblockpfn = end_pfn;\ntrace_mm_compaction_isolate_freepages(*start_pfn, blockpfn,\nnr_scanned, total_isolated);\n/* Record how far we have got within the block */\n*start_pfn = blockpfn;\n/*\n* If strict isolation is requested by CMA then check that all the\n* pages requested were isolated. If there were any failures, 0 is\n* returned and CMA will fail.\n*/\nif (strict && blockpfn < end_pfn)\ntotal_isolated = 0;\ncc->total_free_scanned += nr_scanned;\nif (total_isolated)\ncount_compact_events(COMPACTISOLATED, total_isolated);\nreturn total_isolated;\n}\n```",
  "function_name": "isolate_freepages_block",
  "function_prototype": "static unsigned long isolate_freepages_block(struct compact_control *cc, unsigned long *start_pfn, unsigned long end_pfn, struct list_head *freelist, unsigned int stride, bool strict)",
  "code_semantics": "The function isolates free pages within a specified range of page frame numbers (PFNs) by scanning through a block of memory. It identifies free pages and isolates them for further use, considering conditions like compound pages, buddy pages, and locking mechanisms. The function handles different modes, such as strict isolation, which prioritizes isolation over speed. It initializes variables, iterates over the memory block, checks each page for isolation eligibility, and updates counters for scanned and isolated pages. It periodically releases locks and checks for fatal signals to ensure safe operation. After completing the loop, it records progress and returns the total number of isolated pages.",
  "vulnerability_checklist": "Check if function PageCompound is safely handling variable order. Verify that variable order is properly bounded before it is used in the expression (1UL << order). Ensure that the condition (order <= MAX_PAGE_ORDER) is checked before performing operations involving order.",
  "safe_verification_cot": "1. The PageCompound(page) function is called to check if a page is a compound page. 2. The compound_order(page) function is used to retrieve the order of the page. 3. In the Target Code, an additional condition (order <= MAX_PAGE_ORDER) is added to ensure that order is within a safe range before using it in the expression (1UL << order). 4. This prevents the integer underflow by ensuring that the bitwise shift operation is only performed with a valid order value.",
  "verification_cot": "1. The PageCompound(page) function is called to check if a page is a compound page. 2. The compound_order(page) function is used to retrieve the order of the page. 3. In the Vulnerable Code, there is no check to ensure that order is within a safe range before using it in the expression (1UL << order). 4. If order is too large, the bitwise shift operation can result in an integer underflow, leading to potential memory corruption.",
  "vulnerability_related_variables": {
    "order": "This variable represents the size of a memory block in terms of the number of contiguous pages. It is used to determine how many pages should be isolated and to manage the free list of pages.",
    "blockpfn": "This variable acts as a pointer to the current position in a range of memory addresses. It is used to iterate over a block of memory pages, isolating free pages for compaction.",
    "end_pfn": "This variable represents the endpoint of a memory range. It is used as a boundary to ensure that operations do not exceed the specified range during memory page isolation."
  },
  "vulnerability_related_functions": {
    "PageCompound": "This function determines if a memory page is part of a larger, multi-page structure by checking specific bits in its metadata. It returns a true or false value based on whether the page is identified as a compound page.",
    "compound_order": "This function calculates the size of a multi-page structure by examining specific metadata of the page. It returns a numerical value representing the order, or size, of the compound page."
  },
  "root_cause": "Integer underflow due to unchecked large 'order' value in bitwise shift operation.",
  "patch_cot": "First, identify all instances where the 'order' variable is used in bitwise shift operations. Add a check to ensure that 'order' does not exceed a defined maximum value ('MAX_PAGE_ORDER') before performing the shift. In the function 'PageCompound', ensure that the value returned for 'order' is within a safe range. This might involve adding a check or modifying the logic to prevent returning excessively large values. After any operation that modifies 'blockpfn', ensure that it is checked against 'end_pfn' to confirm it has not exceeded the bounds. This can be done by adding a conditional check and adjusting 'blockpfn' if necessary. Validate 'end_pfn' at the start of the function to ensure it is a valid endpoint for the operations being performed. This might involve checking its value against known safe limits or conditions.",
  "fix_list": "Verify that variable 'order' is properly checked to ensure it does not exceed a safe maximum value before it is used in any bitwise shift operations. Check if function 'PageCompound' is safely handling variable 'order' by ensuring it does not return an excessively large value that could lead to an underflow. Ensure that variable 'blockpfn' is always less than or equal to 'end_pfn' after any operations that modify it, to prevent it from exceeding bounds. Verify that variable 'end_pfn' is properly validated to ensure it is a valid and safe endpoint for operations involving 'blockpfn'."
}