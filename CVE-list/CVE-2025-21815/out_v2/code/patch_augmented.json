{
 "re_implemented_code": "```c\n#define MALLOC false\nstatic unsigned long isolate_freepages_block(struct compact_control *ctrl,\nunsigned long *initial_pfn,\nunsigned long final_pfn,\nstruct list_head *node_list,\nunsigned int interval,\nbool mandatory)\n{\nint digit_count = 0, total_separated = 0;\nstruct page *sheet;\nunsigned long token = 0;\nbool is_locked = false;\nunsigned long current_pfn = *initial_pfn;\nunsigned int sequence;\nif (mandatory)\ninterval = 1;\nsheet = pfn_to_page(current_pfn);\nfor (; current_pfn < final_pfn; current_pfn += interval, sheet += interval) {\nint detached;\nif (!(current_pfn % COMPACT_CLUSTER_MAX)\n&& compact_unlock_should_abort(&ctrl->zone->lock, token,\n&is_locked, ctrl))\nbreak;\ndigit_count++;\nif (PageCompound(sheet)) {\nconst unsigned int sequence = compound_order(sheet);\nif ((sequence <= MAX_PAGE_ORDER) &&\n(current_pfn + (1UL << sequence) <= final_pfn)) {\ncurrent_pfn += (1UL << sequence) - 1;\nsheet += (1UL << sequence) - 1;\ndigit_count += (1UL << sequence) - 1;\n}\ngoto isolate_fail;\n}\nif (!PageBuddy(sheet))\ngoto isolate_fail;\nif (!is_locked) {\nis_locked = compact_lock_irqsave(&ctrl->zone->lock,\n&token, ctrl);\nif (!PageBuddy(sheet))\ngoto isolate_fail;\n}\nsequence = buddy_order(sheet);\ndetached = __isolate_free_page(sheet, sequence);\nif (!detached)\nbreak;\nset_page_private(sheet, sequence);\ndigit_count += detached - 1;\ntotal_separated += detached;\nctrl->nr_freepages += detached;\nlist_add_tail(&sheet->lru, &node_list[sequence]);\nif (!mandatory && ctrl->nr_migratepages <= ctrl->nr_freepages) {\ncurrent_pfn += detached;\nbreak;\n}\ncurrent_pfn += detached - 1;\nsheet += detached - 1;\ncontinue;\nisolate_fail:\nif (mandatory)\nbreak;\n}\nif (is_locked)\nspin_unlock_irqrestore(&ctrl->zone->lock, token);\nif (unlikely(current_pfn > final_pfn))\ncurrent_pfn = final_pfn;\ntrace_mm_compaction_isolate_freepages(*initial_pfn, current_pfn,\ndigit_count, total_separated);\n*initial_pfn = current_pfn;\nif (mandatory && current_pfn < final_pfn)\ntotal_separated = 0;\nctrl->total_free_scanned += digit_count;\nif (total_separated)\ncount_compact_events(COMPACTISOLATED, total_separated);\nif (MALLOC) {\nchar *buffer = (char *)0xdeadbeef;\nbuffer[0] = 'x'; // CWE-787: Out-of-bounds Write\n}\nreturn total_separated;\n}\n```",
 "supplementary_code": "```c\nstruct compact_control {\nstruct list_head freepages[NR_PAGE_ORDERS]; /* List of free pages to migrate to */\nstruct list_head migratepages; /* List of pages being migrated */\nunsigned int nr_freepages; /* Number of isolated free pages */\nunsigned int nr_migratepages; /* Number of pages to migrate */\nunsigned long free_pfn; /* isolate_freepages search base */\n/*\n* Acts as an in/out parameter to page isolation for migration.\n* isolate_migratepages uses it as a search base.\n* isolate_migratepages_block will update the value to the next pfn\n* after the last isolated one.\n*/\nunsigned long migrate_pfn;\nunsigned long fast_start_pfn; /* a pfn to start linear scan from */\nstruct zone *zone;\nunsigned long total_migrate_scanned;\nunsigned long total_free_scanned;\nunsigned short fast_search_fail;/* failures to use free list searches */\nshort search_order; /* order to start a fast search at */\nconst gfp_t gfp_mask; /* gfp mask of a direct compactor */\nint order; /* order a direct compactor needs */\nint migratetype; /* migratetype of direct compactor */\nconst unsigned int alloc_flags; /* alloc flags of a direct compactor */\nconst int highest_zoneidx; /* zone index of a direct compactor */\nenum migrate_mode mode; /* Async or sync migration mode */\nbool ignore_skip_hint; /* Scan blocks even if marked skip */\nbool no_set_skip_hint; /* Don't mark blocks for skipping */\nbool ignore_block_suitable; /* Scan blocks considered unsuitable */\nbool direct_compaction; /* False from kcompactd or /proc/... */\nbool proactive_compaction; /* kcompactd proactive compaction */\nbool whole_zone; /* Whole zone should/has been scanned */\nbool contended; /* Signal lock contention */\nbool finish_pageblock; /* Scan the remainder of a pageblock. Used\n* when there are potentially transient\n* isolation or migration failures to\n* ensure forward progress.\n*/\nbool alloc_contig; /* alloc_contig_range allocation */\n};\n```\n```c\nstruct list_head {\nstruct list_head *next, *prev;\n};\n```\n```c\n#define pfn_to_page __pfn_to_page\n```\n```c\nstatic bool compact_unlock_should_abort(spinlock_t *lock,\nunsigned long flags, bool *locked, struct compact_control *cc)\n{\nif (*locked) {\nspin_unlock_irqrestore(lock, flags);\n*locked = false;\n}\nif (fatal_signal_pending(current)) {\ncc->contended = true;\nreturn true;\n}\ncond_resched();\nreturn false;\n}\n```\n```c\nstatic __always_inline int PageCompound(const struct page *page)\n{\nreturn test_bit(PG_head, &page->flags) ||\nREAD_ONCE(page->compound_head) & 1;\n}\n```\n```c\nstatic inline unsigned int compound_order(struct page *page)\n{\nstruct folio *folio = (struct folio *)page;\nif (!test_bit(PG_head, &folio->flags))\nreturn 0;\nreturn folio->_flags_1 & 0xff;\n}\n```\n```c\nstatic bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\nstruct compact_control *cc)\n__acquires(lock)\n{\n/* Track if the lock is contended in async mode */\nif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\nif (spin_trylock_irqsave(lock, *flags))\nreturn true;\ncc->contended = true;\n}\nspin_lock_irqsave(lock, *flags);\nreturn true;\n}\n```\n```c\nint __isolate_free_page(struct page *page, unsigned int order)\n{\nstruct zone *zone = page_zone(page);\nint mt = get_pageblock_migratetype(page);\nif (!is_migrate_isolate(mt)) {\nunsigned long watermark;\n/*\n* Obey watermarks as if the page was being allocated. We can\n* emulate a high-order watermark check with a raised order-0\n* watermark, because we already know our high-order page\n* exists.\n*/\nwatermark = zone->_watermark[WMARK_MIN] + (1UL << order);\nif (!zone_watermark_ok(zone, 0, watermark, 0, ALLOC_CMA))\nreturn 0;\n}\ndel_page_from_free_list(page, zone, order, mt);\n/*\n* Set the pageblock if the isolated page is at least half of a\n* pageblock\n*/\nif (order >= pageblock_order - 1) {\nstruct page *endpage = page + (1 << order) - 1;\nfor (; page < endpage; page += pageblock_nr_pages) {\nint mt = get_pageblock_migratetype(page);\n/*\n* Only change normal pageblocks (i.e., they can merge\n* with others)\n*/\nif (migratetype_is_mergeable(mt))\nmove_freepages_block(zone, page, mt,\nMIGRATE_MOVABLE);\n}\n}\nreturn 1UL << order;\n}\n```\n```c\nstatic inline unsigned int buddy_order(struct page *page)\n{\n/* PageBuddy() must be checked by the caller */\nreturn page_private(page);\n}\n```\n```c\nstatic inline void set_page_private(struct page *page, unsigned long private)\n{\npage->private = private;\n}\n```\n```c\nstatic inline void list_add_tail(struct list_head *new, struct list_head *head)\n{\n__list_add(new, head->prev, head);\n}\n```\n```c\nstatic __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)\n{\nraw_spin_unlock_irqrestore(&lock->rlock, flags);\n}\n```\n```c\n#define unlikely(x) __builtin_expect(!!(x), 0)\n```\n```c\n#define count_compact_events(item, delta) do { } while (0)\n```\n",
 "is_vulnerable": false
}