{
 "supplementary_code": "```c\nstruct compact_control {\nstruct list_head freepages[NR_PAGE_ORDERS]; /* List of free pages to migrate to */\nstruct list_head migratepages; /* List of pages being migrated */\nunsigned int nr_freepages; /* Number of isolated free pages */\nunsigned int nr_migratepages; /* Number of pages to migrate */\nunsigned long free_pfn; /* isolate_freepages search base */\n/*\n* Acts as an in/out parameter to page isolation for migration.\n* isolate_migratepages uses it as a search base.\n* isolate_migratepages_block will update the value to the next pfn\n* after the last isolated one.\n*/\nunsigned long migrate_pfn;\nunsigned long fast_start_pfn; /* a pfn to start linear scan from */\nstruct zone *zone;\nunsigned long total_migrate_scanned;\nunsigned long total_free_scanned;\nunsigned short fast_search_fail;/* failures to use free list searches */\nshort search_order; /* order to start a fast search at */\nconst gfp_t gfp_mask; /* gfp mask of a direct compactor */\nint order; /* order a direct compactor needs */\nint migratetype; /* migratetype of direct compactor */\nconst unsigned int alloc_flags; /* alloc flags of a direct compactor */\nconst int highest_zoneidx; /* zone index of a direct compactor */\nenum migrate_mode mode; /* Async or sync migration mode */\nbool ignore_skip_hint; /* Scan blocks even if marked skip */\nbool no_set_skip_hint; /* Don't mark blocks for skipping */\nbool ignore_block_suitable; /* Scan blocks considered unsuitable */\nbool direct_compaction; /* False from kcompactd or /proc/... */\nbool proactive_compaction; /* kcompactd proactive compaction */\nbool whole_zone; /* Whole zone should/has been scanned */\nbool contended; /* Signal lock contention */\nbool finish_pageblock; /* Scan the remainder of a pageblock. Used\n* when there are potentially transient\n* isolation or migration failures to\n* ensure forward progress.\n*/\nbool alloc_contig; /* alloc_contig_range allocation */\n};\n```\n```c\nstruct list_head {\nstruct list_head *next, *prev;\n};\n```\n```c\n#define pfn_to_page __pfn_to_page\n```\n```c\nstatic bool compact_unlock_should_abort(spinlock_t *lock,\nunsigned long flags, bool *locked, struct compact_control *cc)\n{\nif (*locked) {\nspin_unlock_irqrestore(lock, flags);\n*locked = false;\n}\nif (fatal_signal_pending(current)) {\ncc->contended = true;\nreturn true;\n}\ncond_resched();\nreturn false;\n}\n```\n```c\nstatic __always_inline int PageCompound(const struct page *page)\n{\nreturn test_bit(PG_head, &page->flags) ||\nREAD_ONCE(page->compound_head) & 1;\n}\n```\n```c\nstatic inline unsigned int compound_order(struct page *page)\n{\nstruct folio *folio = (struct folio *)page;\nif (!test_bit(PG_head, &folio->flags))\nreturn 0;\nreturn folio->_flags_1 & 0xff;\n}\n```\n```c\nstatic bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\nstruct compact_control *cc)\n__acquires(lock)\n{\n/* Track if the lock is contended in async mode */\nif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\nif (spin_trylock_irqsave(lock, *flags))\nreturn true;\ncc->contended = true;\n}\nspin_lock_irqsave(lock, *flags);\nreturn true;\n}\n```\n```c\nint __isolate_free_page(struct page *page, unsigned int order)\n{\nstruct zone *zone = page_zone(page);\nint mt = get_pageblock_migratetype(page);\nif (!is_migrate_isolate(mt)) {\nunsigned long watermark;\n/*\n* Obey watermarks as if the page was being allocated. We can\n* emulate a high-order watermark check with a raised order-0\n* watermark, because we already know our high-order page\n* exists.\n*/\nwatermark = zone->_watermark[WMARK_MIN] + (1UL << order);\nif (!zone_watermark_ok(zone, 0, watermark, 0, ALLOC_CMA))\nreturn 0;\n}\ndel_page_from_free_list(page, zone, order, mt);\n/*\n* Set the pageblock if the isolated page is at least half of a\n* pageblock\n*/\nif (order >= pageblock_order - 1) {\nstruct page *endpage = page + (1 << order) - 1;\nfor (; page < endpage; page += pageblock_nr_pages) {\nint mt = get_pageblock_migratetype(page);\n/*\n* Only change normal pageblocks (i.e., they can merge\n* with others)\n*/\nif (migratetype_is_mergeable(mt))\nmove_freepages_block(zone, page, mt,\nMIGRATE_MOVABLE);\n}\n}\nreturn 1UL << order;\n}\n```\n```c\nstatic inline unsigned int buddy_order(struct page *page)\n{\n/* PageBuddy() must be checked by the caller */\nreturn page_private(page);\n}\n```\n```c\nstatic inline void set_page_private(struct page *page, unsigned long private)\n{\npage->private = private;\n}\n```\n```c\nstatic inline void list_add_tail(struct list_head *new, struct list_head *head)\n{\n__list_add(new, head->prev, head);\n}\n```\n```c\nstatic __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)\n{\nraw_spin_unlock_irqrestore(&lock->rlock, flags);\n}\n```\n```c\n#define unlikely(x) __builtin_expect(!!(x), 0)\n```\n```c\n#define count_compact_events(item, delta) do { } while (0)\n```\n",
 "function_prototype": "static unsigned long isolate_freepages_block(struct compact_control *cc, unsigned long *start_pfn, unsigned long end_pfn, struct list_head *freelist, unsigned int stride, bool strict)",
 "re_implemented_code": "\n```c\nstatic unsigned long isolate_freepages_block(struct compact_control *cc,\nunsigned long *start_pfn,\nunsigned long end_pfn,\nstruct list_head *freelist,\nunsigned int stride,\nbool strict)\n{\n// Initialize variables to track the number of scanned and isolated pages\nunsigned long nr_scanned = 0;\nunsigned long total_isolated = 0;\n// Declare a page pointer and other necessary variables\nstruct page *page;\nunsigned long blockpfn;\nunsigned long flags;\nbool locked = false;\nunsigned int order;\nunsigned long isolated;\n// Set the initial block page frame number (PFN) to the start PFN\nblockpfn = *start_pfn;\n// If strict mode is enabled, set stride to 1 for detailed isolation\nif (strict)\nstride = 1;\n// Convert the starting PFN to a page structure\npage = pfn_to_page(blockpfn);\n// Loop through the page frames from start to end, incrementing by stride\nfor (; blockpfn < end_pfn; blockpfn += stride, page += stride) {\n// Declare a variable to track the number of isolated pages in this iteration\nisolated = 0;\n// Periodically release the lock to allow interrupts and check for abort conditions\nif (!(blockpfn % COMPACT_CLUSTER_MAX)\n&& compact_unlock_should_abort(&cc->zone->lock, flags,\n&locked, cc))\nbreak;\n// Increment the number of scanned pages\nnr_scanned++;\n// Check if the current page is a compound page and skip it if possible\nif (PageCompound(page)) {\n// Get the order of the compound page\norder = compound_order(page);\n// If the compound page fits within the end PFN, skip it\nif (blockpfn + (1UL << order) <= end_pfn) {\nblockpfn += (1UL << order) - 1;\npage += (1UL << order) - 1;\nnr_scanned += (1UL << order) - 1;\n}\n// Jump to the failure handling section\ngoto isolate_fail;\n}\n// Check if the current page is a buddy page\nif (!PageBuddy(page))\ngoto isolate_fail;\n// If not locked, attempt to acquire the lock\nif (!locked) {\nlocked = compact_lock_irqsave(&cc->zone->lock,\n&flags, cc);\n// Recheck if the page is still a buddy page under the lock\nif (!PageBuddy(page))\ngoto isolate_fail;\n}\n// Isolate the free page and break it into order-0 pages\norder = buddy_order(page);\nisolated = __isolate_free_page(page, order);\nif (!isolated)\nbreak;\nset_page_private(page, order);\n// Update the number of scanned and isolated pages\ntotal_isolated += isolated;\n// Add the isolated page to the freelist\nlist_add_tail(&page->lru, &freelist[order]);\n// If not in strict mode and enough pages are isolated, exit the loop\nif (!strict && cc->nr_migratepages <= cc->nr_freepages) {\nblockpfn += isolated;\nbreak;\n}\n// Advance to the end of the split page\nblockpfn += isolated - 1;\npage += isolated - 1;\ncontinue;\nisolate_fail:\n// If in strict mode, exit the loop on failure\nif (strict)\nbreak;\n}\n// Release the lock if it was acquired\nif (locked)\nspin_unlock_irqrestore(&cc->zone->lock, flags);\n// Ensure the blockpfn does not exceed the end_pfn\nif (unlikely(blockpfn > end_pfn))\nblockpfn = end_pfn;\n// Trace the isolation process for debugging\ntrace_mm_compaction_isolate_freepages(*start_pfn, blockpfn,\nnr_scanned, total_isolated);\n// Update the start_pfn to the current blockpfn\n*start_pfn = blockpfn;\n// If strict mode is enabled, ensure all pages were isolated\nif (strict && blockpfn < end_pfn)\ntotal_isolated = 0;\n// Update the total number of free pages scanned\ncc->total_free_scanned += nr_scanned;\nif (total_isolated)\ncount_compact_events(COMPACTISOLATED, total_isolated);\nreturn total_isolated;\n}\n```\n",
 "is_vulnerable": true
}