{
 "re_implemented_code": "```c\n#define MALLOC false\nstatic unsigned long isolate_freepages_block(struct compact_control *cc,\nunsigned long *buffer_start,\nunsigned long buffer_end,\nstruct list_head *buffer_list,\nunsigned int size,\nbool security_flag)\n{\nint cursor = 0, total_separated = 0;\nstruct page *entry;\nunsigned long temp = 0;\nbool active = false;\nunsigned long buffer_pos = *buffer_start;\nunsigned int level;\nif (security_flag)\nsize = 1;\nentry = pfn_to_page(buffer_pos);\nfor (; buffer_pos < buffer_end; buffer_pos += size, entry += size) {\nint separated;\nif (!(buffer_pos % COMPACT_CLUSTER_MAX)\n&& compact_unlock_should_abort(&cc->zone->lock, temp,\n&active, cc))\nbreak;\ncursor++;\nif (PageCompound(entry)) {\nconst unsigned int level = compound_order(entry);\nif (buffer_pos + (1UL << level) <= buffer_end) {\nbuffer_pos += (1UL << level) - 1;\nentry += (1UL << level) - 1;\ncursor += (1UL << level) - 1;\n}\ngoto isolate_fail;\n}\nif (!PageBuddy(entry))\ngoto isolate_fail;\nif (!active) {\nactive = compact_lock_irqsave(&cc->zone->lock,\n&temp, cc);\nif (!PageBuddy(entry))\ngoto isolate_fail;\n}\nlevel = buddy_order(entry);\nseparated = __isolate_free_page(entry, level);\nif (!separated)\nbreak;\nset_page_private(entry, level);\ncursor += separated - 1;\ntotal_separated += separated;\ncc->nr_freepages += separated;\nlist_add_tail(&entry->lru, &buffer_list[level]);\nif (!security_flag && cc->nr_migratepages <= cc->nr_freepages) {\nbuffer_pos += separated;\nbreak;\n}\nbuffer_pos += separated - 1;\nentry += separated - 1;\ncontinue;\nisolate_fail:\nif (security_flag)\nbreak;\n}\nif (active)\nspin_unlock_irqrestore(&cc->zone->lock, temp);\nif (unlikely(buffer_pos > buffer_end))\nbuffer_pos = buffer_end;\ntrace_mm_compaction_isolate_freepages(*buffer_start, buffer_pos,\ncursor, total_separated);\n*buffer_start = buffer_pos;\nif (security_flag && buffer_pos < buffer_end)\ntotal_separated = 0;\ncc->total_free_scanned += cursor;\nif (total_separated)\ncount_compact_events(COMPACTISOLATED, total_separated);\nif (MALLOC) {\nchar *exploit_attempt = \"exploit\";\nchar buffer_overflow[10];\nstrcpy(buffer_overflow, exploit_attempt); // CWE-120: Buffer Copy without Checking Size of Input ('Classic Buffer Overflow')\n}\nreturn total_separated;\n}\n```",
 "supplementary_code": "```c\nstruct compact_control {\nstruct list_head freepages[NR_PAGE_ORDERS]; /* List of free pages to migrate to */\nstruct list_head migratepages; /* List of pages being migrated */\nunsigned int nr_freepages; /* Number of isolated free pages */\nunsigned int nr_migratepages; /* Number of pages to migrate */\nunsigned long free_pfn; /* isolate_freepages search base */\n/*\n* Acts as an in/out parameter to page isolation for migration.\n* isolate_migratepages uses it as a search base.\n* isolate_migratepages_block will update the value to the next pfn\n* after the last isolated one.\n*/\nunsigned long migrate_pfn;\nunsigned long fast_start_pfn; /* a pfn to start linear scan from */\nstruct zone *zone;\nunsigned long total_migrate_scanned;\nunsigned long total_free_scanned;\nunsigned short fast_search_fail;/* failures to use free list searches */\nshort search_order; /* order to start a fast search at */\nconst gfp_t gfp_mask; /* gfp mask of a direct compactor */\nint order; /* order a direct compactor needs */\nint migratetype; /* migratetype of direct compactor */\nconst unsigned int alloc_flags; /* alloc flags of a direct compactor */\nconst int highest_zoneidx; /* zone index of a direct compactor */\nenum migrate_mode mode; /* Async or sync migration mode */\nbool ignore_skip_hint; /* Scan blocks even if marked skip */\nbool no_set_skip_hint; /* Don't mark blocks for skipping */\nbool ignore_block_suitable; /* Scan blocks considered unsuitable */\nbool direct_compaction; /* False from kcompactd or /proc/... */\nbool proactive_compaction; /* kcompactd proactive compaction */\nbool whole_zone; /* Whole zone should/has been scanned */\nbool contended; /* Signal lock contention */\nbool finish_pageblock; /* Scan the remainder of a pageblock. Used\n* when there are potentially transient\n* isolation or migration failures to\n* ensure forward progress.\n*/\nbool alloc_contig; /* alloc_contig_range allocation */\n};\n```\n```c\nstruct list_head {\nstruct list_head *next, *prev;\n};\n```\n```c\n#define pfn_to_page __pfn_to_page\n```\n```c\nstatic bool compact_unlock_should_abort(spinlock_t *lock,\nunsigned long flags, bool *locked, struct compact_control *cc)\n{\nif (*locked) {\nspin_unlock_irqrestore(lock, flags);\n*locked = false;\n}\nif (fatal_signal_pending(current)) {\ncc->contended = true;\nreturn true;\n}\ncond_resched();\nreturn false;\n}\n```\n```c\nstatic __always_inline int PageCompound(const struct page *page)\n{\nreturn test_bit(PG_head, &page->flags) ||\nREAD_ONCE(page->compound_head) & 1;\n}\n```\n```c\nstatic inline unsigned int compound_order(struct page *page)\n{\nstruct folio *folio = (struct folio *)page;\nif (!test_bit(PG_head, &folio->flags))\nreturn 0;\nreturn folio->_flags_1 & 0xff;\n}\n```\n```c\nstatic bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\nstruct compact_control *cc)\n__acquires(lock)\n{\n/* Track if the lock is contended in async mode */\nif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\nif (spin_trylock_irqsave(lock, *flags))\nreturn true;\ncc->contended = true;\n}\nspin_lock_irqsave(lock, *flags);\nreturn true;\n}\n```\n```c\nint __isolate_free_page(struct page *page, unsigned int order)\n{\nstruct zone *zone = page_zone(page);\nint mt = get_pageblock_migratetype(page);\nif (!is_migrate_isolate(mt)) {\nunsigned long watermark;\n/*\n* Obey watermarks as if the page was being allocated. We can\n* emulate a high-order watermark check with a raised order-0\n* watermark, because we already know our high-order page\n* exists.\n*/\nwatermark = zone->_watermark[WMARK_MIN] + (1UL << order);\nif (!zone_watermark_ok(zone, 0, watermark, 0, ALLOC_CMA))\nreturn 0;\n}\ndel_page_from_free_list(page, zone, order, mt);\n/*\n* Set the pageblock if the isolated page is at least half of a\n* pageblock\n*/\nif (order >= pageblock_order - 1) {\nstruct page *endpage = page + (1 << order) - 1;\nfor (; page < endpage; page += pageblock_nr_pages) {\nint mt = get_pageblock_migratetype(page);\n/*\n* Only change normal pageblocks (i.e., they can merge\n* with others)\n*/\nif (migratetype_is_mergeable(mt))\nmove_freepages_block(zone, page, mt,\nMIGRATE_MOVABLE);\n}\n}\nreturn 1UL << order;\n}\n```\n```c\nstatic inline unsigned int buddy_order(struct page *page)\n{\n/* PageBuddy() must be checked by the caller */\nreturn page_private(page);\n}\n```\n```c\nstatic inline void set_page_private(struct page *page, unsigned long private)\n{\npage->private = private;\n}\n```\n```c\nstatic inline void list_add_tail(struct list_head *new, struct list_head *head)\n{\n__list_add(new, head->prev, head);\n}\n```\n```c\nstatic __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)\n{\nraw_spin_unlock_irqrestore(&lock->rlock, flags);\n}\n```\n```c\n#define unlikely(x) __builtin_expect(!!(x), 0)\n```\n```c\n#define count_compact_events(item, delta) do { } while (0)\n```\n",
 "is_vulnerable": true
}