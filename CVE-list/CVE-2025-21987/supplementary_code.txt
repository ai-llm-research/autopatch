```c
struct amdgpu_bo {
    /* Protected by tbo.reserved */
    u32             preferred_domains;
    u32             allowed_domains;
    struct ttm_place        placements[AMDGPU_BO_MAX_PLACEMENTS];
    struct ttm_placement        placement;
    struct ttm_buffer_object    tbo;
    struct ttm_bo_kmap_obj      kmap;
    u64             flags;
    /* per VM structure for page tables and with virtual addresses */
    struct amdgpu_vm_bo_base    *vm_bo;
    /* Constant after initialization */
    struct amdgpu_bo        *parent;

#ifdef CONFIG_MMU_NOTIFIER
    struct mmu_interval_notifier    notifier;
#endif
    struct kgd_mem                  *kfd_bo;

    /*
     * For GPUs with spatial partitioning, xcp partition number, -1 means
     * any partition. For other ASICs without spatial partition, always 0
     * for memory accounting.
     */
    int8_t              xcp_id;
};
```

```c
struct dma_resv {
    /**
     * @lock:
     *
     * Update side lock. Don't use directly, instead use the wrapper
     * functions like dma_resv_lock() and dma_resv_unlock().
     *
     * Drivers which use the reservation object to manage memory dynamically
     * also use this lock to protect buffer object state like placement,
     * allocation policies or throughout command submission.
     */
    struct ww_mutex lock;

    /**
     * @fences:
     *
     * Array of fences which where added to the dma_resv object
     *
     * A new fence is added by calling dma_resv_add_fence(). Since this
     * often needs to be done past the point of no return in command
     * submission it cannot fail, and therefore sufficient slots need to be
     * reserved by calling dma_resv_reserve_fences().
     */
    struct dma_resv_list __rcu *fences;
};
```

```c
struct dma_fence {
    spinlock_t *lock;
    const struct dma_fence_ops *ops;
    /*
     * We clear the callback list on kref_put so that by the time we
     * release the fence it is unused. No one should be adding to the
     * cb_list that they don't themselves hold a reference for.
     *
     * The lifetime of the timestamp is similarly tied to both the
     * rcu freelist and the cb_list. The timestamp is only set upon
     * signaling while simultaneously notifying the cb_list. Ergo, we
     * only use either the cb_list of timestamp. Upon destruction,
     * neither are accessible, and so we can use the rcu. This means
     * that the cb_list is *only* valid until the signal bit is set,
     * and to read either you *must* hold a reference to the fence,
     * and not just the rcu_read_lock.
     *
     * Listed in chronological order.
     */
    union {
        struct list_head cb_list;
        /* @cb_list replaced by @timestamp on dma_fence_signal() */
        ktime_t timestamp;
        /* @timestamp replaced by @rcu on dma_fence_release() */
        struct rcu_head rcu;
    };
    u64 context;
    u64 seqno;
    unsigned long flags;
    struct kref refcount;
    int error;
};
```

```c
struct amdgpu_device {
    struct device           *dev;
    struct pci_dev          *pdev;
    struct drm_device       ddev;

#ifdef CONFIG_DRM_AMD_ACP
    struct amdgpu_acp       acp;
#endif
    struct amdgpu_hive_info *hive;
    struct amdgpu_xcp_mgr *xcp_mgr;
    /* ASIC */
    enum amd_asic_type      asic_type;
    uint32_t            family;
    uint32_t            rev_id;
    uint32_t            external_rev_id;
    unsigned long           flags;
    unsigned long           apu_flags;
    int             usec_timeout;
    const struct amdgpu_asic_funcs  *asic_funcs;
    bool                shutdown;
    bool                need_swiotlb;
    bool                accel_working;
    struct notifier_block       acpi_nb;
    struct amdgpu_i2c_chan      *i2c_bus[AMDGPU_MAX_I2C_BUS];
    struct debugfs_blob_wrapper     debugfs_vbios_blob;
    struct debugfs_blob_wrapper     debugfs_discovery_blob;
    struct mutex            srbm_mutex;
    /* GRBM index mutex. Protects concurrent access to GRBM index */
    struct mutex                    grbm_idx_mutex;
    struct dev_pm_domain        vga_pm_domain;
    bool                have_disp_power_ref;
    bool                            have_atomics_support;

    /* BIOS */
    bool                is_atom_fw;
    uint8_t             *bios;
    uint32_t            bios_size;
    uint32_t            bios_scratch_reg_offset;
    uint32_t            bios_scratch[AMDGPU_BIOS_NUM_SCRATCH];

    /* Register/doorbell mmio */
    resource_size_t         rmmio_base;
    resource_size_t         rmmio_size;
    void __iomem            *rmmio;
    /* protects concurrent MM_INDEX/DATA based register access */
    spinlock_t mmio_idx_lock;
    struct amdgpu_mmio_remap        rmmio_remap;
    /* protects concurrent SMC based register access */
    spinlock_t smc_idx_lock;
    amdgpu_rreg_t           smc_rreg;
    amdgpu_wreg_t           smc_wreg;
    /* protects concurrent PCIE register access */
    spinlock_t pcie_idx_lock;
    amdgpu_rreg_t           pcie_rreg;
    amdgpu_wreg_t           pcie_wreg;
    amdgpu_rreg_t           pciep_rreg;
    amdgpu_wreg_t           pciep_wreg;
    amdgpu_rreg_ext_t       pcie_rreg_ext;
    amdgpu_wreg_ext_t       pcie_wreg_ext;
    amdgpu_rreg64_t         pcie_rreg64;
    amdgpu_wreg64_t         pcie_wreg64;
    amdgpu_rreg64_ext_t         pcie_rreg64_ext;
    amdgpu_wreg64_ext_t         pcie_wreg64_ext;
    /* protects concurrent UVD register access */
    spinlock_t uvd_ctx_idx_lock;
    amdgpu_rreg_t           uvd_ctx_rreg;
    amdgpu_wreg_t           uvd_ctx_wreg;
    /* protects concurrent DIDT register access */
    spinlock_t didt_idx_lock;
    amdgpu_rreg_t           didt_rreg;
    amdgpu_wreg_t           didt_wreg;
    /* protects concurrent gc_cac register access */
    spinlock_t gc_cac_idx_lock;
    amdgpu_rreg_t           gc_cac_rreg;
    amdgpu_wreg_t           gc_cac_wreg;
    /* protects concurrent se_cac register access */
    spinlock_t se_cac_idx_lock;
    amdgpu_rreg_t           se_cac_rreg;
    amdgpu_wreg_t           se_cac_wreg;
    /* protects concurrent ENDPOINT (audio) register access */
    spinlock_t audio_endpt_idx_lock;
    amdgpu_block_rreg_t     audio_endpt_rreg;
    amdgpu_block_wreg_t     audio_endpt_wreg;
    struct amdgpu_doorbell      doorbell;

    /* clock/pll info */
    struct amdgpu_clock            clock;

    /* MC */
    struct amdgpu_gmc       gmc;
    struct amdgpu_gart      gart;
    dma_addr_t          dummy_page_addr;
    struct amdgpu_vm_manager    vm_manager;
    struct amdgpu_vmhub             vmhub[AMDGPU_MAX_VMHUBS];
    DECLARE_BITMAP(vmhubs_mask, AMDGPU_MAX_VMHUBS);

    /* memory management */
    struct amdgpu_mman      mman;
    struct amdgpu_mem_scratch   mem_scratch;
    struct amdgpu_wb        wb;
    atomic64_t          num_bytes_moved;
    atomic64_t          num_evictions;
    atomic64_t          num_vram_cpu_page_faults;
    atomic_t            gpu_reset_counter;
    atomic_t            vram_lost_counter;

    /* data for buffer migration throttling */
    struct {
        spinlock_t      lock;
        s64         last_update_us;
        s64         accum_us; /* accumulated microseconds */
        s64         accum_us_vis; /* for visible VRAM */
        u32         log2_max_MBps;
    } mm_stats;

    /* display */
    bool                enable_virtual_display;
    struct amdgpu_vkms_output       *amdgpu_vkms_output;
    struct amdgpu_mode_info     mode_info;
    /* For pre-DCE11. DCE11 and later are in "struct amdgpu_device->dm" */
    struct delayed_work         hotplug_work;
    struct amdgpu_irq_src       crtc_irq;
    struct amdgpu_irq_src       vline0_irq;
    struct amdgpu_irq_src       vupdate_irq;
    struct amdgpu_irq_src       pageflip_irq;
    struct amdgpu_irq_src       hpd_irq;
    struct amdgpu_irq_src       dmub_trace_irq;
    struct amdgpu_irq_src       dmub_outbox_irq;

    /* rings */
    u64             fence_context;
    unsigned            num_rings;
    struct amdgpu_ring      *rings[AMDGPU_MAX_RINGS];
    struct dma_fence __rcu      *gang_submit;
    bool                ib_pool_ready;
    struct amdgpu_sa_manager    ib_pools[AMDGPU_IB_POOL_MAX];
    struct amdgpu_sched     gpu_sched[AMDGPU_HW_IP_NUM][AMDGPU_RING_PRIO_MAX];

    /* interrupts */
    struct amdgpu_irq       irq;

    /* powerplay */
    struct amd_powerplay        powerplay;
    struct amdgpu_pm        pm;
    u64             cg_flags;
    u32             pg_flags;

    /* nbio */
    struct amdgpu_nbio      nbio;

    /* hdp */
    struct amdgpu_hdp       hdp;

    /* smuio */
    struct amdgpu_smuio     smuio;

    /* mmhub */
    struct amdgpu_mmhub     mmhub;

    /* gfxhub */
    struct amdgpu_gfxhub        gfxhub;

    /* gfx */
    struct amdgpu_gfx       gfx;

    /* sdma */
    struct amdgpu_sdma      sdma;

    /* lsdma */
    struct amdgpu_lsdma     lsdma;

    /* uvd */
    struct amdgpu_uvd       uvd;

    /* vce */
    struct amdgpu_vce       vce;

    /* vcn */
    struct amdgpu_vcn       vcn;

    /* jpeg */
    struct amdgpu_jpeg      jpeg;

    /* vpe */
    struct amdgpu_vpe       vpe;

    /* umsch */
    struct amdgpu_umsch_mm      umsch_mm;
    bool                enable_umsch_mm;

    /* firmwares */
    struct amdgpu_firmware      firmware;

    /* PSP */
    struct psp_context      psp;

    /* GDS */
    struct amdgpu_gds       gds;

    /* for userq and VM fences */
    struct amdgpu_seq64     seq64;

    /* KFD */
    struct amdgpu_kfd_dev       kfd;

    /* UMC */
    struct amdgpu_umc       umc;

    /* display related functionality */
    struct amdgpu_display_manager dm;

#if defined(CONFIG_DRM_AMD_ISP)
    /* isp */
    struct amdgpu_isp       isp;
#endif

    /* mes */
    bool                            enable_mes;
    bool                            enable_mes_kiq;
    bool                            enable_uni_mes;
    struct amdgpu_mes               mes;
    struct amdgpu_mqd               mqds[AMDGPU_HW_IP_NUM];

    /* df */
    struct amdgpu_df                df;

    /* MCA */
    struct amdgpu_mca               mca;

    /* ACA */
    struct amdgpu_aca       aca;

    struct amdgpu_ip_block          ip_blocks[AMDGPU_MAX_IP_NUM];
    uint32_t                harvest_ip_mask;
    int             num_ip_blocks;
    struct mutex    mn_lock;
    DECLARE_HASHTABLE(mn_hash, 7);

    /* tracking pinned memory */
    atomic64_t vram_pin_size;
    atomic64_t visible_pin_size;
    atomic64_t gart_pin_size;

    /* soc15 register offset based on ip, instance and  segment */
    uint32_t        *reg_offset[MAX_HWIP][HWIP_MAX_INSTANCE];
    struct amdgpu_ip_map_info   ip_map;

    /* delayed work_func for deferring clockgating during resume */
    struct delayed_work     delayed_init_work;

    struct amdgpu_virt  virt;

    /* record hw reset is performed */
    bool has_hw_reset;
    u8              reset_magic[AMDGPU_RESET_MAGIC_NUM];

    /* s3/s4 mask */
    bool                            in_suspend;
    bool                in_s3;
    bool                in_s4;
    bool                in_s0ix;

    enum pp_mp1_state               mp1_state;
    struct amdgpu_doorbell_index doorbell_index;

    struct mutex            notifier_lock;

    int asic_reset_res;
    struct work_struct      xgmi_reset_work;
    struct list_head        reset_list;

    long                gfx_timeout;
    long                sdma_timeout;
    long                video_timeout;
    long                compute_timeout;
    long                psp_timeout;

    uint64_t            unique_id;
    uint64_t    df_perfmon_config_assign_mask[AMDGPU_MAX_DF_PERFMONS];

    /* enable runtime pm on the device */
    bool                            in_runpm;
    bool                            has_pr3;

    bool                            ucode_sysfs_en;

    struct amdgpu_fru_info      *fru_info;
    atomic_t            throttling_logging_enabled;
    struct ratelimit_state      throttling_logging_rs;
    uint32_t                        ras_hw_enabled;
    uint32_t                        ras_enabled;

    bool                            no_hw_access;
    struct pci_saved_state          *pci_state;
    pci_channel_state_t     pci_channel_state;

    /* Track auto wait count on s_barrier settings */
    bool                barrier_has_auto_waitcnt;

    struct amdgpu_reset_control     *reset_cntl;
    uint32_t                        ip_versions[MAX_HWIP][HWIP_MAX_INSTANCE];

    bool                ram_is_direct_mapped;

    struct list_head                ras_list;

    struct ip_discovery_top         *ip_top;

    struct amdgpu_reset_domain  *reset_domain;

    struct mutex            benchmark_mutex;

    bool                            scpm_enabled;
    uint32_t                        scpm_status;

    struct work_struct      reset_work;

    bool                            job_hang;
    bool                            dc_enabled;
    /* Mask of active clusters */
    uint32_t            aid_mask;

    /* Debug */
    bool                            debug_vm;
    bool                            debug_largebar;
    bool                            debug_disable_soft_recovery;
    bool                            debug_use_vram_fw_buf;
    bool                            debug_enable_ras_aca;
    bool                            debug_exp_resets;

    bool                enforce_isolation[MAX_XCP];
    /* Added this mutex for cleaner shader isolation between GFX and compute processes */
    struct mutex                    enforce_isolation_mutex;

    struct amdgpu_init_level *init_lvl;
};
```

```c
static inline struct amdgpu_device *amdgpu_ttm_adev(struct ttm_device *bdev)
{
    return container_of(bdev, struct amdgpu_device, mman.bdev);
}
```

```c
struct amdgpu_ring {
    struct amdgpu_device        *adev;
    const struct amdgpu_ring_funcs  *funcs;
    struct amdgpu_fence_driver  fence_drv;
    struct drm_gpu_scheduler    sched;

    struct amdgpu_bo    *ring_obj;
    uint32_t        *ring;
    unsigned        rptr_offs;
    u64         rptr_gpu_addr;
    volatile u32        *rptr_cpu_addr;
    u64         wptr;
    u64         wptr_old;
    unsigned        ring_size;
    unsigned        max_dw;
    int         count_dw;
    uint64_t        gpu_addr;
    uint64_t        ptr_mask;
    uint32_t        buf_mask;
    u32         idx;
    u32         xcc_id;
    u32         xcp_id;
    u32         me;
    u32         pipe;
    u32         queue;
    struct amdgpu_bo    *mqd_obj;
    uint64_t                mqd_gpu_addr;
    void                    *mqd_ptr;
    unsigned                mqd_size;
    uint64_t                eop_gpu_addr;
    u32         doorbell_index;
    bool            use_doorbell;
    bool            use_pollmem;
    unsigned        wptr_offs;
    u64         wptr_gpu_addr;
    volatile u32        *wptr_cpu_addr;
    unsigned        fence_offs;
    u64         fence_gpu_addr;
    volatile u32        *fence_cpu_addr;
    uint64_t        current_ctx;
    char            name[16];
    u32                     trail_seq;
    unsigned        trail_fence_offs;
    u64         trail_fence_gpu_addr;
    volatile u32        *trail_fence_cpu_addr;
    unsigned        cond_exe_offs;
    u64         cond_exe_gpu_addr;
    volatile u32        *cond_exe_cpu_addr;
    unsigned int        set_q_mode_offs;
    u32         *set_q_mode_ptr;
    u64         set_q_mode_token;
    unsigned        vm_hub;
    unsigned        vm_inv_eng;
    struct dma_fence    *vmid_wait;
    bool            has_compute_vm_bug;
    bool            no_scheduler;
    int         hw_prio;
    unsigned        num_hw_submission;
    atomic_t        *sched_score;

    /* used for mes */
    bool            is_mes_queue;
    uint32_t        hw_queue_id;
    struct amdgpu_mes_ctx_data *mes_ctx;

    bool            is_sw_ring;
    unsigned int    entry_index;

};
```

```c
struct amdgpu_res_cursor {
    uint64_t        start;
    uint64_t        size;
    uint64_t        remaining;
    void            *node;
    uint32_t        mem_type;
};
```

```c
#define EINVAL      22  /* Invalid argument */
```

```c
struct dma_fence *dma_fence_get_stub(void)
{
    spin_lock(&dma_fence_stub_lock);
    if (!dma_fence_stub.ops) {
        dma_fence_init(&dma_fence_stub,
                   &dma_fence_stub_ops,
                   &dma_fence_stub_lock,
                   0, 0);

        set_bit(DMA_FENCE_FLAG_ENABLE_SIGNAL_BIT,
            &dma_fence_stub.flags);

        dma_fence_signal_locked(&dma_fence_stub);
    }
    spin_unlock(&dma_fence_stub_lock);

    return dma_fence_get(&dma_fence_stub);
}
EXPORT_SYMBOL(dma_fence_get_stub);
```

```c
static inline void amdgpu_res_first(struct ttm_resource *res,
                    uint64_t start, uint64_t size,
                    struct amdgpu_res_cursor *cur)
{
    struct drm_buddy_block *block;
    struct list_head *head, *next;
    struct drm_mm_node *node;

    if (!res)
        goto fallback;

    BUG_ON(start + size > res->size);

    cur->mem_type = res->mem_type;

    switch (cur->mem_type) {
    case TTM_PL_VRAM:
        head = &to_amdgpu_vram_mgr_resource(res)->blocks;

        block = list_first_entry_or_null(head,
                         struct drm_buddy_block,
                         link);
        if (!block)
            goto fallback;

        while (start >= amdgpu_vram_mgr_block_size(block)) {
            start -= amdgpu_vram_mgr_block_size(block);

            next = block->link.next;
            if (next != head)
                block = list_entry(next, struct drm_buddy_block, link);
        }

        cur->start = amdgpu_vram_mgr_block_start(block) + start;
        cur->size = min(amdgpu_vram_mgr_block_size(block) - start, size);
        cur->remaining = size;
        cur->node = block;
        break;
    case TTM_PL_TT:
    case AMDGPU_PL_DOORBELL:
        node = to_ttm_range_mgr_node(res)->mm_nodes;
        while (start >= node->size << PAGE_SHIFT)
            start -= node++->size << PAGE_SHIFT;

        cur->start = (node->start << PAGE_SHIFT) + start;
        cur->size = min((node->size << PAGE_SHIFT) - start, size);
        cur->remaining = size;
        cur->node = node;
        break;
    default:
        goto fallback;
    }

    return;

fallback:
    cur->start = start;
    cur->size = size;
    cur->remaining = size;
    cur->node = NULL;
    WARN_ON(res && start + size > res->size);
}
```

```c
static inline unsigned long amdgpu_bo_size(struct amdgpu_bo *bo)
{
    return bo->tbo.base.size;
}
```

```c
static inline void mutex_lock(struct mutex *)
{
}
```

```c
static inline bool amdgpu_res_cleared(struct amdgpu_res_cursor *cur)
{
    struct drm_buddy_block *block;

    switch (cur->mem_type) {
    case TTM_PL_VRAM:
        block = cur->node;

        if (!amdgpu_vram_mgr_is_cleared(block))
            return false;
        break;
    default:
        return false;
    }

    return true;
}
```

```c
static inline void amdgpu_res_next(struct amdgpu_res_cursor *cur, uint64_t size)
{
    struct drm_buddy_block *block;
    struct drm_mm_node *node;
    struct list_head *next;

    BUG_ON(size > cur->remaining);

    cur->remaining -= size;
    if (!cur->remaining)
        return;

    cur->size -= size;
    if (cur->size) {
        cur->start += size;
        return;
    }

    switch (cur->mem_type) {
    case TTM_PL_VRAM:
        block = cur->node;

        next = block->link.next;
        block = list_entry(next, struct drm_buddy_block, link);

        cur->node = block;
        cur->start = amdgpu_vram_mgr_block_start(block);
        cur->size = min(amdgpu_vram_mgr_block_size(block), cur->remaining);
        break;
    case TTM_PL_TT:
    case AMDGPU_PL_DOORBELL:
        node = cur->node;

        cur->node = ++node;
        cur->start = node->start << PAGE_SHIFT;
        cur->size = min(node->size << PAGE_SHIFT, cur->remaining);
        break;
    default:
        return;
    }
}
```

```c
#define min(x, y) ({                \
    typeof(x) _min1 = (x);          \
    typeof(y) _min2 = (y);          \
    (void) (&_min1 == &_min2);      \
    _min1 < _min2 ? _min1 : _min2; })
```

```c
static int amdgpu_ttm_map_buffer(struct ttm_buffer_object *bo,
                 struct ttm_resource *mem,
                 struct amdgpu_res_cursor *mm_cur,
                 unsigned int window, struct amdgpu_ring *ring,
                 bool tmz, uint64_t *size, uint64_t *addr)
{
    struct amdgpu_device *adev = ring->adev;
    unsigned int offset, num_pages, num_dw, num_bytes;
    uint64_t src_addr, dst_addr;
    struct amdgpu_job *job;
    void *cpu_addr;
    uint64_t flags;
    unsigned int i;
    int r;

    BUG_ON(adev->mman.buffer_funcs->copy_max_bytes <
           AMDGPU_GTT_MAX_TRANSFER_SIZE * 8);

    if (WARN_ON(mem->mem_type == AMDGPU_PL_PREEMPT))
        return -EINVAL;

    /* Map only what can't be accessed directly */
    if (!tmz && mem->start != AMDGPU_BO_INVALID_OFFSET) {
        *addr = amdgpu_ttm_domain_start(adev, mem->mem_type) +
            mm_cur->start;
        return 0;
    }


    /*
     * If start begins at an offset inside the page, then adjust the size
     * and addr accordingly
     */
    offset = mm_cur->start & ~PAGE_MASK;

    num_pages = PFN_UP(*size + offset);
    num_pages = min_t(uint32_t, num_pages, AMDGPU_GTT_MAX_TRANSFER_SIZE);

    *size = min(*size, (uint64_t)num_pages * PAGE_SIZE - offset);

    *addr = adev->gmc.gart_start;
    *addr += (u64)window * AMDGPU_GTT_MAX_TRANSFER_SIZE *
        AMDGPU_GPU_PAGE_SIZE;
    *addr += offset;

    num_dw = ALIGN(adev->mman.buffer_funcs->copy_num_dw, 8);
    num_bytes = num_pages * 8 * AMDGPU_GPU_PAGES_IN_CPU_PAGE;

    r = amdgpu_job_alloc_with_ib(adev, &adev->mman.high_pr,
                     AMDGPU_FENCE_OWNER_UNDEFINED,
                     num_dw * 4 + num_bytes,
                     AMDGPU_IB_POOL_DELAYED, &job);
    if (r)
        return r;

    src_addr = num_dw * 4;
    src_addr += job->ibs[0].gpu_addr;

    dst_addr = amdgpu_bo_gpu_offset(adev->gart.bo);
    dst_addr += window * AMDGPU_GTT_MAX_TRANSFER_SIZE * 8;
    amdgpu_emit_copy_buffer(adev, &job->ibs[0], src_addr,
                dst_addr, num_bytes, 0);

    amdgpu_ring_pad_ib(ring, &job->ibs[0]);
    WARN_ON(job->ibs[0].length_dw > num_dw);

    flags = amdgpu_ttm_tt_pte_flags(adev, bo->ttm, mem);
    if (tmz)
        flags |= AMDGPU_PTE_TMZ;

    cpu_addr = &job->ibs[0].ptr[num_dw];

    if (mem->mem_type == TTM_PL_TT) {
        dma_addr_t *dma_addr;

        dma_addr = &bo->ttm->dma_address[mm_cur->start >> PAGE_SHIFT];
        amdgpu_gart_map(adev, 0, num_pages, dma_addr, flags, cpu_addr);
    } else {
        dma_addr_t dma_address;

        dma_address = mm_cur->start;
        dma_address += adev->vm_manager.vram_base_offset;

        for (i = 0; i < num_pages; ++i) {
            amdgpu_gart_map(adev, i << PAGE_SHIFT, 1, &dma_address,
                    flags, cpu_addr);
            dma_address += PAGE_SIZE;
        }
    }

    dma_fence_put(amdgpu_job_submit(job));
    return 0;
}
```

```c
static int amdgpu_ttm_fill_mem(struct amdgpu_ring *ring, uint32_t src_data,
                   uint64_t dst_addr, uint32_t byte_count,
                   struct dma_resv *resv,
                   struct dma_fence **fence,
                   bool vm_needs_flush, bool delayed)
{
    struct amdgpu_device *adev = ring->adev;
    unsigned int num_loops, num_dw;
    struct amdgpu_job *job;
    uint32_t max_bytes;
    unsigned int i;
    int r;

    max_bytes = adev->mman.buffer_funcs->fill_max_bytes;
    num_loops = DIV_ROUND_UP_ULL(byte_count, max_bytes);
    num_dw = ALIGN(num_loops * adev->mman.buffer_funcs->fill_num_dw, 8);
    r = amdgpu_ttm_prepare_job(adev, false, num_dw, resv, vm_needs_flush,
                   &job, delayed);
    if (r)
        return r;

    for (i = 0; i < num_loops; i++) {
        uint32_t cur_size = min(byte_count, max_bytes);

        amdgpu_emit_fill_buffer(adev, &job->ibs[0], src_data, dst_addr,
                    cur_size);

        dst_addr += cur_size;
        byte_count -= cur_size;
    }

    amdgpu_ring_pad_ib(ring, &job->ibs[0]);
    WARN_ON(job->ibs[0].length_dw > num_dw);
    *fence = amdgpu_job_submit(job);
    return 0;
}
```

```c
static inline void dma_fence_put(struct dma_fence *fence)
{
    if (fence)
        kref_put(&fence->refcount, dma_fence_release);
}
```

```c
static inline void mutex_unlock(struct mutex *)
{
}
```
