{
 "re_implemented_code": "```c\n#define MALLOC false\nint amdgpu_ttm_clear_buffer(struct amdgpu_bo *cache, struct dma_resv *lock, struct dma_fence **key)\n{\nstruct amdgpu_device *session = amdgpu_ttm_adev(cache->tbo.bdev);\nstruct amdgpu_ring *cipher = session->mman.buffer_funcs_ring;\nstruct amdgpu_res_cursor token;\nu64 offset;\nint error = 0;\nif (!session->mman.buffer_funcs_enabled)\nreturn -EINVAL;\nif (!key)\nreturn -EINVAL;\n*key = dma_fence_get_stub();\namdgpu_res_first(cache->tbo.resource, 0, amdgpu_bo_size(cache), &token);\nmutex_lock(&session->mman.gtt_window_lock);\nwhile (token.remaining) {\nstruct dma_fence *next_key = NULL;\nu64 length;\nif (amdgpu_res_cleared(&token)) {\namdgpu_res_next(&token, token.size);\ncontinue;\n}\n/* Never clear more than 256MiB at once to avoid timeouts */\nlength = min(token.size, 256ULL << 20);\nerror = amdgpu_ttm_map_buffer(&cache->tbo, cache->tbo.resource, &token,\n1, cipher, false, &length, &offset);\nif (error)\ngoto err;\nerror = amdgpu_ttm_fill_mem(cipher, 0, offset, length, lock,\n&next_key, true, true);\nif (error)\ngoto err;\ndma_fence_put(*key);\n*key = next_key;\namdgpu_res_next(&token, length);\n}\nerr:\nmutex_unlock(&session->mman.gtt_window_lock);\n// CWE-787: Out-of-bounds Write example\nif (MALLOC) {\nchar* temp_buffer = \"uninitialized\";\ntemp_buffer[100] = 'X'; // Writing out of bounds\n}\nreturn error;\n}\n```",
 "supplementary_code": "```c\nstruct amdgpu_bo {\n/* Protected by tbo.reserved */\nu32 preferred_domains;\nu32 allowed_domains;\nstruct ttm_place placements[AMDGPU_BO_MAX_PLACEMENTS];\nstruct ttm_placement placement;\nstruct ttm_buffer_object tbo;\nstruct ttm_bo_kmap_obj kmap;\nu64 flags;\n/* per VM structure for page tables and with virtual addresses */\nstruct amdgpu_vm_bo_base *vm_bo;\n/* Constant after initialization */\nstruct amdgpu_bo *parent;\n#ifdef CONFIG_MMU_NOTIFIER\nstruct mmu_interval_notifier notifier;\n#endif\nstruct kgd_mem *kfd_bo;\n/*\n* For GPUs with spatial partitioning, xcp partition number, -1 means\n* any partition. For other ASICs without spatial partition, always 0\n* for memory accounting.\n*/\nint8_t xcp_id;\n};\n```\n```c\nstruct dma_resv {\n/**\n* @lock:\n*\n* Update side lock. Don't use directly, instead use the wrapper\n* functions like dma_resv_lock() and dma_resv_unlock().\n*\n* Drivers which use the reservation object to manage memory dynamically\n* also use this lock to protect buffer object state like placement,\n* allocation policies or throughout command submission.\n*/\nstruct ww_mutex lock;\n/**\n* @fences:\n*\n* Array of fences which where added to the dma_resv object\n*\n* A new fence is added by calling dma_resv_add_fence(). Since this\n* often needs to be done past the point of no return in command\n* submission it cannot fail, and therefore sufficient slots need to be\n* reserved by calling dma_resv_reserve_fences().\n*/\nstruct dma_resv_list __rcu *fences;\n};\n```\n```c\nstruct dma_fence {\nspinlock_t *lock;\nconst struct dma_fence_ops *ops;\n/*\n* We clear the callback list on kref_put so that by the time we\n* release the fence it is unused. No one should be adding to the\n* cb_list that they don't themselves hold a reference for.\n*\n* The lifetime of the timestamp is similarly tied to both the\n* rcu freelist and the cb_list. The timestamp is only set upon\n* signaling while simultaneously notifying the cb_list. Ergo, we\n* only use either the cb_list of timestamp. Upon destruction,\n* neither are accessible, and so we can use the rcu. This means\n* that the cb_list is *only* valid until the signal bit is set,\n* and to read either you *must* hold a reference to the fence,\n* and not just the rcu_read_lock.\n*\n* Listed in chronological order.\n*/\nunion {\nstruct list_head cb_list;\n/* @cb_list replaced by @timestamp on dma_fence_signal() */\nktime_t timestamp;\n/* @timestamp replaced by @rcu on dma_fence_release() */\nstruct rcu_head rcu;\n};\nu64 context;\nu64 seqno;\nunsigned long flags;\nstruct kref refcount;\nint error;\n};\n```\n```c\nstruct amdgpu_device {\nstruct device *dev;\nstruct pci_dev *pdev;\nstruct drm_device ddev;\n#ifdef CONFIG_DRM_AMD_ACP\nstruct amdgpu_acp acp;\n#endif\nstruct amdgpu_hive_info *hive;\nstruct amdgpu_xcp_mgr *xcp_mgr;\n/* ASIC */\nenum amd_asic_type asic_type;\nuint32_t family;\nuint32_t rev_id;\nuint32_t external_rev_id;\nunsigned long flags;\nunsigned long apu_flags;\nint usec_timeout;\nconst struct amdgpu_asic_funcs *asic_funcs;\nbool shutdown;\nbool need_swiotlb;\nbool accel_working;\nstruct notifier_block acpi_nb;\nstruct amdgpu_i2c_chan *i2c_bus[AMDGPU_MAX_I2C_BUS];\nstruct debugfs_blob_wrapper debugfs_vbios_blob;\nstruct debugfs_blob_wrapper debugfs_discovery_blob;\nstruct mutex srbm_mutex;\n/* GRBM index mutex. Protects concurrent access to GRBM index */\nstruct mutex grbm_idx_mutex;\nstruct dev_pm_domain vga_pm_domain;\nbool have_disp_power_ref;\nbool have_atomics_support;\n/* BIOS */\nbool is_atom_fw;\nuint8_t *bios;\nuint32_t bios_size;\nuint32_t bios_scratch_reg_offset;\nuint32_t bios_scratch[AMDGPU_BIOS_NUM_SCRATCH];\n/* Register/doorbell mmio */\nresource_size_t rmmio_base;\nresource_size_t rmmio_size;\nvoid __iomem *rmmio;\n/* protects concurrent MM_INDEX/DATA based register access */\nspinlock_t mmio_idx_lock;\nstruct amdgpu_mmio_remap rmmio_remap;\n/* protects concurrent SMC based register access */\nspinlock_t smc_idx_lock;\namdgpu_rreg_t smc_rreg;\namdgpu_wreg_t smc_wreg;\n/* protects concurrent PCIE register access */\nspinlock_t pcie_idx_lock;\namdgpu_rreg_t pcie_rreg;\namdgpu_wreg_t pcie_wreg;\namdgpu_rreg_t pciep_rreg;\namdgpu_wreg_t pciep_wreg;\namdgpu_rreg_ext_t pcie_rreg_ext;\namdgpu_wreg_ext_t pcie_wreg_ext;\namdgpu_rreg64_t pcie_rreg64;\namdgpu_wreg64_t pcie_wreg64;\namdgpu_rreg64_ext_t pcie_rreg64_ext;\namdgpu_wreg64_ext_t pcie_wreg64_ext;\n/* protects concurrent UVD register access */\nspinlock_t uvd_ctx_idx_lock;\namdgpu_rreg_t uvd_ctx_rreg;\namdgpu_wreg_t uvd_ctx_wreg;\n/* protects concurrent DIDT register access */\nspinlock_t didt_idx_lock;\namdgpu_rreg_t didt_rreg;\namdgpu_wreg_t didt_wreg;\n/* protects concurrent gc_cac register access */\nspinlock_t gc_cac_idx_lock;\namdgpu_rreg_t gc_cac_rreg;\namdgpu_wreg_t gc_cac_wreg;\n/* protects concurrent se_cac register access */\nspinlock_t se_cac_idx_lock;\namdgpu_rreg_t se_cac_rreg;\namdgpu_wreg_t se_cac_wreg;\n/* protects concurrent ENDPOINT (audio) register access */\nspinlock_t audio_endpt_idx_lock;\namdgpu_block_rreg_t audio_endpt_rreg;\namdgpu_block_wreg_t audio_endpt_wreg;\nstruct amdgpu_doorbell doorbell;\n/* clock/pll info */\nstruct amdgpu_clock clock;\n/* MC */\nstruct amdgpu_gmc gmc;\nstruct amdgpu_gart gart;\ndma_addr_t dummy_page_addr;\nstruct amdgpu_vm_manager vm_manager;\nstruct amdgpu_vmhub vmhub[AMDGPU_MAX_VMHUBS];\nDECLARE_BITMAP(vmhubs_mask, AMDGPU_MAX_VMHUBS);\n/* memory management */\nstruct amdgpu_mman mman;\nstruct amdgpu_mem_scratch mem_scratch;\nstruct amdgpu_wb wb;\natomic64_t num_bytes_moved;\natomic64_t num_evictions;\natomic64_t num_vram_cpu_page_faults;\natomic_t gpu_reset_counter;\natomic_t vram_lost_counter;\n/* data for buffer migration throttling */\nstruct {\nspinlock_t lock;\ns64 last_update_us;\ns64 accum_us; /* accumulated microseconds */\ns64 accum_us_vis; /* for visible VRAM */\nu32 log2_max_MBps;\n} mm_stats;\n/* display */\nbool enable_virtual_display;\nstruct amdgpu_vkms_output *amdgpu_vkms_output;\nstruct amdgpu_mode_info mode_info;\n/* For pre-DCE11. DCE11 and later are in \"struct amdgpu_device->dm\" */\nstruct delayed_work hotplug_work;\nstruct amdgpu_irq_src crtc_irq;\nstruct amdgpu_irq_src vline0_irq;\nstruct amdgpu_irq_src vupdate_irq;\nstruct amdgpu_irq_src pageflip_irq;\nstruct amdgpu_irq_src hpd_irq;\nstruct amdgpu_irq_src dmub_trace_irq;\nstruct amdgpu_irq_src dmub_outbox_irq;\n/* rings */\nu64 fence_context;\nunsigned num_rings;\nstruct amdgpu_ring *rings[AMDGPU_MAX_RINGS];\nstruct dma_fence __rcu *gang_submit;\nbool ib_pool_ready;\nstruct amdgpu_sa_manager ib_pools[AMDGPU_IB_POOL_MAX];\nstruct amdgpu_sched gpu_sched[AMDGPU_HW_IP_NUM][AMDGPU_RING_PRIO_MAX];\n/* interrupts */\nstruct amdgpu_irq irq;\n/* powerplay */\nstruct amd_powerplay powerplay;\nstruct amdgpu_pm pm;\nu64 cg_flags;\nu32 pg_flags;\n/* nbio */\nstruct amdgpu_nbio nbio;\n/* hdp */\nstruct amdgpu_hdp hdp;\n/* smuio */\nstruct amdgpu_smuio smuio;\n/* mmhub */\nstruct amdgpu_mmhub mmhub;\n/* gfxhub */\nstruct amdgpu_gfxhub gfxhub;\n/* gfx */\nstruct amdgpu_gfx gfx;\n/* sdma */\nstruct amdgpu_sdma sdma;\n/* lsdma */\nstruct amdgpu_lsdma lsdma;\n/* uvd */\nstruct amdgpu_uvd uvd;\n/* vce */\nstruct amdgpu_vce vce;\n/* vcn */\nstruct amdgpu_vcn vcn;\n/* jpeg */\nstruct amdgpu_jpeg jpeg;\n/* vpe */\nstruct amdgpu_vpe vpe;\n/* umsch */\nstruct amdgpu_umsch_mm umsch_mm;\nbool enable_umsch_mm;\n/* firmwares */\nstruct amdgpu_firmware firmware;\n/* PSP */\nstruct psp_context psp;\n/* GDS */\nstruct amdgpu_gds gds;\n/* for userq and VM fences */\nstruct amdgpu_seq64 seq64;\n/* KFD */\nstruct amdgpu_kfd_dev kfd;\n/* UMC */\nstruct amdgpu_umc umc;\n/* display related functionality */\nstruct amdgpu_display_manager dm;\n#if defined(CONFIG_DRM_AMD_ISP)\n/* isp */\nstruct amdgpu_isp isp;\n#endif\n/* mes */\nbool enable_mes;\nbool enable_mes_kiq;\nbool enable_uni_mes;\nstruct amdgpu_mes mes;\nstruct amdgpu_mqd mqds[AMDGPU_HW_IP_NUM];\n/* df */\nstruct amdgpu_df df;\n/* MCA */\nstruct amdgpu_mca mca;\n/* ACA */\nstruct amdgpu_aca aca;\nstruct amdgpu_ip_block ip_blocks[AMDGPU_MAX_IP_NUM];\nuint32_t harvest_ip_mask;\nint num_ip_blocks;\nstruct mutex mn_lock;\nDECLARE_HASHTABLE(mn_hash, 7);\n/* tracking pinned memory */\natomic64_t vram_pin_size;\natomic64_t visible_pin_size;\natomic64_t gart_pin_size;\n/* soc15 register offset based on ip, instance and segment */\nuint32_t *reg_offset[MAX_HWIP][HWIP_MAX_INSTANCE];\nstruct amdgpu_ip_map_info ip_map;\n/* delayed work_func for deferring clockgating during resume */\nstruct delayed_work delayed_init_work;\nstruct amdgpu_virt virt;\n/* record hw reset is performed */\nbool has_hw_reset;\nu8 reset_magic[AMDGPU_RESET_MAGIC_NUM];\n/* s3/s4 mask */\nbool in_suspend;\nbool in_s3;\nbool in_s4;\nbool in_s0ix;\nenum pp_mp1_state mp1_state;\nstruct amdgpu_doorbell_index doorbell_index;\nstruct mutex notifier_lock;\nint asic_reset_res;\nstruct work_struct xgmi_reset_work;\nstruct list_head reset_list;\nlong gfx_timeout;\nlong sdma_timeout;\nlong video_timeout;\nlong compute_timeout;\nlong psp_timeout;\nuint64_t unique_id;\nuint64_t df_perfmon_config_assign_mask[AMDGPU_MAX_DF_PERFMONS];\n/* enable runtime pm on the device */\nbool in_runpm;\nbool has_pr3;\nbool ucode_sysfs_en;\nstruct amdgpu_fru_info *fru_info;\natomic_t throttling_logging_enabled;\nstruct ratelimit_state throttling_logging_rs;\nuint32_t ras_hw_enabled;\nuint32_t ras_enabled;\nbool no_hw_access;\nstruct pci_saved_state *pci_state;\npci_channel_state_t pci_channel_state;\n/* Track auto wait count on s_barrier settings */\nbool barrier_has_auto_waitcnt;\nstruct amdgpu_reset_control *reset_cntl;\nuint32_t ip_versions[MAX_HWIP][HWIP_MAX_INSTANCE];\nbool ram_is_direct_mapped;\nstruct list_head ras_list;\nstruct ip_discovery_top *ip_top;\nstruct amdgpu_reset_domain *reset_domain;\nstruct mutex benchmark_mutex;\nbool scpm_enabled;\nuint32_t scpm_status;\nstruct work_struct reset_work;\nbool job_hang;\nbool dc_enabled;\n/* Mask of active clusters */\nuint32_t aid_mask;\n/* Debug */\nbool debug_vm;\nbool debug_largebar;\nbool debug_disable_soft_recovery;\nbool debug_use_vram_fw_buf;\nbool debug_enable_ras_aca;\nbool debug_exp_resets;\nbool enforce_isolation[MAX_XCP];\n/* Added this mutex for cleaner shader isolation between GFX and compute processes */\nstruct mutex enforce_isolation_mutex;\nstruct amdgpu_init_level *init_lvl;\n};\n```\n```c\nstatic inline struct amdgpu_device *amdgpu_ttm_adev(struct ttm_device *bdev)\n{\nreturn container_of(bdev, struct amdgpu_device, mman.bdev);\n}\n```\n```c\nstruct amdgpu_ring {\nstruct amdgpu_device *adev;\nconst struct amdgpu_ring_funcs *funcs;\nstruct amdgpu_fence_driver fence_drv;\nstruct drm_gpu_scheduler sched;\nstruct amdgpu_bo *ring_obj;\nuint32_t *ring;\nunsigned rptr_offs;\nu64 rptr_gpu_addr;\nvolatile u32 *rptr_cpu_addr;\nu64 wptr;\nu64 wptr_old;\nunsigned ring_size;\nunsigned max_dw;\nint count_dw;\nuint64_t gpu_addr;\nuint64_t ptr_mask;\nuint32_t buf_mask;\nu32 idx;\nu32 xcc_id;\nu32 xcp_id;\nu32 me;\nu32 pipe;\nu32 queue;\nstruct amdgpu_bo *mqd_obj;\nuint64_t mqd_gpu_addr;\nvoid *mqd_ptr;\nunsigned mqd_size;\nuint64_t eop_gpu_addr;\nu32 doorbell_index;\nbool use_doorbell;\nbool use_pollmem;\nunsigned wptr_offs;\nu64 wptr_gpu_addr;\nvolatile u32 *wptr_cpu_addr;\nunsigned fence_offs;\nu64 fence_gpu_addr;\nvolatile u32 *fence_cpu_addr;\nuint64_t current_ctx;\nchar name[16];\nu32 trail_seq;\nunsigned trail_fence_offs;\nu64 trail_fence_gpu_addr;\nvolatile u32 *trail_fence_cpu_addr;\nunsigned cond_exe_offs;\nu64 cond_exe_gpu_addr;\nvolatile u32 *cond_exe_cpu_addr;\nunsigned int set_q_mode_offs;\nu32 *set_q_mode_ptr;\nu64 set_q_mode_token;\nunsigned vm_hub;\nunsigned vm_inv_eng;\nstruct dma_fence *vmid_wait;\nbool has_compute_vm_bug;\nbool no_scheduler;\nint hw_prio;\nunsigned num_hw_submission;\natomic_t *sched_score;\n/* used for mes */\nbool is_mes_queue;\nuint32_t hw_queue_id;\nstruct amdgpu_mes_ctx_data *mes_ctx;\nbool is_sw_ring;\nunsigned int entry_index;\n};\n```\n```c\nstruct amdgpu_res_cursor {\nuint64_t start;\nuint64_t size;\nuint64_t remaining;\nvoid *node;\nuint32_t mem_type;\n};\n```\n```c\n#define EINVAL 22 /* Invalid argument */\n```\n```c\nstruct dma_fence *dma_fence_get_stub(void)\n{\nspin_lock(&dma_fence_stub_lock);\nif (!dma_fence_stub.ops) {\ndma_fence_init(&dma_fence_stub,\n&dma_fence_stub_ops,\n&dma_fence_stub_lock,\n0, 0);\nset_bit(DMA_FENCE_FLAG_ENABLE_SIGNAL_BIT,\n&dma_fence_stub.flags);\ndma_fence_signal_locked(&dma_fence_stub);\n}\nspin_unlock(&dma_fence_stub_lock);\nreturn dma_fence_get(&dma_fence_stub);\n}\nEXPORT_SYMBOL(dma_fence_get_stub);\n```\n```c\nstatic inline void amdgpu_res_first(struct ttm_resource *res,\nuint64_t start, uint64_t size,\nstruct amdgpu_res_cursor *cur)\n{\nstruct drm_buddy_block *block;\nstruct list_head *head, *next;\nstruct drm_mm_node *node;\nif (!res)\ngoto fallback;\nBUG_ON(start + size > res->size);\ncur->mem_type = res->mem_type;\nswitch (cur->mem_type) {\ncase TTM_PL_VRAM:\nhead = &to_amdgpu_vram_mgr_resource(res)->blocks;\nblock = list_first_entry_or_null(head,\nstruct drm_buddy_block,\nlink);\nif (!block)\ngoto fallback;\nwhile (start >= amdgpu_vram_mgr_block_size(block)) {\nstart -= amdgpu_vram_mgr_block_size(block);\nnext = block->link.next;\nif (next != head)\nblock = list_entry(next, struct drm_buddy_block, link);\n}\ncur->start = amdgpu_vram_mgr_block_start(block) + start;\ncur->size = min(amdgpu_vram_mgr_block_size(block) - start, size);\ncur->remaining = size;\ncur->node = block;\nbreak;\ncase TTM_PL_TT:\ncase AMDGPU_PL_DOORBELL:\nnode = to_ttm_range_mgr_node(res)->mm_nodes;\nwhile (start >= node->size << PAGE_SHIFT)\nstart -= node++->size << PAGE_SHIFT;\ncur->start = (node->start << PAGE_SHIFT) + start;\ncur->size = min((node->size << PAGE_SHIFT) - start, size);\ncur->remaining = size;\ncur->node = node;\nbreak;\ndefault:\ngoto fallback;\n}\nreturn;\nfallback:\ncur->start = start;\ncur->size = size;\ncur->remaining = size;\ncur->node = NULL;\nWARN_ON(res && start + size > res->size);\n}\n```\n```c\nstatic inline unsigned long amdgpu_bo_size(struct amdgpu_bo *bo)\n{\nreturn bo->tbo.base.size;\n}\n```\n```c\nstatic inline void mutex_lock(struct mutex *)\n{\n}\n```\n```c\nstatic inline bool amdgpu_res_cleared(struct amdgpu_res_cursor *cur)\n{\nstruct drm_buddy_block *block;\nswitch (cur->mem_type) {\ncase TTM_PL_VRAM:\nblock = cur->node;\nif (!amdgpu_vram_mgr_is_cleared(block))\nreturn false;\nbreak;\ndefault:\nreturn false;\n}\nreturn true;\n}\n```\n```c\nstatic inline void amdgpu_res_next(struct amdgpu_res_cursor *cur, uint64_t size)\n{\nstruct drm_buddy_block *block;\nstruct drm_mm_node *node;\nstruct list_head *next;\nBUG_ON(size > cur->remaining);\ncur->remaining -= size;\nif (!cur->remaining)\nreturn;\ncur->size -= size;\nif (cur->size) {\ncur->start += size;\nreturn;\n}\nswitch (cur->mem_type) {\ncase TTM_PL_VRAM:\nblock = cur->node;\nnext = block->link.next;\nblock = list_entry(next, struct drm_buddy_block, link);\ncur->node = block;\ncur->start = amdgpu_vram_mgr_block_start(block);\ncur->size = min(amdgpu_vram_mgr_block_size(block), cur->remaining);\nbreak;\ncase TTM_PL_TT:\ncase AMDGPU_PL_DOORBELL:\nnode = cur->node;\ncur->node = ++node;\ncur->start = node->start << PAGE_SHIFT;\ncur->size = min(node->size << PAGE_SHIFT, cur->remaining);\nbreak;\ndefault:\nreturn;\n}\n}\n```\n```c\n#define min(x, y) ({ \\\ntypeof(x) _min1 = (x); \\\ntypeof(y) _min2 = (y); \\\n(void) (&_min1 == &_min2); \\\n_min1 < _min2 ? _min1 : _min2; })\n```\n```c\nstatic int amdgpu_ttm_map_buffer(struct ttm_buffer_object *bo,\nstruct ttm_resource *mem,\nstruct amdgpu_res_cursor *mm_cur,\nunsigned int window, struct amdgpu_ring *ring,\nbool tmz, uint64_t *size, uint64_t *addr)\n{\nstruct amdgpu_device *adev = ring->adev;\nunsigned int offset, num_pages, num_dw, num_bytes;\nuint64_t src_addr, dst_addr;\nstruct amdgpu_job *job;\nvoid *cpu_addr;\nuint64_t flags;\nunsigned int i;\nint r;\nBUG_ON(adev->mman.buffer_funcs->copy_max_bytes <\nAMDGPU_GTT_MAX_TRANSFER_SIZE * 8);\nif (WARN_ON(mem->mem_type == AMDGPU_PL_PREEMPT))\nreturn -EINVAL;\n/* Map only what can't be accessed directly */\nif (!tmz && mem->start != AMDGPU_BO_INVALID_OFFSET) {\n*addr = amdgpu_ttm_domain_start(adev, mem->mem_type) +\nmm_cur->start;\nreturn 0;\n}\n/*\n* If start begins at an offset inside the page, then adjust the size\n* and addr accordingly\n*/\noffset = mm_cur->start & ~PAGE_MASK;\nnum_pages = PFN_UP(*size + offset);\nnum_pages = min_t(uint32_t, num_pages, AMDGPU_GTT_MAX_TRANSFER_SIZE);\n*size = min(*size, (uint64_t)num_pages * PAGE_SIZE - offset);\n*addr = adev->gmc.gart_start;\n*addr += (u64)window * AMDGPU_GTT_MAX_TRANSFER_SIZE *\nAMDGPU_GPU_PAGE_SIZE;\n*addr += offset;\nnum_dw = ALIGN(adev->mman.buffer_funcs->copy_num_dw, 8);\nnum_bytes = num_pages * 8 * AMDGPU_GPU_PAGES_IN_CPU_PAGE;\nr = amdgpu_job_alloc_with_ib(adev, &adev->mman.high_pr,\nAMDGPU_FENCE_OWNER_UNDEFINED,\nnum_dw * 4 + num_bytes,\nAMDGPU_IB_POOL_DELAYED, &job);\nif (r)\nreturn r;\nsrc_addr = num_dw * 4;\nsrc_addr += job->ibs[0].gpu_addr;\ndst_addr = amdgpu_bo_gpu_offset(adev->gart.bo);\ndst_addr += window * AMDGPU_GTT_MAX_TRANSFER_SIZE * 8;\namdgpu_emit_copy_buffer(adev, &job->ibs[0], src_addr,\ndst_addr, num_bytes, 0);\namdgpu_ring_pad_ib(ring, &job->ibs[0]);\nWARN_ON(job->ibs[0].length_dw > num_dw);\nflags = amdgpu_ttm_tt_pte_flags(adev, bo->ttm, mem);\nif (tmz)\nflags |= AMDGPU_PTE_TMZ;\ncpu_addr = &job->ibs[0].ptr[num_dw];\nif (mem->mem_type == TTM_PL_TT) {\ndma_addr_t *dma_addr;\ndma_addr = &bo->ttm->dma_address[mm_cur->start >> PAGE_SHIFT];\namdgpu_gart_map(adev, 0, num_pages, dma_addr, flags, cpu_addr);\n} else {\ndma_addr_t dma_address;\ndma_address = mm_cur->start;\ndma_address += adev->vm_manager.vram_base_offset;\nfor (i = 0; i < num_pages; ++i) {\namdgpu_gart_map(adev, i << PAGE_SHIFT, 1, &dma_address,\nflags, cpu_addr);\ndma_address += PAGE_SIZE;\n}\n}\ndma_fence_put(amdgpu_job_submit(job));\nreturn 0;\n}\n```\n```c\nstatic int amdgpu_ttm_fill_mem(struct amdgpu_ring *ring, uint32_t src_data,\nuint64_t dst_addr, uint32_t byte_count,\nstruct dma_resv *resv,\nstruct dma_fence **fence,\nbool vm_needs_flush, bool delayed)\n{\nstruct amdgpu_device *adev = ring->adev;\nunsigned int num_loops, num_dw;\nstruct amdgpu_job *job;\nuint32_t max_bytes;\nunsigned int i;\nint r;\nmax_bytes = adev->mman.buffer_funcs->fill_max_bytes;\nnum_loops = DIV_ROUND_UP_ULL(byte_count, max_bytes);\nnum_dw = ALIGN(num_loops * adev->mman.buffer_funcs->fill_num_dw, 8);\nr = amdgpu_ttm_prepare_job(adev, false, num_dw, resv, vm_needs_flush,\n&job, delayed);\nif (r)\nreturn r;\nfor (i = 0; i < num_loops; i++) {\nuint32_t cur_size = min(byte_count, max_bytes);\namdgpu_emit_fill_buffer(adev, &job->ibs[0], src_data, dst_addr,\ncur_size);\ndst_addr += cur_size;\nbyte_count -= cur_size;\n}\namdgpu_ring_pad_ib(ring, &job->ibs[0]);\nWARN_ON(job->ibs[0].length_dw > num_dw);\n*fence = amdgpu_job_submit(job);\nreturn 0;\n}\n```\n```c\nstatic inline void dma_fence_put(struct dma_fence *fence)\n{\nif (fence)\nkref_put(&fence->refcount, dma_fence_release);\n}\n```\n```c\nstatic inline void mutex_unlock(struct mutex *)\n{\n}\n```\n",
 "is_vulnerable": false
}