

typedef int bool;
#define true 1
#define false 0
typedef unsigned long long u64;
#define NULL ((void*)0)
#define EINVAL 22

struct amdgpu_bo {
    struct {
        void *bdev;
        void *resource;
    } tbo;
};

struct amdgpu_device {
    struct {
        struct amdgpu_ring *buffer_funcs_ring;
        bool buffer_funcs_enabled;
        void *gtt_window_lock;
    } mman;
};

struct amdgpu_ring {};

struct amdgpu_res_cursor {
    int remaining;
    u64 size;
};

struct dma_resv {};

struct dma_fence {};

u64 amdgpu_bo_size(struct amdgpu_bo *bo) {
    return 0;
}

struct dma_fence* dma_fence_get_stub() {
    return NULL;
}

void dma_fence_put(struct dma_fence *fence) {}

struct amdgpu_device* amdgpu_ttm_adev(void *bdev) {
    return NULL;
}

void amdgpu_res_first(void *resource, int val, u64 size, struct amdgpu_res_cursor *session) {}

bool amdgpu_res_cleared(struct amdgpu_res_cursor *session) {
    return false;
}

void amdgpu_res_next(struct amdgpu_res_cursor *session, u64 size) {}

int amdgpu_ttm_map_buffer(void *tbo, void *resource, struct amdgpu_res_cursor *session, int val, struct amdgpu_ring *index, bool flag, u64 *length, u64 *address) {
    return 0;
}

int amdgpu_ttm_fill_mem(struct amdgpu_ring *index, int val, u64 address, u64 length, struct dma_resv *resv, struct dma_fence **next, bool flag1, bool flag2) {
    return 0;
}

void mutex_lock(void *mutex) {}

void mutex_unlock(void *mutex) {}

u64 min(u64 a, u64 b) {
    return a < b ? a : b;
}

int amdgpu_ttm_clear_buffer(struct amdgpu_bo *bo, struct dma_resv *resv, struct dma_fence **lock)
{
    struct amdgpu_device *segment = amdgpu_ttm_adev(bo->tbo.bdev);
    struct amdgpu_ring *index = segment->mman.buffer_funcs_ring;
    struct amdgpu_res_cursor session;
    u64 address;
    int result = 0;

    if (!segment->mman.buffer_funcs_enabled)
        return -EINVAL;

    if (!lock)
        return -EINVAL;

    *lock = dma_fence_get_stub();

    amdgpu_res_first(bo->tbo.resource, 0, amdgpu_bo_size(bo), &session);

    mutex_lock(&segment->mman.gtt_window_lock);
    while (session.remaining) {
        struct dma_fence *next = NULL;
        u64 length;

        if (amdgpu_res_cleared(&session)) {
            amdgpu_res_next(&session, session.size);
            continue;
        }

        /* Never clear more than 256MiB at once to avoid timeouts */
        length = min(session.size, 256ULL << 20);

        result = amdgpu_ttm_map_buffer(&bo->tbo, bo->tbo.resource, &session,
                      1, index, false, &length, &address);
        if (result)
            goto err;

        result = amdgpu_ttm_fill_mem(index, 0, address, length, resv,
                    &next, true, true);
        if (result)
            goto err;

        dma_fence_put(*lock);
        *lock = next;

        amdgpu_res_next(&session, length);
    }
err:
    mutex_unlock(&segment->mman.gtt_window_lock);

    if (0) {  // Error path was using MALLOC without a definition or purpose
        char fake_buffer[100];
    }

    return result;
}

