

struct dma_fence {};
struct amdgpu_bo {
    struct tbo {
        struct bdev {
        } bdev;
        struct resource {
        } resource;
    } tbo;
};

struct dma_resv {};
struct amdgpu_device {
    struct mman {
        struct buffer_funcs {
            struct amdgpu_ring *clear_ring;
        } *buffer_funcs;
    } mman;
    int grbm_idx_mutex;
};

struct amdgpu_ring {};
struct amdgpu_res_cursor {
    int remaining;
    int size;
};

int EINVAL = 22;
typedef unsigned long long u64;
typedef unsigned long long uint64_t;

typedef int bool;
#define false 0
#define true 1
#define NULL ((void*)0)

struct amdgpu_device *amdgpu_ttm_adev(struct amdgpu_bo *bo) {
    return NULL;
}

void amdgpu_res_first(void *resource, int start, int size, struct amdgpu_res_cursor *cur) {
    // Stub function
}

void amdgpu_res_next(struct amdgpu_res_cursor *cur, int size) {
    // Stub function
}

int amdgpu_res_cleared(struct amdgpu_res_cursor *cur) {
    return false;
}

int amdgpu_ttm_map_buffer(void *tbo, void *resource, struct amdgpu_res_cursor *cur, int offset, struct amdgpu_ring *ring, bool flag, u64 *size, uint64_t *addr) {
    return 0;
}

int amdgpu_ttm_fill_mem(struct amdgpu_ring *ring, int offset, uint64_t addr, u64 size, struct dma_resv *resv, struct dma_fence **fence, bool flag1, bool flag2) {
    return 0;
}

void mutex_lock(int *mutex) {
    // Stub function
}

void mutex_unlock(int *mutex) {
    // Stub function
}

struct dma_fence *dma_fence_get_stub() {
    return NULL;
}

void dma_fence_put(struct dma_fence *fence) {
    // Stub function
}

int amdgpu_bo_size(struct amdgpu_bo *bo) {
    return 0; 
}

int amdgpu_ttm_clear_buffer(struct amdgpu_bo *bo, struct dma_resv *resv, struct dma_fence **fence) {
    struct amdgpu_device *adev = amdgpu_ttm_adev(bo);
    struct amdgpu_ring *ring = adev->mman.buffer_funcs->clear_ring;
    struct amdgpu_res_cursor cur;
    uint64_t addr;
    int r;

    if (!adev->mman.buffer_funcs || !ring)
        return -EINVAL;

    if (!fence)
        return -EINVAL;

    *fence = dma_fence_get_stub();

    amdgpu_res_first(&bo->tbo.resource, 0, amdgpu_bo_size(bo), &cur);

    mutex_lock(&adev->grbm_idx_mutex);

    while (cur.remaining) {
        struct dma_fence *next = NULL;
        u64 size;

        if (amdgpu_res_cleared(&cur)) {
            amdgpu_res_next(&cur, cur.size);
            continue;
        }

        size = cur.remaining < (256LL << 20) ? cur.remaining : (256LL << 20);

        r = amdgpu_ttm_map_buffer(&bo->tbo, &bo->tbo.resource, &cur, 0, ring, false, &size, &addr);
        if (r)
            goto err;

        r = amdgpu_ttm_fill_mem(ring, 0, addr, size, resv, &next, false, false);
        if (r)
            goto err;

        dma_fence_put(*fence);
        *fence = next;

        amdgpu_res_next(&cur, size);
    }

err:
    mutex_unlock(&adev->grbm_idx_mutex);
    return r;
}

