{
 "supplementary_code": "```c\nstruct rxrpc_net {\nstruct proc_dir_entry *proc_net; /* Subdir in /proc/net */\nu32 epoch; /* Local epoch for detecting local-end reset */\nstruct list_head calls; /* List of calls active in this namespace */\nspinlock_t call_lock; /* Lock for ->calls */\natomic_t nr_calls; /* Count of allocated calls */\natomic_t nr_conns;\nstruct list_head bundle_proc_list; /* List of bundles for proc */\nstruct list_head conn_proc_list; /* List of conns in this namespace for proc */\nstruct list_head service_conns; /* Service conns in this namespace */\nrwlock_t conn_lock; /* Lock for ->conn_proc_list, ->service_conns */\nstruct work_struct service_conn_reaper;\nstruct timer_list service_conn_reap_timer;\nbool live;\natomic_t nr_client_conns;\nstruct hlist_head local_endpoints;\nstruct mutex local_mutex; /* Lock for ->local_endpoints */\nDECLARE_HASHTABLE (peer_hash, 10);\nspinlock_t peer_hash_lock; /* Lock for ->peer_hash */\n#define RXRPC_KEEPALIVE_TIME 20 /* NAT keepalive time in seconds */\nu8 peer_keepalive_cursor;\ntime64_t peer_keepalive_base;\nstruct list_head peer_keepalive[32];\nstruct list_head peer_keepalive_new;\nstruct timer_list peer_keepalive_timer;\nstruct work_struct peer_keepalive_work;\natomic_t stat_tx_data;\natomic_t stat_tx_data_retrans;\natomic_t stat_tx_data_send;\natomic_t stat_tx_data_send_frag;\natomic_t stat_tx_data_send_fail;\natomic_t stat_tx_data_underflow;\natomic_t stat_tx_data_cwnd_reset;\natomic_t stat_rx_data;\natomic_t stat_rx_data_reqack;\natomic_t stat_rx_data_jumbo;\natomic_t stat_tx_ack_fill;\natomic_t stat_tx_ack_send;\natomic_t stat_tx_ack_skip;\natomic_t stat_tx_acks[256];\natomic_t stat_rx_acks[256];\natomic_t stat_why_req_ack[8];\natomic_t stat_io_loop;\n};\n```\n```c\nstruct list_head {\nstruct list_head *next, *prev;\n};\n```\n```c\nstruct rxrpc_peer {\nstruct rcu_head rcu; /* This must be first */\nrefcount_t ref;\nunsigned long hash_key;\nstruct hlist_node hash_link;\nstruct rxrpc_local *local;\nstruct hlist_head error_targets; /* targets for net error distribution */\nstruct rb_root service_conns; /* Service connections */\nstruct list_head keepalive_link; /* Link in net->peer_keepalive[] */\ntime64_t last_tx_at; /* Last time packet sent here */\nseqlock_t service_conn_lock;\nspinlock_t lock; /* access lock */\nunsigned int if_mtu; /* interface MTU for this peer */\nunsigned int mtu; /* network MTU for this peer */\nunsigned int maxdata; /* data size (MTU - hdrsize) */\nunsigned short hdrsize; /* header size (IP + UDP + RxRPC) */\nint debug_id; /* debug ID for printks */\nstruct sockaddr_rxrpc srx; /* remote address */\n/* calculated RTT cache */\n#define RXRPC_RTT_CACHE_SIZE 32\nspinlock_t rtt_input_lock; /* RTT lock for input routine */\nktime_t rtt_last_req; /* Time of last RTT request */\nunsigned int rtt_count; /* Number of samples we've got */\nu32 srtt_us; /* smoothed round trip time << 3 in usecs */\nu32 mdev_us; /* medium deviation */\nu32 mdev_max_us; /* maximal mdev for the last rtt period */\nu32 rttvar_us; /* smoothed mdev_max */\nu32 rto_us; /* Retransmission timeout in usec */\nu8 backoff; /* Backoff timeout (as shift) */\nu8 cong_ssthresh; /* Congestion slow-start threshold */\n};\n```\n```c\nstatic inline void spin_lock(spinlock_t *lock)\n{\nint ret = pthread_spin_lock(lock);\nassert(!ret);\n}\n```\n```c\n#define list_entry(ptr, type, member) \\\ncontainer_of(ptr, type, member)\n```\n```c\nstatic inline void list_del_init(struct list_head *entry)\n{\n__list_del_entry(entry);\nINIT_LIST_HEAD(entry);\n}\n```\n```c\nstruct rxrpc_peer *rxrpc_get_peer_maybe(struct rxrpc_peer *peer,\nenum rxrpc_peer_trace why)\n{\nint r;\nif (peer) {\nif (__refcount_inc_not_zero(&peer->ref, &r))\ntrace_rxrpc_peer(peer->debug_id, r + 1, why);\nelse\npeer = NULL;\n}\nreturn peer;\n}\n```\n```c\nstatic inline bool __rxrpc_use_local(struct rxrpc_local *local,\nenum rxrpc_local_trace why)\n{\nint r, u;\nr = refcount_read(&local->ref);\nu = atomic_fetch_add_unless(&local->active_users, 1, 0);\ntrace_rxrpc_local(local->debug_id, why, r, u);\nreturn u != 0;\n}\n```\n```c\nstatic inline void spin_unlock(spinlock_t *lock)\n{\nint ret = pthread_spin_unlock(lock);\nassert(!ret);\n}\n```\n```c\nvoid rxrpc_send_keepalive(struct rxrpc_peer *peer)\n{\nstruct rxrpc_wire_header whdr;\nstruct msghdr msg;\nstruct kvec iov[2];\nsize_t len;\nint ret;\n_enter(\"\");\nmsg.msg_name = &peer->srx.transport;\nmsg.msg_namelen = peer->srx.transport_len;\nmsg.msg_control = NULL;\nmsg.msg_controllen = 0;\nmsg.msg_flags = 0;\nwhdr.epoch = htonl(peer->local->rxnet->epoch);\nwhdr.cid = 0;\nwhdr.callNumber = 0;\nwhdr.seq = 0;\nwhdr.serial = 0;\nwhdr.type = RXRPC_PACKET_TYPE_VERSION; /* Not client-initiated */\nwhdr.flags = RXRPC_LAST_PACKET;\nwhdr.userStatus = 0;\nwhdr.securityIndex = 0;\nwhdr._rsvd = 0;\nwhdr.serviceId = 0;\niov[0].iov_base = &whdr;\niov[0].iov_len = sizeof(whdr);\niov[1].iov_base = (char *)rxrpc_keepalive_string;\niov[1].iov_len = sizeof(rxrpc_keepalive_string);\nlen = iov[0].iov_len + iov[1].iov_len;\niov_iter_kvec(&msg.msg_iter, WRITE, iov, 2, len);\nret = do_udp_sendmsg(peer->local->socket, &msg, len);\nif (ret < 0)\ntrace_rxrpc_tx_fail(peer->debug_id, 0, ret,\nrxrpc_tx_point_version_keepalive);\nelse\ntrace_rxrpc_tx_packet(peer->debug_id, &whdr,\nrxrpc_tx_point_version_keepalive);\npeer->last_tx_at = ktime_get_seconds();\n_leave(\"\");\n}\n```\n```c\nstatic inline void list_add_tail(struct list_head *new, struct list_head *head)\n{\n__list_add(new, head->prev, head);\n}\n```\n```c\nvoid rxrpc_unuse_local(struct rxrpc_local *local, enum rxrpc_local_trace why)\n{\nunsigned int debug_id;\nint r, u;\nif (local) {\ndebug_id = local->debug_id;\nr = refcount_read(&local->ref);\nu = atomic_dec_return(&local->active_users);\ntrace_rxrpc_local(debug_id, why, r, u);\nif (u == 0)\nkthread_stop(local->io_thread);\n}\n}\n```\n```c\nvoid rxrpc_put_peer(struct rxrpc_peer *peer, enum rxrpc_peer_trace why)\n{\nunsigned int debug_id;\nbool dead;\nint r;\nif (peer) {\ndebug_id = peer->debug_id;\ndead = __refcount_dec_and_test(&peer->ref, &r);\ntrace_rxrpc_peer(debug_id, r - 1, why);\nif (dead)\n__rxrpc_put_peer(peer);\n}\n}\n```\n",
 "function_prototype": "static void rxrpc_peer_keepalive_dispatch(struct rxrpc_net *rxnet, struct list_head *collector, time64_t base, u8 cursor)",
 "re_implemented_code": "\n```c\nstatic void rxrpc_peer_keepalive_dispatch(struct rxrpc_net *rxnet, struct list_head *collector, time64_t base, u8 cursor)\n{\n// Declare a pointer to an rxrpc_peer structure\nstruct rxrpc_peer *peer;\n// Define a mask for the size of the peer_keepalive array\nconst u8 mask = ARRAY_SIZE(rxnet->peer_keepalive) - 1;\n// Declare a variable to store the calculated keepalive time\ntime64_t now;\n// Declare a boolean to check if the local endpoint can be used\nbool use_local;\n// Declare a variable to store the slot index\nu8 idx;\n// Acquire the spin lock for the peer hash\nspin_lock(&rxnet->peer_hash_lock);\n// Loop while the collector list is not empty\nwhile (!list_empty(collector)) {\n// Get the first peer from the collector list\npeer = list_first_entry(collector, struct rxrpc_peer, keepalive_link);\n// Remove the peer from the collector list and reinitialize its list head\nlist_del_init(&peer->keepalive_link);\n// Attempt to increment the reference count of the peer\nif (!__rxrpc_use_local(peer->local, RTRACERXRPCPEERUSELOCAL)) {\n// If the peer reference count could not be incremented, continue to the next peer\ncontinue;\n}\n// Release the spin lock for the peer hash\nspin_unlock(&rxnet->peer_hash_lock);\n// If the local endpoint can be used\nif (use_local) {\n// Calculate the next keepalive time for the peer\nnow = ktime_get_seconds();\n// Calculate the slot index based on the keepalive time\nidx = ((now - base) % RXRPC_KEEPALIVE_TIME) >> 1;\n// Debugging output for the peer and slot\npr_info(\"Peer %d Keepalive Slot Index: %d\\n\", peer->debug_id, idx);\n// Check if the keepalive time is outside the valid range\nif (idx >= ARRAY_SIZE(rxnet->peer_keepalive)) {\n// Send a keepalive packet to the peer\nrxrpc_send_keepalive(peer);\n// Set the slot index to the maximum keepalive time\nidx = ARRAY_SIZE(rxnet->peer_keepalive) - 1;\n}\n// Adjust the slot index based on the current cursor position\nidx += cursor;\n// Apply the mask to the slot index\nidx &= mask;\n// Acquire the spin lock for the peer hash\nspin_lock(&rxnet->peer_hash_lock);\n// Add the peer to the appropriate slot in the peer_keepalive array\nlist_add_tail(&peer->keepalive_link, &rxnet->peer_keepalive[idx]);\n// Release the spin lock for the peer hash\nspin_unlock(&rxnet->peer_hash_lock);\n// Decrement the active user count for the local endpoint\nrxrpc_unuse_local(peer->local, RTRACERXPCRUNUSELOCAL);\n}\n// Decrement the reference count for the peer\nrxrpc_put_peer(peer, RTRACERXPURPPUTPEER);\n// Acquire the spin lock for the peer hash\nspin_lock(&rxnet->peer_hash_lock);\n}\n// Release the spin lock for the peer hash\nspin_unlock(&rxnet->peer_hash_lock);\n}\n",
 "is_vulnerable": true
}