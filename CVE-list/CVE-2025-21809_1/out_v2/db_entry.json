{
  "cwe_type": "Improper Locking",
  "cve_id": "CVE-2025-21809",
  "supplementary_code": "```c\nstruct rxrpc_net {\nstruct proc_dir_entry *proc_net; /* Subdir in /proc/net */\nu32 epoch; /* Local epoch for detecting local-end reset */\nstruct list_head calls; /* List of calls active in this namespace */\nspinlock_t call_lock; /* Lock for ->calls */\natomic_t nr_calls; /* Count of allocated calls */\natomic_t nr_conns;\nstruct list_head bundle_proc_list; /* List of bundles for proc */\nstruct list_head conn_proc_list; /* List of conns in this namespace for proc */\nstruct list_head service_conns; /* Service conns in this namespace */\nrwlock_t conn_lock; /* Lock for ->conn_proc_list, ->service_conns */\nstruct work_struct service_conn_reaper;\nstruct timer_list service_conn_reap_timer;\nbool live;\natomic_t nr_client_conns;\nstruct hlist_head local_endpoints;\nstruct mutex local_mutex; /* Lock for ->local_endpoints */\nDECLARE_HASHTABLE (peer_hash, 10);\nspinlock_t peer_hash_lock; /* Lock for ->peer_hash */\n#define RXRPC_KEEPALIVE_TIME 20 /* NAT keepalive time in seconds */\nu8 peer_keepalive_cursor;\ntime64_t peer_keepalive_base;\nstruct list_head peer_keepalive[32];\nstruct list_head peer_keepalive_new;\nstruct timer_list peer_keepalive_timer;\nstruct work_struct peer_keepalive_work;\natomic_t stat_tx_data;\natomic_t stat_tx_data_retrans;\natomic_t stat_tx_data_send;\natomic_t stat_tx_data_send_frag;\natomic_t stat_tx_data_send_fail;\natomic_t stat_tx_data_underflow;\natomic_t stat_tx_data_cwnd_reset;\natomic_t stat_rx_data;\natomic_t stat_rx_data_reqack;\natomic_t stat_rx_data_jumbo;\natomic_t stat_tx_ack_fill;\natomic_t stat_tx_ack_send;\natomic_t stat_tx_ack_skip;\natomic_t stat_tx_acks[256];\natomic_t stat_rx_acks[256];\natomic_t stat_why_req_ack[8];\natomic_t stat_io_loop;\n};\n```\n```c\nstruct list_head {\nstruct list_head *next, *prev;\n};\n```\n```c\nstruct rxrpc_peer {\nstruct rcu_head rcu; /* This must be first */\nrefcount_t ref;\nunsigned long hash_key;\nstruct hlist_node hash_link;\nstruct rxrpc_local *local;\nstruct hlist_head error_targets; /* targets for net error distribution */\nstruct rb_root service_conns; /* Service connections */\nstruct list_head keepalive_link; /* Link in net->peer_keepalive[] */\ntime64_t last_tx_at; /* Last time packet sent here */\nseqlock_t service_conn_lock;\nspinlock_t lock; /* access lock */\nunsigned int if_mtu; /* interface MTU for this peer */\nunsigned int mtu; /* network MTU for this peer */\nunsigned int maxdata; /* data size (MTU - hdrsize) */\nunsigned short hdrsize; /* header size (IP + UDP + RxRPC) */\nint debug_id; /* debug ID for printks */\nstruct sockaddr_rxrpc srx; /* remote address */\n/* calculated RTT cache */\n#define RXRPC_RTT_CACHE_SIZE 32\nspinlock_t rtt_input_lock; /* RTT lock for input routine */\nktime_t rtt_last_req; /* Time of last RTT request */\nunsigned int rtt_count; /* Number of samples we've got */\nu32 srtt_us; /* smoothed round trip time << 3 in usecs */\nu32 mdev_us; /* medium deviation */\nu32 mdev_max_us; /* maximal mdev for the last rtt period */\nu32 rttvar_us; /* smoothed mdev_max */\nu32 rto_us; /* Retransmission timeout in usec */\nu8 backoff; /* Backoff timeout (as shift) */\nu8 cong_ssthresh; /* Congestion slow-start threshold */\n};\n```\n```c\nstatic inline void spin_lock(spinlock_t *lock)\n{\nint ret = pthread_spin_lock(lock);\nassert(!ret);\n}\n```\n```c\n#define list_entry(ptr, type, member) \\\ncontainer_of(ptr, type, member)\n```\n```c\nstatic inline void list_del_init(struct list_head *entry)\n{\n__list_del_entry(entry);\nINIT_LIST_HEAD(entry);\n}\n```\n```c\nstruct rxrpc_peer *rxrpc_get_peer_maybe(struct rxrpc_peer *peer,\nenum rxrpc_peer_trace why)\n{\nint r;\nif (peer) {\nif (__refcount_inc_not_zero(&peer->ref, &r))\ntrace_rxrpc_peer(peer->debug_id, r + 1, why);\nelse\npeer = NULL;\n}\nreturn peer;\n}\n```\n```c\nstatic inline bool __rxrpc_use_local(struct rxrpc_local *local,\nenum rxrpc_local_trace why)\n{\nint r, u;\nr = refcount_read(&local->ref);\nu = atomic_fetch_add_unless(&local->active_users, 1, 0);\ntrace_rxrpc_local(local->debug_id, why, r, u);\nreturn u != 0;\n}\n```\n```c\nstatic inline void spin_unlock(spinlock_t *lock)\n{\nint ret = pthread_spin_unlock(lock);\nassert(!ret);\n}\n```\n```c\nvoid rxrpc_send_keepalive(struct rxrpc_peer *peer)\n{\nstruct rxrpc_wire_header whdr;\nstruct msghdr msg;\nstruct kvec iov[2];\nsize_t len;\nint ret;\n_enter(\"\");\nmsg.msg_name = &peer->srx.transport;\nmsg.msg_namelen = peer->srx.transport_len;\nmsg.msg_control = NULL;\nmsg.msg_controllen = 0;\nmsg.msg_flags = 0;\nwhdr.epoch = htonl(peer->local->rxnet->epoch);\nwhdr.cid = 0;\nwhdr.callNumber = 0;\nwhdr.seq = 0;\nwhdr.serial = 0;\nwhdr.type = RXRPC_PACKET_TYPE_VERSION; /* Not client-initiated */\nwhdr.flags = RXRPC_LAST_PACKET;\nwhdr.userStatus = 0;\nwhdr.securityIndex = 0;\nwhdr._rsvd = 0;\nwhdr.serviceId = 0;\niov[0].iov_base = &whdr;\niov[0].iov_len = sizeof(whdr);\niov[1].iov_base = (char *)rxrpc_keepalive_string;\niov[1].iov_len = sizeof(rxrpc_keepalive_string);\nlen = iov[0].iov_len + iov[1].iov_len;\niov_iter_kvec(&msg.msg_iter, WRITE, iov, 2, len);\nret = do_udp_sendmsg(peer->local->socket, &msg, len);\nif (ret < 0)\ntrace_rxrpc_tx_fail(peer->debug_id, 0, ret,\nrxrpc_tx_point_version_keepalive);\nelse\ntrace_rxrpc_tx_packet(peer->debug_id, &whdr,\nrxrpc_tx_point_version_keepalive);\npeer->last_tx_at = ktime_get_seconds();\n_leave(\"\");\n}\n```\n```c\nstatic inline void list_add_tail(struct list_head *new, struct list_head *head)\n{\n__list_add(new, head->prev, head);\n}\n```\n```c\nvoid rxrpc_unuse_local(struct rxrpc_local *local, enum rxrpc_local_trace why)\n{\nunsigned int debug_id;\nint r, u;\nif (local) {\ndebug_id = local->debug_id;\nr = refcount_read(&local->ref);\nu = atomic_dec_return(&local->active_users);\ntrace_rxrpc_local(debug_id, why, r, u);\nif (u == 0)\nkthread_stop(local->io_thread);\n}\n}\n```\n```c\nvoid rxrpc_put_peer(struct rxrpc_peer *peer, enum rxrpc_peer_trace why)\n{\nunsigned int debug_id;\nbool dead;\nint r;\nif (peer) {\ndebug_id = peer->debug_id;\ndead = __refcount_dec_and_test(&peer->ref, &r);\ntrace_rxrpc_peer(debug_id, r - 1, why);\nif (dead)\n__rxrpc_put_peer(peer);\n}\n}\n```",
  "original_code": "```c\nstatic void rxrpc_peer_keepalive_dispatch(struct rxrpc_net *rxnet, struct list_head *collector, time64_t base, u8 cursor)\n{\nstruct rxrpc_peer *peer;\nconst u8 mask = ARRAY_SIZE(rxnet->peer_keepalive) - 1;\ntime64_t keepalive_at;\nbool use;\nint slot;\nspin_lock(&rxnet->peer_hash_lock);\nwhile (!list_empty(collector)) {\npeer = list_entry(collector->next,\nstruct rxrpc_peer, keepalive_link);\nlist_del_init(&peer->keepalive_link);\nif (!rxrpc_get_peer_maybe(peer, rxrpc_peer_get_keepalive))\ncontinue;\nuse = __rxrpc_use_local(peer->local, rxrpc_local_use_peer_keepalive);\nspin_unlock(&rxnet->peer_hash_lock);\nif (use) {\nkeepalive_at = peer->last_tx_at + RXRPC_KEEPALIVE_TIME;\nslot = keepalive_at - base;\n_debug(\"%02x peer %u t=%d {%pISp}\",\ncursor, peer->debug_id, slot, &peer->srx.transport);\nif (keepalive_at <= base ||\nkeepalive_at > base + RXRPC_KEEPALIVE_TIME) {\nrxrpc_send_keepalive(peer);\nslot = RXRPC_KEEPALIVE_TIME;\n}\n/* A transmission to this peer occurred since last we\n* examined it so put it into the appropriate future\n* bucket.\n*/\nslot += cursor;\nslot &= mask;\nspin_lock(&rxnet->peer_hash_lock);\nlist_add_tail(&peer->keepalive_link,\n&rxnet->peer_keepalive[slot & mask]);\nspin_unlock(&rxnet->peer_hash_lock);\nrxrpc_unuse_local(peer->local, rxrpc_local_unuse_peer_keepalive);\n}\nrxrpc_put_peer(peer, rxrpc_peer_put_keepalive);\nspin_lock(&rxnet->peer_hash_lock);\n}\nspin_unlock(&rxnet->peer_hash_lock);\n}\n```",
  "vuln_patch": "```c\nstatic void rxrpc_peer_keepalive_dispatch(struct rxrpc_net *rxnet,\nstruct list_head *collector,\ntime64_t base,\nu8 cursor)\n{\nstruct rxrpc_peer *peer;\nconst u8 mask = ARRAY_SIZE(rxnet->peer_keepalive) - 1;\ntime64_t keepalive_at;\nbool use;\nint slot;\nspin_lock_bh(&rxnet->peer_hash_lock);\nwhile (!list_empty(collector)) {\npeer = list_entry(collector->next,\nstruct rxrpc_peer, keepalive_link);\nlist_del_init(&peer->keepalive_link);\nif (!rxrpc_get_peer_maybe(peer, rxrpc_peer_get_keepalive))\ncontinue;\nuse = __rxrpc_use_local(peer->local, rxrpc_local_use_peer_keepalive);\nspin_unlock_bh(&rxnet->peer_hash_lock);\nif (use) {\nkeepalive_at = peer->last_tx_at + RXRPC_KEEPALIVE_TIME;\nslot = keepalive_at - base;\n_debug(\"%02x peer %u t=%d {%pISp}\",\ncursor, peer->debug_id, slot, &peer->srx.transport);\nif (keepalive_at <= base ||\nkeepalive_at > base + RXRPC_KEEPALIVE_TIME) {\nrxrpc_send_keepalive(peer);\nslot = RXRPC_KEEPALIVE_TIME;\n}\n/* A transmission to this peer occurred since last we\n* examined it so put it into the appropriate future\n* bucket.\n*/\nslot += cursor;\nslot &= mask;\nspin_lock_bh(&rxnet->peer_hash_lock);\nlist_add_tail(&peer->keepalive_link,\n&rxnet->peer_keepalive[slot & mask]);\nspin_unlock_bh(&rxnet->peer_hash_lock);\nrxrpc_unuse_local(peer->local, rxrpc_local_unuse_peer_keepalive);\n}\nrxrpc_put_peer(peer, rxrpc_peer_put_keepalive);\nspin_lock_bh(&rxnet->peer_hash_lock);\n}\nspin_unlock_bh(&rxnet->peer_hash_lock);\n}\n```",
  "function_name": "rxrpc_peer_keepalive_dispatch",
  "function_prototype": "static void rxrpc_peer_keepalive_dispatch(struct rxrpc_net *rxnet, struct list_head *collector, time64_t base, u8 cursor)",
  "code_semantics": "The function manages a list of items, processing each item by removing it from the list, checking if it should be processed further, and potentially sending a message. It calculates a future time slot for each item and repositions it in a data structure based on this slot. The function uses locks to ensure exclusive access to shared data structures during these operations.",
  "safe_verification_cot": "The spin_lock_bh function is used for rxnet->peer_hash_lock, which disables bottom halves, preventing race conditions with interrupt handlers. The spin_unlock_bh function is used for rxnet->peer_hash_lock, which re-enables bottom halves, ensuring system consistency. The collector list is accessed with bottom halves disabled, preventing concurrent modifications. The peer->keepalive_link is manipulated with proper locking, ensuring data integrity.",
  "verification_cot": "The spin_lock function is used for rxnet->peer_hash_lock, which does not disable bottom halves. This can lead to race conditions if a bottom half accesses the same data structure. The spin_unlock function is used for rxnet->peer_hash_lock, which does not re-enable bottom halves, potentially leaving the system in an inconsistent state. The collector list is accessed without ensuring that bottom halves are disabled, which can lead to concurrent modifications. The peer->keepalive_link is manipulated without proper locking, leading to potential data corruption.",
  "vulnerability_related_variables": {
    "rxnet->peer_hash_lock": "This variable is used to control access to a shared resource by multiple threads. It ensures that only one thread can access the resource at a time, preventing data corruption and ensuring consistency.",
    "collector": "This variable acts as a holder for a collection of elements. It allows for the iteration over the elements, processing each one in turn until the collection is empty.",
    "peer->keepalive_link": "This variable is a component of a data structure that allows an element to be part of a doubly linked list. It facilitates the addition and removal of the element from the list, maintaining the list's integrity."
  },
  "vulnerability_related_functions": {
    "spin_lock": "This function is used to acquire a lock on a synchronization primitive to prevent concurrent access to a shared resource by multiple threads.",
    "spin_unlock": "This function is used to release a lock on a synchronization primitive, allowing other threads to access the shared resource.",
    "list_del_init": "This function is used to remove an element from a doubly linked list and reset its pointers to indicate that it is no longer part of any list.",
    "list_add_tail": "This function is used to append an element to the end of a doubly linked list, updating the necessary pointers to maintain the list structure."
  },
  "root_cause": "Improper locking mechanism using spin_lock instead of spin_lock_bh, leading to potential race conditions with bottom halves.",
  "patch_cot": "1. First, identify all instances where spin_lock and spin_unlock are used with the variable rxnet->peer_hash_lock.\n2. Replace each spin_lock(&rxnet->peer_hash_lock) with spin_lock_bh(&rxnet->peer_hash_lock).\n3. Replace each spin_unlock(&rxnet->peer_hash_lock) with spin_unlock_bh(&rxnet->peer_hash_lock).\n4. Ensure that any operations on the collector and peer->keepalive_link variables are performed within the critical section protected by spin_lock_bh and spin_unlock_bh."
}