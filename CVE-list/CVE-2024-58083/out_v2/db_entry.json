{
  "cwe_type": "Use After Free",
  "cve_id": "CVE-2024-58083",
  "supplementary_code": "```c\nstruct kvm {\n#ifdef KVM_HAVE_MMU_RWLOCK\nrwlock_t mmu_lock;\n#else\nspinlock_t mmu_lock;\n#endif /* KVM_HAVE_MMU_RWLOCK */\nstruct mutex slots_lock;\n/*\n* Protects the arch-specific fields of struct kvm_memory_slots in\n* use by the VM. To be used under the slots_lock (above) or in a\n* kvm->srcu critical section where acquiring the slots_lock would\n* lead to deadlock with the synchronize_srcu in\n* kvm_swap_active_memslots().\n*/\nstruct mutex slots_arch_lock;\nstruct mm_struct *mm; /* userspace tied to this vm */\nunsigned long nr_memslot_pages;\n/* The two memslot sets - active and inactive (per address space) */\nstruct kvm_memslots __memslots[KVM_MAX_NR_ADDRESS_SPACES][2];\n/* The current active memslot set for each address space */\nstruct kvm_memslots __rcu *memslots[KVM_MAX_NR_ADDRESS_SPACES];\nstruct xarray vcpu_array;\n/*\n* Protected by slots_lock, but can be read outside if an\n* incorrect answer is acceptable.\n*/\natomic_t nr_memslots_dirty_logging;\n/* Used to wait for completion of MMU notifiers. */\nspinlock_t mn_invalidate_lock;\nunsigned long mn_active_invalidate_count;\nstruct rcuwait mn_memslots_update_rcuwait;\n/* For management / invalidation of gfn_to_pfn_caches */\nspinlock_t gpc_lock;\nstruct list_head gpc_list;\n/*\n* created_vcpus is protected by kvm->lock, and is incremented\n* at the beginning of KVM_CREATE_VCPU. online_vcpus is only\n* incremented after storing the kvm_vcpu pointer in vcpus,\n* and is accessed atomically.\n*/\natomic_t online_vcpus;\nint max_vcpus;\nint created_vcpus;\nint last_boosted_vcpu;\nstruct list_head vm_list;\nstruct mutex lock;\nstruct kvm_io_bus __rcu *buses[KVM_NR_BUSES];\n#ifdef CONFIG_HAVE_KVM_IRQCHIP\nstruct {\nspinlock_t lock;\nstruct list_head items;\n/* resampler_list update side is protected by resampler_lock. */\nstruct list_head resampler_list;\nstruct mutex resampler_lock;\n} irqfds;\n#endif\nstruct list_head ioeventfds;\nstruct kvm_vm_stat stat;\nstruct kvm_arch arch;\nrefcount_t users_count;\n#ifdef CONFIG_KVM_MMIO\nstruct kvm_coalesced_mmio_ring *coalesced_mmio_ring;\nspinlock_t ring_lock;\nstruct list_head coalesced_zones;\n#endif\nstruct mutex irq_lock;\n#ifdef CONFIG_HAVE_KVM_IRQCHIP\n/*\n* Update side is protected by irq_lock.\n*/\nstruct kvm_irq_routing_table __rcu *irq_routing;\nstruct hlist_head irq_ack_notifier_list;\n#endif\n#ifdef CONFIG_KVM_GENERIC_MMU_NOTIFIER\nstruct mmu_notifier mmu_notifier;\nunsigned long mmu_invalidate_seq;\nlong mmu_invalidate_in_progress;\ngfn_t mmu_invalidate_range_start;\ngfn_t mmu_invalidate_range_end;\n#endif\nstruct list_head devices;\nu64 manual_dirty_log_protect;\nstruct dentry *debugfs_dentry;\nstruct kvm_stat_data **debugfs_stat_data;\nstruct srcu_struct srcu;\nstruct srcu_struct irq_srcu;\npid_t userspace_pid;\nbool override_halt_poll_ns;\nunsigned int max_halt_poll_ns;\nu32 dirty_ring_size;\nbool dirty_ring_with_bitmap;\nbool vm_bugged;\nbool vm_dead;\n#ifdef CONFIG_HAVE_KVM_PM_NOTIFIER\nstruct notifier_block pm_notifier;\n#endif\n#ifdef CONFIG_KVM_GENERIC_MEMORY_ATTRIBUTES\n/* Protected by slots_locks (for writes) and RCU (for reads) */\nstruct xarray mem_attr_array;\n#endif\nchar stats_id[KVM_STATS_NAME_SIZE];\n};\n```\n```c\nstatic inline int atomic_read(const atomic_t *v)\n{\nreturn READ_ONCE((v)->counter);\n}\n```\n```c\n#define array_index_nospec(index, size)\t\\\n({\t\\\ntypeof(index) _i = (index);\t\\\ntypeof(size) _s = (size);\t\\\nunsigned long _mask = array_index_mask_nospec(_i, _s);\t\\\n\\\nBUILD_BUG_ON(sizeof(_i) > sizeof(long));\t\\\nBUILD_BUG_ON(sizeof(_s) > sizeof(long));\t\\\n\\\n(typeof(_i)) (_i & _mask);\t\\\n})\n```\n```c\n#define smp_rmb()\tbarrier()\n```\n```c\nvoid *xa_load(struct xarray *xa, unsigned long index)\n{\nXA_STATE(xas, xa, index);\nvoid *entry;\nrcu_read_lock();\ndo {\nentry = xas_load(&xas);\nif (xa_is_zero(entry))\nentry = NULL;\n} while (xas_retry(&xas, entry));\nrcu_read_unlock();\nreturn entry;\n}\nEXPORT_SYMBOL(xa_load);\n```",
  "original_code": "```c\nstatic inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)\n{\nint num_vcpus = atomic_read(&kvm->online_vcpus);\ni = array_index_nospec(i, num_vcpus);\n/* Pairs with smp_wmb() in kvm_vm_ioctl_create_vcpu. */\nsmp_rmb();\nreturn kvm->vcpus[i];\n}\n```",
  "vuln_patch": "```c\nstatic inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)\n{\nint num_vcpus = atomic_read(&kvm->online_vcpus);\n/*\n* Explicitly verify the target vCPU is online, as the anti-speculation\n* logic only limits the CPU's ability to speculate, e.g. given a \"bad\"\n* index, clamping the index to 0 would return vCPU0, not NULL.\n*/\nif (i >= num_vcpus)\nreturn NULL;\ni = array_index_nospec(i, num_vcpus);\n/* Pairs with smp_wmb() in kvm_vm_ioctl_create_vcpu. */\nsmp_rmb();\nreturn kvm->vcpus[i];\n}\n```",
  "function_name": "kvm_get_vcpu",
  "function_prototype": "static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)",
  "code_semantics": "The function safely accesses an element from an array within a data structure. It determines the valid range of indices by reading a value representing the number of available elements. It adjusts the provided index to ensure it is within this range. A memory barrier ensures correct memory operation ordering. The function retrieves and returns the element at the adjusted index from the array.",
  "safe_verification_cot": "1. The function kvm_get_vcpu includes a check to ensure i is within the bounds of num_vcpus before accessing the vcpus array, preventing out-of-bounds access.\n2. The variable i is properly sanitized by checking its bounds before being passed to array_index_nospec.\n3. The atomic_read function is correctly used to obtain num_vcpus, and the boundary check ensures safe access.\n4. The smp_rmb is used correctly for memory ordering, and the boundary check ensures that the memory access is safe.",
  "verification_cot": "1. The function kvm_get_vcpu does not check if i is within the bounds of num_vcpus before accessing the vcpus array.\n2. The variable i is not properly sanitized before being passed to array_index_nospec, which could lead to out-of-bounds access.\n3. The atomic_read function is correctly used to obtain num_vcpus, but without the boundary check, it does not prevent out-of-bounds access.\n4. The smp_rmb is used correctly for memory ordering, but it does not address the boundary check issue.",
  "vulnerability_related_variables": {
    "i": "This variable represents an index that is initially provided as input. It is then adjusted to ensure it is within a valid range, preventing out-of-bounds access. The adjusted index is used to access a specific element in a collection.",
    "num_vcpus": "This variable holds a count of active elements in a collection. It is retrieved in a manner that ensures the value is read consistently, even in a concurrent environment. This count is used to validate or adjust an index to prevent out-of-bounds access."
  },
  "vulnerability_related_functions": {
    "atomic_read": "This function retrieves the current value of a variable that is designed to be accessed atomically, ensuring that the read operation is performed in a manner that is consistent with concurrent memory operations.",
    "array_index_nospec": "This macro calculates a safe index for accessing an array by applying a mask to the index, which prevents speculative execution from accessing out-of-bounds memory locations.",
    "smp_rmb": "This macro acts as a barrier to ensure that all memory read operations specified before the barrier are completed before any subsequent read operations, maintaining the correct order of operations in a multi-processor environment."
  },
  "root_cause": "The root cause of the vulnerability is the lack of boundary checking for the index 'i' before accessing the 'vcpus' array, leading to a potential use-after-free condition.",
  "patch_cot": "To patch the vulnerability, first introduce a boundary check for the index i before it is used to access the vcpus array. Add a conditional statement to verify if i is greater than or equal to num_vcpus. If true, return NULL immediately to prevent further access to the vcpus array. Retain the use of array_index_nospec to mitigate speculative execution attacks, ensuring it is only called after the boundary check. Ensure that smp_rmb() is used correctly to maintain memory ordering, as it pairs with smp_wmb() in related code."
}