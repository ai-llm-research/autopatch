{
 "supplementary_code": "```c\nstruct kvm {\n#ifdef KVM_HAVE_MMU_RWLOCK\nrwlock_t mmu_lock;\n#else\nspinlock_t mmu_lock;\n#endif /* KVM_HAVE_MMU_RWLOCK */\nstruct mutex slots_lock;\n/*\n* Protects the arch-specific fields of struct kvm_memory_slots in\n* use by the VM. To be used under the slots_lock (above) or in a\n* kvm->srcu critical section where acquiring the slots_lock would\n* lead to deadlock with the synchronize_srcu in\n* kvm_swap_active_memslots().\n*/\nstruct mutex slots_arch_lock;\nstruct mm_struct *mm; /* userspace tied to this vm */\nunsigned long nr_memslot_pages;\n/* The two memslot sets - active and inactive (per address space) */\nstruct kvm_memslots __memslots[KVM_MAX_NR_ADDRESS_SPACES][2];\n/* The current active memslot set for each address space */\nstruct kvm_memslots __rcu *memslots[KVM_MAX_NR_ADDRESS_SPACES];\nstruct xarray vcpu_array;\n/*\n* Protected by slots_lock, but can be read outside if an\n* incorrect answer is acceptable.\n*/\natomic_t nr_memslots_dirty_logging;\n/* Used to wait for completion of MMU notifiers. */\nspinlock_t mn_invalidate_lock;\nunsigned long mn_active_invalidate_count;\nstruct rcuwait mn_memslots_update_rcuwait;\n/* For management / invalidation of gfn_to_pfn_caches */\nspinlock_t gpc_lock;\nstruct list_head gpc_list;\n/*\n* created_vcpus is protected by kvm->lock, and is incremented\n* at the beginning of KVM_CREATE_VCPU. online_vcpus is only\n* incremented after storing the kvm_vcpu pointer in vcpus,\n* and is accessed atomically.\n*/\natomic_t online_vcpus;\nint max_vcpus;\nint created_vcpus;\nint last_boosted_vcpu;\nstruct list_head vm_list;\nstruct mutex lock;\nstruct kvm_io_bus __rcu *buses[KVM_NR_BUSES];\n#ifdef CONFIG_HAVE_KVM_IRQCHIP\nstruct {\nspinlock_t lock;\nstruct list_head items;\n/* resampler_list update side is protected by resampler_lock. */\nstruct list_head resampler_list;\nstruct mutex resampler_lock;\n} irqfds;\n#endif\nstruct list_head ioeventfds;\nstruct kvm_vm_stat stat;\nstruct kvm_arch arch;\nrefcount_t users_count;\n#ifdef CONFIG_KVM_MMIO\nstruct kvm_coalesced_mmio_ring *coalesced_mmio_ring;\nspinlock_t ring_lock;\nstruct list_head coalesced_zones;\n#endif\nstruct mutex irq_lock;\n#ifdef CONFIG_HAVE_KVM_IRQCHIP\n/*\n* Update side is protected by irq_lock.\n*/\nstruct kvm_irq_routing_table __rcu *irq_routing;\nstruct hlist_head irq_ack_notifier_list;\n#endif\n#ifdef CONFIG_KVM_GENERIC_MMU_NOTIFIER\nstruct mmu_notifier mmu_notifier;\nunsigned long mmu_invalidate_seq;\nlong mmu_invalidate_in_progress;\ngfn_t mmu_invalidate_range_start;\ngfn_t mmu_invalidate_range_end;\n#endif\nstruct list_head devices;\nu64 manual_dirty_log_protect;\nstruct dentry *debugfs_dentry;\nstruct kvm_stat_data **debugfs_stat_data;\nstruct srcu_struct srcu;\nstruct srcu_struct irq_srcu;\npid_t userspace_pid;\nbool override_halt_poll_ns;\nunsigned int max_halt_poll_ns;\nu32 dirty_ring_size;\nbool dirty_ring_with_bitmap;\nbool vm_bugged;\nbool vm_dead;\n#ifdef CONFIG_HAVE_KVM_PM_NOTIFIER\nstruct notifier_block pm_notifier;\n#endif\n#ifdef CONFIG_KVM_GENERIC_MEMORY_ATTRIBUTES\n/* Protected by slots_locks (for writes) and RCU (for reads) */\nstruct xarray mem_attr_array;\n#endif\nchar stats_id[KVM_STATS_NAME_SIZE];\n};\n```\n```c\nstatic inline int atomic_read(const atomic_t *v)\n{\nreturn READ_ONCE((v)->counter);\n}\n```\n```c\n#define array_index_nospec(index, size)\t\\\n({\t\\\ntypeof(index) _i = (index);\t\\\ntypeof(size) _s = (size);\t\\\nunsigned long _mask = array_index_mask_nospec(_i, _s);\t\\\n\\\nBUILD_BUG_ON(sizeof(_i) > sizeof(long));\t\\\nBUILD_BUG_ON(sizeof(_s) > sizeof(long));\t\\\n\\\n(typeof(_i)) (_i & _mask);\t\\\n})\n```\n```c\n#define smp_rmb()\tbarrier()\n```\n```c\nvoid *xa_load(struct xarray *xa, unsigned long index)\n{\nXA_STATE(xas, xa, index);\nvoid *entry;\nrcu_read_lock();\ndo {\nentry = xas_load(&xas);\nif (xa_is_zero(entry))\nentry = NULL;\n} while (xas_retry(&xas, entry));\nrcu_read_unlock();\nreturn entry;\n}\nEXPORT_SYMBOL(xa_load);\n```\n",
 "function_prototype": "static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)",
 "re_implemented_code": "\n```c\nstatic inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i) {\n// Read the number of online virtual CPUs from the kvm structure atomically\nint n = atomic_read(&kvm->online_vcpus);\n// Ensure the index is within bounds using a no-speculation array index macro\ni = array_index_nospec(i, n);\n// Ensure memory ordering by using a read memory barrier\nsmp_rmb();\n// Return the virtual CPU at the specified index from the kvm structure\nreturn radix_tree_lookup(&kvm->vcpu_array, i);\n}\n```\n",
 "is_vulnerable": true
}