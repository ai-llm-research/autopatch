{
  "cwe_type": "Improper Locking",
  "cve_id": "CVE-2025-21801",
  "supplementary_code": "```c\nstruct device {\nstruct kobject kobj;\nstruct device *parent;\nstruct device_private *p;\nconst char *init_name; /* initial name of the device */\nconst struct device_type *type;\nconst struct bus_type *bus; /* type of bus device is on */\nstruct device_driver *driver; /* which driver has allocated this\ndevice */\nvoid *platform_data; /* Platform specific data, device\ncore doesn't touch it */\nvoid *driver_data; /* Driver data, set and get with\ndev_set_drvdata/dev_get_drvdata */\nstruct mutex mutex; /* mutex to synchronize calls to\n* its driver.\n*/\nstruct dev_links_info links;\nstruct dev_pm_info power;\nstruct dev_pm_domain *pm_domain;\n#ifdef CONFIG_ENERGY_MODEL\nstruct em_perf_domain *em_pd;\n#endif\n#ifdef CONFIG_PINCTRL\nstruct dev_pin_info *pins;\n#endif\nstruct dev_msi_info msi;\n#ifdef CONFIG_ARCH_HAS_DMA_OPS\nconst struct dma_map_ops *dma_ops;\n#endif\nu64 *dma_mask; /* dma mask (if dma'able device) */\nu64 coherent_dma_mask;/* Like dma_mask, but for\nalloc_coherent mappings as\nnot all hardware supports\n64 bit addresses for consistent\nallocations such descriptors. */\nu64 bus_dma_limit; /* upstream dma constraint */\nconst struct bus_dma_region *dma_range_map;\nstruct device_dma_parameters *dma_parms;\nstruct list_head dma_pools; /* dma pools (if dma'ble) */\n#ifdef CONFIG_DMA_DECLARE_COHERENT\nstruct dma_coherent_mem *dma_mem; /* internal for coherent mem\noverride */\n#endif\n#ifdef CONFIG_DMA_CMA\nstruct cma *cma_area; /* contiguous memory area for dma\nallocations */\n#endif\n#ifdef CONFIG_SWIOTLB\nstruct io_tlb_mem *dma_io_tlb_mem;\n#endif\n#ifdef CONFIG_SWIOTLB_DYNAMIC\nstruct list_head dma_io_tlb_pools;\nspinlock_t dma_io_tlb_lock;\nbool dma_uses_io_tlb;\n#endif\n/* arch specific additions */\nstruct dev_archdata archdata;\nstruct device_node *of_node; /* associated device tree node */\nstruct fwnode_handle *fwnode; /* firmware device node */\n#ifdef CONFIG_NUMA\nint numa_node; /* NUMA node this device is close to */\n#endif\ndev_t devt; /* dev_t, creates the sysfs \"dev\" */\nu32 id; /* device instance */\nspinlock_t devres_lock;\nstruct list_head devres_head;\nconst struct class *class;\nconst struct attribute_group **groups; /* optional groups */\nvoid (*release)(struct device *dev);\nstruct iommu_group *iommu_group;\nstruct dev_iommu *iommu;\nstruct device_physical_location *physical_location;\nenum device_removable removable;\nbool offline_disabled:1;\nbool offline:1;\nbool of_node_reused:1;\nbool state_synced:1;\nbool can_match:1;\n#if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \\\ndefined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \\\ndefined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL)\nbool dma_coherent:1;\n#endif\n#ifdef CONFIG_DMA_OPS_BYPASS\nbool dma_ops_bypass : 1;\n#endif\n#ifdef CONFIG_DMA_NEED_SYNC\nbool dma_skip_sync:1;\n#endif\n#ifdef CONFIG_IOMMU_DMA\nbool dma_iommu:1;\n#endif\n};\n```\n```c\nstruct net_device {\n/* Cacheline organization can be found documented in\n* Documentation/networking/net_cachelines/net_device.rst.\n* Please update the document when adding new fields.\n*/\n/* TX read-mostly hotpath */\n__cacheline_group_begin(net_device_read_tx);\nstruct_group(priv_flags_fast,\nunsigned long priv_flags:32;\nunsigned long lltx:1;\n);\nconst struct net_device_ops *netdev_ops;\nconst struct header_ops *header_ops;\nstruct netdev_queue *_tx;\nnetdev_features_t gso_partial_features;\nunsigned int real_num_tx_queues;\nunsigned int gso_max_size;\nunsigned int gso_ipv4_max_size;\nu16 gso_max_segs;\ns16 num_tc;\n/* Note : dev->mtu is often read without holding a lock.\n* Writers usually hold RTNL.\n* It is recommended to use READ_ONCE() to annotate the reads,\n* and to use WRITE_ONCE() to annotate the writes.\n*/\nunsigned int mtu;\nunsigned short needed_headroom;\nstruct netdev_tc_txq tc_to_txq[TC_MAX_QUEUE];\n#ifdef CONFIG_XPS\nstruct xps_dev_maps __rcu *xps_maps[XPS_MAPS_MAX];\n#endif\n#ifdef CONFIG_NETFILTER_EGRESS\nstruct nf_hook_entries __rcu *nf_hooks_egress;\n#endif\n#ifdef CONFIG_NET_XGRESS\nstruct bpf_mprog_entry __rcu *tcx_egress;\n#endif\n__cacheline_group_end(net_device_read_tx);\n/* TXRX read-mostly hotpath */\n__cacheline_group_begin(net_device_read_txrx);\nunion {\nstruct pcpu_lstats __percpu *lstats;\nstruct pcpu_sw_netstats __percpu *tstats;\nstruct pcpu_dstats __percpu *dstats;\n};\nunsigned long state;\nunsigned int flags;\nunsigned short hard_header_len;\nnetdev_features_t features;\nstruct inet6_dev __rcu *ip6_ptr;\n__cacheline_group_end(net_device_read_txrx);\n/* RX read-mostly hotpath */\n__cacheline_group_begin(net_device_read_rx);\nstruct bpf_prog __rcu *xdp_prog;\nstruct list_head ptype_specific;\nint ifindex;\nunsigned int real_num_rx_queues;\nstruct netdev_rx_queue *_rx;\nunsigned int gro_max_size;\nunsigned int gro_ipv4_max_size;\nrx_handler_func_t __rcu *rx_handler;\nvoid __rcu *rx_handler_data;\npossible_net_t nd_net;\n#ifdef CONFIG_NETPOLL\nstruct netpoll_info __rcu *npinfo;\n#endif\n#ifdef CONFIG_NET_XGRESS\nstruct bpf_mprog_entry __rcu *tcx_ingress;\n#endif\n__cacheline_group_end(net_device_read_rx);\nchar name[IFNAMSIZ];\nstruct netdev_name_node *name_node;\nstruct dev_ifalias __rcu *ifalias;\n/*\n* I/O specific fields\n* FIXME: Merge these and struct ifmap into one\n*/\nunsigned long mem_end;\nunsigned long mem_start;\nunsigned long base_addr;\n/*\n* Some hardware also needs these fields (state,dev_list,\n* napi_list,unreg_list,close_list) but they are not\n* part of the usual set specified in Space.c.\n*/\nstruct list_head dev_list;\nstruct list_head napi_list;\nstruct list_head unreg_list;\nstruct list_head close_list;\nstruct list_head ptype_all;\nstruct {\nstruct list_head upper;\nstruct list_head lower;\n} adj_list;\n/* Read-mostly cache-line for fast-path access */\nxdp_features_t xdp_features;\nconst struct xdp_metadata_ops *xdp_metadata_ops;\nconst struct xsk_tx_metadata_ops *xsk_tx_metadata_ops;\nunsigned short gflags;\nunsigned short needed_tailroom;\nnetdev_features_t hw_features;\nnetdev_features_t wanted_features;\nnetdev_features_t vlan_features;\nnetdev_features_t hw_enc_features;\nnetdev_features_t mpls_features;\nunsigned int min_mtu;\nunsigned int max_mtu;\nunsigned short type;\nunsigned char min_header_len;\nunsigned char name_assign_type;\nint group;\nstruct net_device_stats stats; /* not used by modern drivers */\nstruct net_device_core_stats __percpu *core_stats;\n/* Stats to monitor link on/off, flapping */\natomic_t carrier_up_count;\natomic_t carrier_down_count;\n#ifdef CONFIG_WIRELESS_EXT\nconst struct iw_handler_def *wireless_handlers;\n#endif\nconst struct ethtool_ops *ethtool_ops;\n#ifdef CONFIG_NET_L3_MASTER_DEV\nconst struct l3mdev_ops *l3mdev_ops;\n#endif\n#if IS_ENABLED(CONFIG_IPV6)\nconst struct ndisc_ops *ndisc_ops;\n#endif\n#ifdef CONFIG_XFRM_OFFLOAD\nconst struct xfrmdev_ops *xfrmdev_ops;\n#endif\n#if IS_ENABLED(CONFIG_TLS_DEVICE)\nconst struct tlsdev_ops *tlsdev_ops;\n#endif\nunsigned int operstate;\nunsigned char link_mode;\nunsigned char if_port;\nunsigned char dma;\n/* Interface address info. */\nunsigned char perm_addr[MAX_ADDR_LEN];\nunsigned char addr_assign_type;\nunsigned char addr_len;\nunsigned char upper_level;\nunsigned char lower_level;\nunsigned short neigh_priv_len;\nunsigned short dev_id;\nunsigned short dev_port;\nint irq;\nu32 priv_len;\nspinlock_t addr_list_lock;\nstruct netdev_hw_addr_list uc;\nstruct netdev_hw_addr_list mc;\nstruct netdev_hw_addr_list dev_addrs;\n#ifdef CONFIG_SYSFS\nstruct kset *queues_kset;\n#endif\n#ifdef CONFIG_LOCKDEP\nstruct list_head unlink_list;\n#endif\nunsigned int promiscuity;\nunsigned int allmulti;\nbool uc_promisc;\n#ifdef CONFIG_LOCKDEP\nunsigned char nested_level;\n#endif\n/* Protocol-specific pointers */\nstruct in_device __rcu *ip_ptr;\n/** @fib_nh_head: nexthops associated with this netdev */\nstruct hlist_head fib_nh_head;\n#if IS_ENABLED(CONFIG_VLAN_8021Q)\nstruct vlan_info __rcu *vlan_info;\n#endif\n#if IS_ENABLED(CONFIG_NET_DSA)\nstruct dsa_port *dsa_ptr;\n#endif\n#if IS_ENABLED(CONFIG_TIPC)\nstruct tipc_bearer __rcu *tipc_ptr;\n#endif\n#if IS_ENABLED(CONFIG_ATALK)\nvoid *atalk_ptr;\n#endif\n#if IS_ENABLED(CONFIG_AX25)\nvoid *ax25_ptr;\n#endif\n#if IS_ENABLED(CONFIG_CFG80211)\nstruct wireless_dev *ieee80211_ptr;\n#endif\n#if IS_ENABLED(CONFIG_IEEE802154) || IS_ENABLED(CONFIG_6LOWPAN)\nstruct wpan_dev *ieee802154_ptr;\n#endif\n#if IS_ENABLED(CONFIG_MPLS_ROUTING)\nstruct mpls_dev __rcu *mpls_ptr;\n#endif\n#if IS_ENABLED(CONFIG_MCTP)\nstruct mctp_dev __rcu *mctp_ptr;\n#endif\n/*\n* Cache lines mostly used on receive path (including eth_type_trans())\n*/\n/* Interface address info used in eth_type_trans() */\nconst unsigned char *dev_addr;\nunsigned int num_rx_queues;\n#define GRO_LEGACY_MAX_SIZE 65536u\n/* TCP minimal MSS is 8 (TCP_MIN_GSO_SIZE),\n* and shinfo->gso_segs is a 16bit field.\n*/\n#define GRO_MAX_SIZE (8 * 65535u)\nunsigned int xdp_zc_max_segs;\nstruct netdev_queue __rcu *ingress_queue;\n#ifdef CONFIG_NETFILTER_INGRESS\nstruct nf_hook_entries __rcu *nf_hooks_ingress;\n#endif\nunsigned char broadcast[MAX_ADDR_LEN];\n#ifdef CONFIG_RFS_ACCEL\nstruct cpu_rmap *rx_cpu_rmap;\n#endif\nstruct hlist_node index_hlist;\n/*\n* Cache lines mostly used on transmit path\n*/\nunsigned int num_tx_queues;\nstruct Qdisc __rcu *qdisc;\nunsigned int tx_queue_len;\nspinlock_t tx_global_lock;\nstruct xdp_dev_bulk_queue __percpu *xdp_bulkq;\n#ifdef CONFIG_NET_SCHED\nDECLARE_HASHTABLE (qdisc_hash, 4);\n#endif\n/* These may be needed for future network-power-down code. */\nstruct timer_list watchdog_timer;\nint watchdog_timeo;\nu32 proto_down_reason;\nstruct list_head todo_list;\n#ifdef CONFIG_PCPU_DEV_REFCNT\nint __percpu *pcpu_refcnt;\n#else\nrefcount_t dev_refcnt;\n#endif\nstruct ref_tracker_dir refcnt_tracker;\nstruct list_head link_watch_list;\nu8 reg_state;\nbool dismantle;\nenum {\nRTNL_LINK_INITIALIZED,\nRTNL_LINK_INITIALIZING,\n} rtnl_link_state:16;\nbool needs_free_netdev;\nvoid (*priv_destructor)(struct net_device *dev);\n/* mid-layer private */\nvoid *ml_priv;\nenum netdev_ml_priv_type ml_priv_type;\nenum netdev_stat_type pcpu_stat_type:8;\n#if IS_ENABLED(CONFIG_GARP)\nstruct garp_port __rcu *garp_port;\n#endif\n#if IS_ENABLED(CONFIG_MRP)\nstruct mrp_port __rcu *mrp_port;\n#endif\n#if IS_ENABLED(CONFIG_NET_DROP_MONITOR)\nstruct dm_hw_stat_delta __rcu *dm_private;\n#endif\nstruct device dev;\nconst struct attribute_group *sysfs_groups[4];\nconst struct attribute_group *sysfs_rx_queue_group;\nconst struct rtnl_link_ops *rtnl_link_ops;\nconst struct netdev_stat_ops *stat_ops;\nconst struct netdev_queue_mgmt_ops *queue_mgmt_ops;\n/* for setting kernel sock attribute on TCP connection setup */\n#define GSO_MAX_SEGS 65535u\n#define GSO_LEGACY_MAX_SIZE 65536u\n/* TCP minimal MSS is 8 (TCP_MIN_GSO_SIZE),\n* and shinfo->gso_segs is a 16bit field.\n*/\n#define GSO_MAX_SIZE (8 * GSO_MAX_SEGS)\n#define TSO_LEGACY_MAX_SIZE 65536\n#define TSO_MAX_SIZE UINT_MAX\nunsigned int tso_max_size;\n#define TSO_MAX_SEGS U16_MAX\nu16 tso_max_segs;\n#ifdef CONFIG_DCB\nconst struct dcbnl_rtnl_ops *dcbnl_ops;\n#endif\nu8 prio_tc_map[TC_BITMASK + 1];\n#if IS_ENABLED(CONFIG_FCOE)\nunsigned int fcoe_ddp_xid;\n#endif\n#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)\nstruct netprio_map __rcu *priomap;\n#endif\nstruct phy_link_topology *link_topo;\nstruct phy_device *phydev;\nstruct sfp_bus *sfp_bus;\nstruct lock_class_key *qdisc_tx_busylock;\nbool proto_down;\nbool threaded;\n/* priv_flags_slow, ungrouped to save space */\nunsigned long see_all_hwtstamp_requests:1;\nunsigned long change_proto_down:1;\nunsigned long netns_local:1;\nunsigned long fcoe_mtu:1;\nstruct list_head net_notifier_list;\n#if IS_ENABLED(CONFIG_MACSEC)\n/* MACsec management functions */\nconst struct macsec_ops *macsec_ops;\n#endif\nconst struct udp_tunnel_nic_info *udp_tunnel_nic_info;\nstruct udp_tunnel_nic *udp_tunnel_nic;\nstruct ethtool_netdev_state *ethtool;\n/* protected by rtnl_lock */\nstruct bpf_xdp_entity xdp_state[__MAX_XDP_MODE];\nu8 dev_addr_shadow[MAX_ADDR_LEN];\nnetdevice_tracker linkwatch_dev_tracker;\nnetdevice_tracker watchdog_dev_tracker;\nnetdevice_tracker dev_registered_tracker;\nstruct rtnl_hw_stats64 *offload_xstats_l3;\nstruct devlink_port *devlink_port;\n#if IS_ENABLED(CONFIG_DPLL)\nstruct dpll_pin __rcu *dpll_pin;\n#endif\n#if IS_ENABLED(CONFIG_PAGE_POOL)\n/** @page_pools: page pools created for this netdevice */\nstruct hlist_head page_pools;\n#endif\n/** @irq_moder: dim parameters used if IS_ENABLED(CONFIG_DIMLIB). */\nstruct dim_irq_moder *irq_moder;\nu64 max_pacing_offload_horizon;\nstruct napi_config *napi_config;\nunsigned long gro_flush_timeout;\nu32 napi_defer_hard_irqs;\n/**\n* @lock: protects @net_shaper_hierarchy, feel free to use for other\n* netdev-scope protection. Ordering: take after rtnl_lock.\n*/\nstruct mutex lock;\n#if IS_ENABLED(CONFIG_NET_SHAPER)\n/**\n* @net_shaper_hierarchy: data tracking the current shaper status\n* see include/net/net_shapers.h\n*/\nstruct net_shaper_hierarchy *net_shaper_hierarchy;\n#endif\nstruct hlist_head neighbours[NEIGH_NR_TABLES];\nu8 priv[] ____cacheline_aligned\n__counted_by(priv_len);\n} ____cacheline_aligned;\n#define to_net_dev(d) container_of(d, struct net_device, dev)\n```\n```c\nstatic inline void *dev_get_drvdata(const struct device *dev)\n{\nreturn dev->driver_data;\n}\n```\n```c\nstruct ravb_private {\nstruct net_device *ndev;\nstruct platform_device *pdev;\nvoid __iomem *addr;\nstruct clk *clk;\nstruct clk *refclk;\nstruct clk *gptp_clk;\nstruct mdiobb_ctrl mdiobb;\nu32 num_rx_ring[NUM_RX_QUEUE];\nu32 num_tx_ring[NUM_TX_QUEUE];\nu32 desc_bat_size;\ndma_addr_t desc_bat_dma;\nstruct ravb_desc *desc_bat;\ndma_addr_t rx_desc_dma[NUM_RX_QUEUE];\ndma_addr_t tx_desc_dma[NUM_TX_QUEUE];\nunion {\nstruct ravb_rx_desc *desc;\nstruct ravb_ex_rx_desc *ex_desc;\nvoid *raw;\n} rx_ring[NUM_RX_QUEUE];\nstruct ravb_tx_desc *tx_ring[NUM_TX_QUEUE];\nvoid *tx_align[NUM_TX_QUEUE];\nstruct sk_buff *rx_1st_skb;\nstruct page_pool *rx_pool[NUM_RX_QUEUE];\nstruct ravb_rx_buffer *rx_buffers[NUM_RX_QUEUE];\nstruct sk_buff **tx_skb[NUM_TX_QUEUE];\nu32 rx_over_errors;\nu32 rx_fifo_errors;\nstruct net_device_stats stats[NUM_RX_QUEUE];\nu32 tstamp_tx_ctrl;\nu32 tstamp_rx_ctrl;\nstruct list_head ts_skb_list;\nu32 ts_skb_tag;\nstruct ravb_ptp ptp;\nspinlock_t lock; /* Register access lock */\nu32 cur_rx[NUM_RX_QUEUE]; /* Consumer ring indices */\nu32 dirty_rx[NUM_RX_QUEUE]; /* Producer ring indices */\nu32 cur_tx[NUM_TX_QUEUE];\nu32 dirty_tx[NUM_TX_QUEUE];\nstruct napi_struct napi[NUM_RX_QUEUE];\nstruct work_struct work;\n/* MII transceiver section. */\nstruct mii_bus *mii_bus; /* MDIO bus control */\nint link;\nphy_interface_t phy_interface;\nint msg_enable;\nint speed;\nint emac_irq;\nunsigned no_avb_link:1;\nunsigned avb_link_active_low:1;\nunsigned wol_enabled:1;\nunsigned rxcidm:1; /* RX Clock Internal Delay Mode */\nunsigned txcidm:1; /* TX Clock Internal Delay Mode */\nunsigned rgmii_override:1; /* Deprecated rgmii-*id behavior */\nunsigned int num_tx_desc; /* TX descriptors per packet */\nint duplex;\nconst struct ravb_hw_info *info;\nstruct reset_control *rstc;\nu32 gti_tiv;\n};\n```\n```c\nstatic inline void *netdev_priv(const struct net_device *dev)\n{\nreturn (void *)dev->priv;\n}\n```\n```c\nstatic inline bool netif_running(const struct net_device *dev)\n{\nreturn test_bit(__LINK_STATE_START, &dev->state);\n}\n```\n```c\nvoid netif_device_detach(struct net_device *dev)\n{\nif (test_and_clear_bit(__LINK_STATE_PRESENT, &dev->state) &&\nnetif_running(dev)) {\nnetif_tx_stop_all_queues(dev);\n}\n}\nEXPORT_SYMBOL(netif_device_detach);\n```\n```c\nstatic int ravb_wol_setup(struct net_device *ndev)\n{\nstruct ravb_private *priv = netdev_priv(ndev);\nconst struct ravb_hw_info *info = priv->info;\n/* Disable interrupts by clearing the interrupt masks. */\nravb_write(ndev, 0, RIC0);\nravb_write(ndev, 0, RIC2);\nravb_write(ndev, 0, TIC);\n/* Only allow ECI interrupts */\nsynchronize_irq(priv->emac_irq);\nif (info->nc_queues)\nnapi_disable(&priv->napi[RAVB_NC]);\nnapi_disable(&priv->napi[RAVB_BE]);\nravb_write(ndev, ECSIPR_MPDIP, ECSIPR);\n/* Enable MagicPacket */\nravb_modify(ndev, ECMR, ECMR_MPDE, ECMR_MPDE);\nif (priv->info->ccc_gac)\nravb_ptp_stop(ndev);\nreturn enable_irq_wake(priv->emac_irq);\n}\n```\n```c\nstatic int ravb_close(struct net_device *ndev)\n{\nstruct device_node *np = ndev->dev.parent->of_node;\nstruct ravb_private *priv = netdev_priv(ndev);\nconst struct ravb_hw_info *info = priv->info;\nstruct ravb_tstamp_skb *ts_skb, *ts_skb2;\nstruct device *dev = &priv->pdev->dev;\nint error;\nnetif_tx_stop_all_queues(ndev);\n/* Disable interrupts by clearing the interrupt masks. */\nravb_write(ndev, 0, RIC0);\nravb_write(ndev, 0, RIC2);\nravb_write(ndev, 0, TIC);\n/* PHY disconnect */\nif (ndev->phydev) {\nphy_stop(ndev->phydev);\nphy_disconnect(ndev->phydev);\nif (of_phy_is_fixed_link(np))\nof_phy_deregister_fixed_link(np);\n}\n/* Stop PTP Clock driver */\nif (info->gptp || info->ccc_gac)\nravb_ptp_stop(ndev);\n/* Set the config mode to stop the AVB-DMAC's processes */\nif (ravb_stop_dma(ndev) < 0)\nnetdev_err(ndev,\n\"device will be stopped after h/w processes are done.\\n\");\n/* Clear the timestamp list */\nif (info->gptp || info->ccc_gac) {\nlist_for_each_entry_safe(ts_skb, ts_skb2, &priv->ts_skb_list, list) {\nlist_del(&ts_skb->list);\nkfree_skb(ts_skb->skb);\nkfree(ts_skb);\n}\n}\ncancel_work_sync(&priv->work);\nif (info->nc_queues)\nnapi_disable(&priv->napi[RAVB_NC]);\nnapi_disable(&priv->napi[RAVB_BE]);\n/* Free all the skb's in the RX queue and the DMA buffers. */\nravb_ring_free(ndev, RAVB_BE);\nif (info->nc_queues)\nravb_ring_free(ndev, RAVB_NC);\n/* Update statistics. */\nravb_get_stats(ndev);\n/* Set reset mode. */\nerror = ravb_set_opmode(ndev, CCC_OPC_RESET);\nif (error)\nreturn error;\npm_runtime_mark_last_busy(dev);\npm_runtime_put_autosuspend(dev);\nreturn 0;\n}\n```\n```c\nstatic inline int pm_runtime_force_suspend(struct device *dev) { return 0; }\n```\n```c\nstatic inline int reset_control_assert(struct reset_control *rstc)\n{\nreturn 0;\n}\n```",
  "original_code": "```c\nstatic int ravb_suspend(struct device *dev)\n{\nstruct net_device *ndev = dev_get_drvdata(dev);\nstruct ravb_private *priv = netdev_priv(ndev);\nint ret;\nif (!netif_running(ndev))\ngoto reset_assert;\nnetif_device_detach(ndev);\nif (priv->wol_enabled)\nreturn ravb_wol_setup(ndev);\nret = ravb_close(ndev);\nif (ret)\nreturn ret;\nret = pm_runtime_force_suspend(&priv->pdev->dev);\nif (ret)\nreturn ret;\nreset_assert:\nreturn reset_control_assert(priv->rstc);\n}\n```",
  "vuln_patch": "```c\nstatic int ravb_suspend(struct device *dev)\n{\nstruct net_device *ndev = dev_get_drvdata(dev);\nstruct ravb_private *priv = netdev_priv(ndev);\nint ret;\nif (!netif_running(ndev))\ngoto reset_assert;\nnetif_device_detach(ndev);\nrtnl_lock();\nif (priv->wol_enabled) {\nret = ravb_wol_setup(ndev);\nrtnl_unlock();\nreturn ret;\n}\nret = ravb_close(ndev);\nrtnl_unlock();\nif (ret)\nreturn ret;\nret = pm_runtime_force_suspend(&priv->pdev->dev);\nif (ret)\nreturn ret;\nreset_assert:\nreturn reset_control_assert(priv->rstc);\n}\n```",
  "function_name": "ravb_suspend",
  "function_prototype": "static int ravb_suspend(struct device *dev)",
  "code_semantics": "The function handles the suspension of a network device. It first retrieves the network device associated with the given device. It checks if the network interface is active; if not, it asserts a reset control. If active, it detaches the device from the network stack. If Wake-on-LAN is enabled, it sets up the necessary configurations. Otherwise, it closes the network device by stopping network queues, disabling interrupts, disconnecting the PHY, stopping the PTP clock, and freeing resources. It then forces the device into a suspended state for power management and asserts a reset control to ensure the device is in a reset state.",
  "safe_verification_cot": "1. The Target Code acquires the RTNL lock using rtnl_lock() before calling ravb_wol_setup(ndev) or ravb_close(ndev), ensuring protection from race conditions. 2. The RTNL lock is released using rtnl_unlock() after the critical section, maintaining synchronization. 3. netif_device_detach(ndev) is called before entering the critical section, now effective with added locking. 4. netif_running(ndev) is checked, now part of a properly synchronized sequence.",
  "verification_cot": "1. The Vulnerable Code does not acquire the RTNL lock using rtnl_lock() before calling ravb_wol_setup(ndev) or ravb_close(ndev), allowing race conditions. 2. The lock is not released using rtnl_unlock(), leaving the critical section unprotected. 3. netif_device_detach(ndev) is correctly called before the critical section, but without proper locking. 4. netif_running(ndev) is checked, but without proper locking, it does not prevent race conditions.",
  "vulnerability_related_variables": {
    "ndev": "This variable represents a network interface object that is retrieved from a device object. It is used to check the operational status of the network interface, detach the network interface from the system, and perform specific setup or closure operations on the network interface.",
    "priv": "This variable represents a private data structure associated with a network interface. It is used to access specific configuration settings and hardware controls related to the network interface, such as enabling wake-on-LAN functionality, accessing the parent device, and asserting a reset control."
  },
  "vulnerability_related_functions": {
    "netif_running": "Checks a specific condition in a data structure to determine if a component is active.",
    "netif_device_detach": "Modifies a state indicator and halts operations if certain conditions are met.",
    "ravb_wol_setup": "Configures a component to respond to specific external signals by adjusting settings and enabling certain features.",
    "ravb_close": "Disables a component, stops its operations, disconnects external interfaces, and releases allocated resources.",
    "rtnl_lock": "Manages access to shared resources by acquiring a synchronization mechanism.",
    "rtnl_unlock": "Manages access to shared resources by releasing a synchronization mechanism."
  },
  "root_cause": "Lack of proper locking (rtnl_lock) in the ravb_suspend function leading to race conditions.",
  "patch_cot": "First, identify the critical sections in the ravb_suspend function where race conditions could occur. These are the sections where ndev and priv are accessed or modified. Introduce rtnl_lock() before these critical sections to ensure exclusive access. After the critical operations are completed, use rtnl_unlock() to release the lock. Specifically, place rtnl_lock() before calling ravb_wol_setup(ndev) and ravb_close(ndev), and rtnl_unlock() immediately after these calls. This will ensure that the operations on ndev and priv are atomic and not interrupted by other threads, thus preventing race conditions."
}