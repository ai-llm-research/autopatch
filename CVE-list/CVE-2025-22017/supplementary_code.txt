```c
struct devlink_rel {
    u32 index;
    refcount_t refcount;
    u32 devlink_index;
    struct {
        u32 devlink_index;
        u32 obj_index;
        devlink_rel_notify_cb_t *notify_cb;
        devlink_rel_cleanup_cb_t *cleanup_cb;
        struct delayed_work notify_work;
    } nested_in;
};
```

```c
typedef __u32 u32;
```

```c
#define kzalloc(...)                alloc_hooks(kzalloc_noprof(__VA_ARGS__))
```

```c
#define GFP_KERNEL  (__GFP_RECLAIM | __GFP_IO | __GFP_FS)
```

```c
static inline void * __must_check ERR_PTR(long error)
{
    return (void *) error;
}
```

```c
#define ENOMEM      12  /* Out of memory */
```

```c
static inline int xa_alloc_cyclic(struct xarray *xa, u32 *id, void *entry,
        struct xa_limit limit, u32 *next, gfp_t gfp)
{
    int err;

    might_alloc(gfp);
    xa_lock(xa);
    err = __xa_alloc_cyclic(xa, id, entry, limit, next, gfp);
    xa_unlock(xa);

    return err;
}
```

```c
#define xa_limit_32b    XA_LIMIT(0, UINT_MAX)
```

```c
void kfree(const void *object)
{
    struct folio *folio;
    struct slab *slab;
    struct kmem_cache *s;
    void *x = (void *)object;

    trace_kfree(_RET_IP_, object);

    if (unlikely(ZERO_OR_NULL_PTR(object)))
        return;

    folio = virt_to_folio(object);
    if (unlikely(!folio_test_slab(folio))) {
        free_large_kmalloc(folio, (void *)object);
        return;
    }

    slab = folio_slab(folio);
    s = slab->slab_cache;
    slab_free(s, slab, x, _RET_IP_);
}
EXPORT_SYMBOL(kfree);
```

```c
static inline void refcount_set(refcount_t *r, int n)
{
    atomic_set(&r->refs, n);
}
```

```c
#define INIT_DELAYED_WORK(_work, _func)                 \
    __INIT_DELAYED_WORK(_work, _func, 0)
```

```c
static void devlink_rel_nested_in_notify_work(struct work_struct *work)
{
    struct devlink_rel *rel = container_of(work, struct devlink_rel,
                           nested_in.notify_work.work);
    struct devlink *devlink;

    devlink = devlinks_xa_get(rel->nested_in.devlink_index);
    if (!devlink)
        goto rel_put;
    if (!devl_trylock(devlink)) {
        devlink_put(devlink);
        goto reschedule_work;
    }
    if (!devl_is_registered(devlink)) {
        devl_unlock(devlink);
        devlink_put(devlink);
        goto rel_put;
    }
    if (!xa_get_mark(&devlink_rels, rel->index, DEVLINK_REL_IN_USE))
        rel->nested_in.cleanup_cb(devlink, rel->nested_in.obj_index, rel->index);
    rel->nested_in.notify_cb(devlink, rel->nested_in.obj_index);
    devl_unlock(devlink);
    devlink_put(devlink);

rel_put:
    __devlink_rel_put(rel);
    return;

reschedule_work:
    schedule_delayed_work(&rel->nested_in.notify_work, 1);
}
```
