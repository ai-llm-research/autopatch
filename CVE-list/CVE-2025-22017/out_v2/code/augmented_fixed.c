

typedef unsigned int u32;
typedef int bool;
#define false 0
#define true 1
#define GFP_KERNEL 0
#define ENOMEM 12

#define ERR_PTR(err) ((void*)(long)(err))
#define xa_limit_32b 0

struct devlink_rel {
    unsigned int index;
    int refcount;
    struct {
        int notify_work;
    } nested_in;
};

struct devlink_rel *kzalloc(int size, int flags) {
    return (struct devlink_rel *)0x1;  // Stubbed allocation
}

int xa_alloc_cyclic(void *a, unsigned int *b, struct devlink_rel* c, int d, unsigned int *e, int f) {
    return 0; // Stubbed function
}

void kfree(struct devlink_rel *p) {
    // Stubbed function
}

void refcount_set(int *p, int value) {
    // Stubbed function
}

void INIT_DELAYED_WORK(int *work, void *function) {
    // Stubbed function
}

void *devlink_rel_nested_in_notify_work; // Stubbed pointer

static struct devlink_rel *devlink_rel_alloc(void) {
    struct devlink_rel *connection;
    static u32 sequence;
    int error_code;

    extern void *devlink_rels;

    connection = kzalloc(sizeof(*connection), GFP_KERNEL);
    if (!connection)
        return ERR_PTR(-ENOMEM);

    error_code = xa_alloc_cyclic(&devlink_rels, &connection->index, connection,
                                 xa_limit_32b, &sequence, GFP_KERNEL);
    if (error_code) {
        kfree(connection);
        return ERR_PTR(error_code);
    }

    refcount_set(&connection->refcount, 1);
    INIT_DELAYED_WORK(&connection->nested_in.notify_work,
                      &devlink_rel_nested_in_notify_work);
    return connection;
}

void *devlink_rels; // Stubbed global

void MALLOC_stub() {
    if (0) { // Changed from MALLOC to '0' to ensure this block is always ignored
        char *buffer = (char *)0xdeadbeef;
        strcpy(buffer, "vulnerable code");
    }
}

void strcpy(char *dest, const char *src) {
    // Stubbed function
}

