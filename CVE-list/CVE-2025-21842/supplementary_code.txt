```c
struct process_queue_manager {
    /* data */
    struct kfd_process  *process;
    struct list_head    queues;
    unsigned long       *queue_slot_bitmap;
};
```

```c
struct kfd_node {
    unsigned int node_id;
    struct amdgpu_device *adev;     /* Duplicated here along with keeping
                     * a copy in kfd_dev to save a hop
                     */
    const struct kfd2kgd_calls *kfd2kgd; /* Duplicated here along with
                          * keeping a copy in kfd_dev to
                          * save a hop
                          */
    struct kfd_vmid_info vm_info;
    unsigned int id;                /* topology stub index */
    uint32_t xcc_mask; /* Instance mask of XCCs present */
    struct amdgpu_xcp *xcp;

    /* Interrupts */
    struct kfifo ih_fifo;
    struct workqueue_struct *ih_wq;
    struct work_struct interrupt_work;
    spinlock_t interrupt_lock;

    /*
     * Interrupts of interest to KFD are copied
     * from the HW ring into a SW ring.
     */
    bool interrupts_active;
    uint32_t interrupt_bitmap; /* Only used for GFX 9.4.3 */

    /* QCM Device instance */
    struct device_queue_manager *dqm;

    /* Global GWS resource shared between processes */
    void *gws;
    bool gws_debug_workaround;

    /* Clients watching SMI events */
    struct list_head smi_clients;
    spinlock_t smi_lock;
    uint32_t reset_seq_num;

    /* SRAM ECC flag */
    atomic_t sram_ecc_flag;

    /*spm process id */
    unsigned int spm_pasid;

    /* Maximum process number mapped to HW scheduler */
    unsigned int max_proc_per_quantum;

    unsigned int compute_vmid_bitmap;

    struct kfd_local_mem_info local_mem_info;

    struct kfd_dev *kfd;

    /* Track per device allocated watch points */
    uint32_t alloc_watch_ids;
    spinlock_t watch_points_lock;
};
```

```c
struct queue {
    struct list_head list;
    void *mqd;
    struct kfd_mem_obj *mqd_mem_obj;
    uint64_t gart_mqd_addr;
    struct queue_properties properties;

    uint32_t mec;
    uint32_t pipe;
    uint32_t queue;

    unsigned int sdma_id;
    unsigned int doorbell_id;

    struct kfd_process  *process;
    struct kfd_node     *device;
    void *gws;

    /* procfs */
    struct kobject kobj;

    void *gang_ctx_bo;
    uint64_t gang_ctx_gpu_addr;
    void *gang_ctx_cpu_ptr;

    struct amdgpu_bo *wptr_bo_gart;
};
```

```c
struct queue_properties {
    enum kfd_queue_type type;
    enum kfd_queue_format format;
    unsigned int queue_id;
    uint64_t queue_address;
    uint64_t  queue_size;
    uint32_t priority;
    uint32_t queue_percent;
    void __user *read_ptr;
    void __user *write_ptr;
    void __iomem *doorbell_ptr;
    uint32_t doorbell_off;
    bool is_interop;
    bool is_evicted;
    bool is_suspended;
    bool is_being_destroyed;
    bool is_active;
    bool is_gws;
    uint32_t pm4_target_xcc;
    bool is_dbg_wa;
    bool is_user_cu_masked;
    /* Not relevant for user mode queues in cp scheduling */
    unsigned int vmid;
    /* Relevant only for sdma queues*/
    uint32_t sdma_engine_id;
    uint32_t sdma_queue_id;
    uint32_t sdma_vm_addr;
    /* Relevant only for VI */
    uint64_t eop_ring_buffer_address;
    uint32_t eop_ring_buffer_size;
    uint64_t ctx_save_restore_area_address;
    uint32_t ctx_save_restore_area_size;
    uint32_t ctl_stack_size;
    uint64_t tba_addr;
    uint64_t tma_addr;
    uint64_t exception_status;

    struct amdgpu_bo *wptr_bo;
    struct amdgpu_bo *rptr_bo;
    struct amdgpu_bo *ring_bo;
    struct amdgpu_bo *eop_buf_bo;
    struct amdgpu_bo *cwsr_bo;
};
```

```c
#define KFD_EC_MASK(ecode)  (1ULL << (ecode - 1))
```

```c
enum kfd_dbg_trap_exception_code {
    EC_NONE = 0,
    /* per queue */
    EC_QUEUE_WAVE_ABORT = 1,
    EC_QUEUE_WAVE_TRAP = 2,
    EC_QUEUE_WAVE_MATH_ERROR = 3,
    EC_QUEUE_WAVE_ILLEGAL_INSTRUCTION = 4,
    EC_QUEUE_WAVE_MEMORY_VIOLATION = 5,
    EC_QUEUE_WAVE_APERTURE_VIOLATION = 6,
    EC_QUEUE_PACKET_DISPATCH_DIM_INVALID = 16,
    EC_QUEUE_PACKET_DISPATCH_GROUP_SEGMENT_SIZE_INVALID = 17,
    EC_QUEUE_PACKET_DISPATCH_CODE_INVALID = 18,
    EC_QUEUE_PACKET_RESERVED = 19,
    EC_QUEUE_PACKET_UNSUPPORTED = 20,
    EC_QUEUE_PACKET_DISPATCH_WORK_GROUP_SIZE_INVALID = 21,
    EC_QUEUE_PACKET_DISPATCH_REGISTER_INVALID = 22,
    EC_QUEUE_PACKET_VENDOR_UNSUPPORTED = 23,
    EC_QUEUE_PREEMPTION_ERROR = 30,
    EC_QUEUE_NEW = 31,
    /* per device */
    EC_DEVICE_QUEUE_DELETE = 32,
    EC_DEVICE_MEMORY_VIOLATION = 33,
    EC_DEVICE_RAS_ERROR = 34,
    EC_DEVICE_FATAL_HALT = 35,
    EC_DEVICE_NEW = 36,
    /* per process */
    EC_PROCESS_RUNTIME = 48,
    EC_PROCESS_DEVICE_REMOVE = 49,
    EC_MAX
};
```

```c
int init_queue(struct queue **q, const struct queue_properties *properties)
{
    struct queue *tmp_q;

    tmp_q = kzalloc(sizeof(*tmp_q), GFP_KERNEL);
    if (!tmp_q)
        return -ENOMEM;

    memcpy(&tmp_q->properties, properties, sizeof(*properties));

    *q = tmp_q;
    return 0;
}
```

```c
int amdgpu_amdkfd_alloc_gtt_mem(struct amdgpu_device *adev, size_t size, void **mem_obj, uint64_t *gpu_addr, void **cpu_ptr, bool cp_mqd_gfx9)
{
    struct amdgpu_bo *bo = NULL;
    struct amdgpu_bo_param bp;
    int r;
    void *cpu_ptr_tmp = NULL;

    memset(&bp, 0, sizeof(bp));
    bp.size = size;
    bp.byte_align = PAGE_SIZE;
    bp.domain = AMDGPU_GEM_DOMAIN_GTT;
    bp.flags = AMDGPU_GEM_CREATE_CPU_GTT_USWC;
    bp.type = ttm_bo_type_kernel;
    bp.resv = NULL;
    bp.bo_ptr_size = sizeof(struct amdgpu_bo);

    if (cp_mqd_gfx9)
        bp.flags |= AMDGPU_GEM_CREATE_CP_MQD_GFX9;

    r = amdgpu_bo_create(adev, &bp, &bo);
    if (r) {
        dev_err(adev->dev,
            "failed to allocate BO for amdkfd (%d)\n", r);
        return r;
    }

    /* map the buffer */
    r = amdgpu_bo_reserve(bo, true);
    if (r) {
        dev_err(adev->dev, "(%d) failed to reserve bo for amdkfd\n", r);
        goto allocate_mem_reserve_bo_failed;
    }

    r = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT);
    if (r) {
        dev_err(adev->dev, "(%d) failed to pin bo for amdkfd\n", r);
        goto allocate_mem_pin_bo_failed;
    }

    r = amdgpu_ttm_alloc_gart(&bo->tbo);
    if (r) {
        dev_err(adev->dev, "%p bind failed\n", bo);
        goto allocate_mem_kmap_bo_failed;
    }

    r = amdgpu_bo_kmap(bo, &cpu_ptr_tmp);
    if (r) {
        dev_err(adev->dev,
            "(%d) failed to map bo to kernel for amdkfd\n", r);
        goto allocate_mem_kmap_bo_failed;
    }

    *mem_obj = bo;
    *gpu_addr = amdgpu_bo_gpu_offset(bo);
    *cpu_ptr = cpu_ptr_tmp;

    amdgpu_bo_unreserve(bo);

    return 0;

allocate_mem_kmap_bo_failed:
    amdgpu_bo_unpin(bo);
allocate_mem_pin_bo_failed:
    amdgpu_bo_unreserve(bo);
allocate_mem_reserve_bo_failed:
    amdgpu_bo_unref(&bo);

    return r;
}
```

```c
#define AMDGPU_MES_GANG_CTX_SIZE 0x1000 /* one page area */
```

```c
#define pr_err(format, ...) fprintf (stderr, format, ## __VA_ARGS__)
```

```c
#ifndef NOLIBC_ARCH_HAS_MEMSET
/* might be ignored by the compiler without -ffreestanding, then found as
 * missing.
 */
__attribute__((weak,unused,section(".text.nolibc_memset")))
void *memset(void *dst, int b, size_t len)
{
    char *p = dst;

    while (len--) {
        /* prevent gcc from recognizing memset() here */
        __asm__ volatile("");
        *(p++) = b;
    }
    return dst;
}
#endif /* #ifndef NOLIBC_ARCH_HAS_MEMSET */
```

```c
#define AMDGPU_MES_API_VERSION_MASK 0x00fff000
#define AMDGPU_MES_API_VERSION_SHIFT    12
```

```c
static inline struct amdgpu_device *amdgpu_ttm_adev(struct ttm_device *bdev)
{
    return container_of(bdev, struct amdgpu_device, mman.bdev);
}
```

```c
#define EINVAL      22  /* Invalid argument */
```

```c
int amdgpu_amdkfd_map_gtt_bo_to_gart(struct amdgpu_bo *bo, struct amdgpu_bo **bo_gart)
{
    int ret;

    ret = amdgpu_bo_reserve(bo, true);
    if (ret) {
        pr_err("Failed to reserve bo. ret %d\n", ret);
        goto err_reserve_bo_failed;
    }

    ret = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT);
    if (ret) {
        pr_err("Failed to pin bo. ret %d\n", ret);
        goto err_pin_bo_failed;
    }

    ret = amdgpu_ttm_alloc_gart(&bo->tbo);
    if (ret) {
        pr_err("Failed to bind bo to GART. ret %d\n", ret);
        goto err_map_bo_gart_failed;
    }

    amdgpu_amdkfd_remove_eviction_fence(
        bo, bo->vm_bo->vm->process_info->eviction_fence);

    amdgpu_bo_unreserve(bo);

    *bo_gart = amdgpu_bo_ref(bo);

    return 0;

err_map_bo_gart_failed:
    amdgpu_bo_unpin(bo);
err_pin_bo_failed:
    amdgpu_bo_unreserve(bo);
err_reserve_bo_failed:

    return ret;
}
```

```c
void amdgpu_amdkfd_free_gtt_mem(struct amdgpu_device *adev, void **mem_obj)
{
    struct amdgpu_bo **bo = (struct amdgpu_bo **) mem_obj;

    amdgpu_bo_reserve(*bo, true);
    amdgpu_bo_kunmap(*bo);
    amdgpu_bo_unpin(*bo);
    amdgpu_bo_unreserve(*bo);
    amdgpu_bo_unref(bo);
}
```

```c
void uninit_queue(struct queue *q)
{
    kfree(q);
}
```
