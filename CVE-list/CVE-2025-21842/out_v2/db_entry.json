{
  "cwe_type": "Incorrect Type Conversion or Cast",
  "cve_id": "CVE-2025-21842",
  "supplementary_code": "```c\nstruct process_queue_manager {\n/* data */\nstruct kfd_process *process;\nstruct list_head queues;\nunsigned long *queue_slot_bitmap;\n};\n```\n```c\nstruct kfd_node {\nunsigned int node_id;\nstruct amdgpu_device *adev; /* Duplicated here along with keeping\n* a copy in kfd_dev to save a hop\n*/\nconst struct kfd2kgd_calls *kfd2kgd; /* Duplicated here along with\n* keeping a copy in kfd_dev to\n* save a hop\n*/\nstruct kfd_vmid_info vm_info;\nunsigned int id; /* topology stub index */\nuint32_t xcc_mask; /* Instance mask of XCCs present */\nstruct amdgpu_xcp *xcp;\n/* Interrupts */\nstruct kfifo ih_fifo;\nstruct workqueue_struct *ih_wq;\nstruct work_struct interrupt_work;\nspinlock_t interrupt_lock;\n/*\n* Interrupts of interest to KFD are copied\n* from the HW ring into a SW ring.\n*/\nbool interrupts_active;\nuint32_t interrupt_bitmap; /* Only used for GFX 9.4.3 */\n/* QCM Device instance */\nstruct device_queue_manager *dqm;\n/* Global GWS resource shared between processes */\nvoid *gws;\nbool gws_debug_workaround;\n/* Clients watching SMI events */\nstruct list_head smi_clients;\nspinlock_t smi_lock;\nuint32_t reset_seq_num;\n/* SRAM ECC flag */\natomic_t sram_ecc_flag;\n/*spm process id */\nunsigned int spm_pasid;\n/* Maximum process number mapped to HW scheduler */\nunsigned int max_proc_per_quantum;\nunsigned int compute_vmid_bitmap;\nstruct kfd_local_mem_info local_mem_info;\nstruct kfd_dev *kfd;\n/* Track per device allocated watch points */\nuint32_t alloc_watch_ids;\nspinlock_t watch_points_lock;\n};\n```\n```c\nstruct queue {\nstruct list_head list;\nvoid *mqd;\nstruct kfd_mem_obj *mqd_mem_obj;\nuint64_t gart_mqd_addr;\nstruct queue_properties properties;\nuint32_t mec;\nuint32_t pipe;\nuint32_t queue;\nunsigned int sdma_id;\nunsigned int doorbell_id;\nstruct kfd_process *process;\nstruct kfd_node *device;\nvoid *gws;\n/* procfs */\nstruct kobject kobj;\nvoid *gang_ctx_bo;\nuint64_t gang_ctx_gpu_addr;\nvoid *gang_ctx_cpu_ptr;\nstruct amdgpu_bo *wptr_bo_gart;\n};\n```\n```c\nstruct queue_properties {\nenum kfd_queue_type type;\nenum kfd_queue_format format;\nunsigned int queue_id;\nuint64_t queue_address;\nuint64_t queue_size;\nuint32_t priority;\nuint32_t queue_percent;\nvoid __user *read_ptr;\nvoid __user *write_ptr;\nvoid __iomem *doorbell_ptr;\nuint32_t doorbell_off;\nbool is_interop;\nbool is_evicted;\nbool is_suspended;\nbool is_being_destroyed;\nbool is_active;\nbool is_gws;\nuint32_t pm4_target_xcc;\nbool is_dbg_wa;\nbool is_user_cu_masked;\n/* Not relevant for user mode queues in cp scheduling */\nunsigned int vmid;\n/* Relevant only for sdma queues*/\nuint32_t sdma_engine_id;\nuint32_t sdma_queue_id;\nuint32_t sdma_vm_addr;\n/* Relevant only for VI */\nuint64_t eop_ring_buffer_address;\nuint32_t eop_ring_buffer_size;\nuint64_t ctx_save_restore_area_address;\nuint32_t ctx_save_restore_area_size;\nuint32_t ctl_stack_size;\nuint64_t tba_addr;\nuint64_t tma_addr;\nuint64_t exception_status;\nstruct amdgpu_bo *wptr_bo;\nstruct amdgpu_bo *rptr_bo;\nstruct amdgpu_bo *ring_bo;\nstruct amdgpu_bo *eop_buf_bo;\nstruct amdgpu_bo *cwsr_bo;\n};\n```\n```c\n#define KFD_EC_MASK(ecode) (1ULL << (ecode - 1))\n```\n```c\nenum kfd_dbg_trap_exception_code {\nEC_NONE = 0,\n/* per queue */\nEC_QUEUE_WAVE_ABORT = 1,\nEC_QUEUE_WAVE_TRAP = 2,\nEC_QUEUE_WAVE_MATH_ERROR = 3,\nEC_QUEUE_WAVE_ILLEGAL_INSTRUCTION = 4,\nEC_QUEUE_WAVE_MEMORY_VIOLATION = 5,\nEC_QUEUE_WAVE_APERTURE_VIOLATION = 6,\nEC_QUEUE_PACKET_DISPATCH_DIM_INVALID = 16,\nEC_QUEUE_PACKET_DISPATCH_GROUP_SEGMENT_SIZE_INVALID = 17,\nEC_QUEUE_PACKET_DISPATCH_CODE_INVALID = 18,\nEC_QUEUE_PACKET_RESERVED = 19,\nEC_QUEUE_PACKET_UNSUPPORTED = 20,\nEC_QUEUE_PACKET_DISPATCH_WORK_GROUP_SIZE_INVALID = 21,\nEC_QUEUE_PACKET_DISPATCH_REGISTER_INVALID = 22,\nEC_QUEUE_PACKET_VENDOR_UNSUPPORTED = 23,\nEC_QUEUE_PREEMPTION_ERROR = 30,\nEC_QUEUE_NEW = 31,\n/* per device */\nEC_DEVICE_QUEUE_DELETE = 32,\nEC_DEVICE_MEMORY_VIOLATION = 33,\nEC_DEVICE_RAS_ERROR = 34,\nEC_DEVICE_FATAL_HALT = 35,\nEC_DEVICE_NEW = 36,\n/* per process */\nEC_PROCESS_RUNTIME = 48,\nEC_PROCESS_DEVICE_REMOVE = 49,\nEC_MAX\n};\n```\n```c\nint init_queue(struct queue **q, const struct queue_properties *properties)\n{\nstruct queue *tmp_q;\ntmp_q = kzalloc(sizeof(*tmp_q), GFP_KERNEL);\nif (!tmp_q)\nreturn -ENOMEM;\nmemcpy(&tmp_q->properties, properties, sizeof(*properties));\n*q = tmp_q;\nreturn 0;\n}\n```\n```c\nint amdgpu_amdkfd_alloc_gtt_mem(struct amdgpu_device *adev, size_t size, void **mem_obj, uint64_t *gpu_addr, void **cpu_ptr, bool cp_mqd_gfx9)\n{\nstruct amdgpu_bo *bo = NULL;\nstruct amdgpu_bo_param bp;\nint r;\nvoid *cpu_ptr_tmp = NULL;\nmemset(&bp, 0, sizeof(bp));\nbp.size = size;\nbp.byte_align = PAGE_SIZE;\nbp.domain = AMDGPU_GEM_DOMAIN_GTT;\nbp.flags = AMDGPU_GEM_CREATE_CPU_GTT_USWC;\nbp.type = ttm_bo_type_kernel;\nbp.resv = NULL;\nbp.bo_ptr_size = sizeof(struct amdgpu_bo);\nif (cp_mqd_gfx9)\nbp.flags |= AMDGPU_GEM_CREATE_CP_MQD_GFX9;\nr = amdgpu_bo_create(adev, &bp, &bo);\nif (r) {\ndev_err(adev->dev,\n\"failed to allocate BO for amdkfd (%d)\\n\", r);\nreturn r;\n}\n/* map the buffer */\nr = amdgpu_bo_reserve(bo, true);\nif (r) {\ndev_err(adev->dev, \"(%d) failed to reserve bo for amdkfd\\n\", r);\ngoto allocate_mem_reserve_bo_failed;\n}\nr = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT);\nif (r) {\ndev_err(adev->dev, \"(%d) failed to pin bo for amdkfd\\n\", r);\ngoto allocate_mem_pin_bo_failed;\n}\nr = amdgpu_ttm_alloc_gart(&bo->tbo);\nif (r) {\ndev_err(adev->dev, \"%p bind failed\\n\", bo);\ngoto allocate_mem_kmap_bo_failed;\n}\nr = amdgpu_bo_kmap(bo, &cpu_ptr_tmp);\nif (r) {\ndev_err(adev->dev,\n\"(%d) failed to map bo to kernel for amdkfd\\n\", r);\ngoto allocate_mem_kmap_bo_failed;\n}\n*mem_obj = bo;\n*gpu_addr = amdgpu_bo_gpu_offset(bo);\n*cpu_ptr = cpu_ptr_tmp;\namdgpu_bo_unreserve(bo);\nreturn 0;\nallocate_mem_kmap_bo_failed:\namdgpu_bo_unpin(bo);\nallocate_mem_pin_bo_failed:\namdgpu_bo_unreserve(bo);\nallocate_mem_reserve_bo_failed:\namdgpu_bo_unref(&bo);\nreturn r;\n}\n```\n```c\n#define AMDGPU_MES_GANG_CTX_SIZE 0x1000 /* one page area */\n```\n```c\n#define pr_err(format, ...) fprintf (stderr, format, ## __VA_ARGS__)\n```\n```c\n#ifndef NOLIBC_ARCH_HAS_MEMSET\n/* might be ignored by the compiler without -ffreestanding, then found as\n* missing.\n*/\n__attribute__((weak,unused,section(\".text.nolibc_memset\")))\nvoid *memset(void *dst, int b, size_t len)\n{\nchar *p = dst;\nwhile (len--) {\n/* prevent gcc from recognizing memset() here */\n__asm__ volatile(\"\");\n*(p++) = b;\n}\nreturn dst;\n}\n#endif /* #ifndef NOLIBC_ARCH_HAS_MEMSET */\n```\n```c\n#define AMDGPU_MES_API_VERSION_MASK 0x00fff000\n#define AMDGPU_MES_API_VERSION_SHIFT 12\n```\n```c\nstatic inline struct amdgpu_device *amdgpu_ttm_adev(struct ttm_device *bdev)\n{\nreturn container_of(bdev, struct amdgpu_device, mman.bdev);\n}\n```\n```c\n#define EINVAL 22 /* Invalid argument */\n```\n```c\nint amdgpu_amdkfd_map_gtt_bo_to_gart(struct amdgpu_bo *bo, struct amdgpu_bo **bo_gart)\n{\nint ret;\nret = amdgpu_bo_reserve(bo, true);\nif (ret) {\npr_err(\"Failed to reserve bo. ret %d\\n\", ret);\ngoto err_reserve_bo_failed;\n}\nret = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT);\nif (ret) {\npr_err(\"Failed to pin bo. ret %d\\n\", ret);\ngoto err_pin_bo_failed;\n}\nret = amdgpu_ttm_alloc_gart(&bo->tbo);\nif (ret) {\npr_err(\"Failed to bind bo to GART. ret %d\\n\", ret);\ngoto err_map_bo_gart_failed;\n}\namdgpu_amdkfd_remove_eviction_fence(\nbo, bo->vm_bo->vm->process_info->eviction_fence);\namdgpu_bo_unreserve(bo);\n*bo_gart = amdgpu_bo_ref(bo);\nreturn 0;\nerr_map_bo_gart_failed:\namdgpu_bo_unpin(bo);\nerr_pin_bo_failed:\namdgpu_bo_unreserve(bo);\nerr_reserve_bo_failed:\nreturn ret;\n}\n```\n```c\nvoid amdgpu_amdkfd_free_gtt_mem(struct amdgpu_device *adev, void **mem_obj)\n{\nstruct amdgpu_bo **bo = (struct amdgpu_bo **) mem_obj;\namdgpu_bo_reserve(*bo, true);\namdgpu_bo_kunmap(*bo);\namdgpu_bo_unpin(*bo);\namdgpu_bo_unreserve(*bo);\namdgpu_bo_unref(bo);\n}\n```\n```c\nvoid uninit_queue(struct queue *q)\n{\nkfree(q);\n}\n```",
  "original_code": "```c\nstatic int init_user_queue(struct process_queue_manager *pqm,\nstruct kfd_node *dev, struct queue **q,\nstruct queue_properties *q_properties,\nunsigned int qid)\n{\nint retval;\n/* Doorbell initialized in user space*/\nq_properties->doorbell_ptr = NULL;\nq_properties->exception_status = KFD_EC_MASK(EC_QUEUE_NEW);\n/* let DQM handle it*/\nq_properties->vmid = 0;\nq_properties->queue_id = qid;\nretval = init_queue(q, q_properties);\nif (retval != 0)\nreturn retval;\n(*q)->device = dev;\n(*q)->process = pqm->process;\nif (dev->kfd->shared_resources.enable_mes) {\nretval = amdgpu_amdkfd_alloc_gtt_mem(dev->adev,\nAMDGPU_MES_GANG_CTX_SIZE,\n&(*q)->gang_ctx_bo,\n&(*q)->gang_ctx_gpu_addr,\n&(*q)->gang_ctx_cpu_ptr,\nfalse);\nif (retval) {\npr_err(\"failed to allocate gang context bo\\n\");\ngoto cleanup;\n}\nmemset((*q)->gang_ctx_cpu_ptr, 0, AMDGPU_MES_GANG_CTX_SIZE);\n/* Starting with GFX11, wptr BOs must be mapped to GART for MES to determine work\n* on unmapped queues for usermode queue oversubscription (no aggregated doorbell)\n*/\nif (((dev->adev->mes.sched_version & AMDGPU_MES_API_VERSION_MASK)\n>> AMDGPU_MES_API_VERSION_SHIFT) >= 2) {\nif (dev->adev != amdgpu_ttm_adev(q_properties->wptr_bo->tbo.bdev)) {\npr_err(\"Queue memory allocated to wrong device\\n\");\nretval = -EINVAL;\ngoto free_gang_ctx_bo;\n}\nretval = amdgpu_amdkfd_map_gtt_bo_to_gart(q_properties->wptr_bo,\n&(*q)->wptr_bo_gart);\nif (retval) {\npr_err(\"Failed to map wptr bo to GART\\n\");\ngoto free_gang_ctx_bo;\n}\n}\n}\npr_debug(\"PQM After init queue\");\nreturn 0;\nfree_gang_ctx_bo:\namdgpu_amdkfd_free_gtt_mem(dev->adev, (*q)->gang_ctx_bo);\ncleanup:\nuninit_queue(*q);\n*q = NULL;\nreturn retval;\n}\n```",
  "vuln_patch": "```c\nstatic int init_user_queue(struct process_queue_manager *pqm,\nstruct kfd_node *dev, struct queue **q,\nstruct queue_properties *q_properties,\nunsigned int qid)\n{\nint retval;\n/* Doorbell initialized in user space*/\nq_properties->doorbell_ptr = NULL;\nq_properties->exception_status = KFD_EC_MASK(EC_QUEUE_NEW);\n/* let DQM handle it*/\nq_properties->vmid = 0;\nq_properties->queue_id = qid;\nretval = init_queue(q, q_properties);\nif (retval != 0)\nreturn retval;\n(*q)->device = dev;\n(*q)->process = pqm->process;\nif (dev->kfd->shared_resources.enable_mes) {\nretval = amdgpu_amdkfd_alloc_gtt_mem(dev->adev,\nAMDGPU_MES_GANG_CTX_SIZE,\n&(*q)->gang_ctx_bo,\n&(*q)->gang_ctx_gpu_addr,\n&(*q)->gang_ctx_cpu_ptr,\nfalse);\nif (retval) {\npr_err(\"failed to allocate gang context bo\\n\");\ngoto cleanup;\n}\nmemset((*q)->gang_ctx_cpu_ptr, 0, AMDGPU_MES_GANG_CTX_SIZE);\n/* Starting with GFX11, wptr BOs must be mapped to GART for MES to determine work\n* on unmapped queues for usermode queue oversubscription (no aggregated doorbell)\n*/\nif (((dev->adev->mes.sched_version & AMDGPU_MES_API_VERSION_MASK)\n>> AMDGPU_MES_API_VERSION_SHIFT) >= 2) {\nif (dev->adev != amdgpu_ttm_adev(q_properties->wptr_bo->tbo.bdev)) {\npr_err(\"Queue memory allocated to wrong device\\n\");\nretval = -EINVAL;\ngoto free_gang_ctx_bo;\n}\nretval = amdgpu_amdkfd_map_gtt_bo_to_gart(q_properties->wptr_bo,\n&(*q)->wptr_bo_gart);\nif (retval) {\npr_err(\"Failed to map wptr bo to GART\\n\");\ngoto free_gang_ctx_bo;\n}\n}\n}\npr_debug(\"PQM After init queue\");\nreturn 0;\nfree_gang_ctx_bo:\namdgpu_amdkfd_free_gtt_mem(dev->adev, &(*q)->gang_ctx_bo);\ncleanup:\nuninit_queue(*q);\n*q = NULL;\nreturn retval;\n}\n```",
  "function_name": "init_user_queue",
  "function_prototype": "static int init_user_queue(struct process_queue_manager *pqm, struct kfd_node *dev, struct queue **q, struct queue_properties *q_properties, unsigned int qid)",
  "code_semantics": "The function initializes a user queue by setting initial properties, allocating and clearing memory for a context buffer if a feature is enabled, and mapping a buffer object to a memory table if certain conditions are met. It handles errors by cleaning up resources.",
  "safe_verification_cot": "1. gang_ctx_bo is correctly passed as a pointer to a pointer, satisfying the checklist and resolving the vulnerability.\n2. The function amdgpu_amdkfd_free_gtt_mem can safely handle gang_ctx_bo with the correct argument type.",
  "verification_cot": "1. gang_ctx_bo is passed directly, not as a pointer to a pointer, violating the checklist and causing the vulnerability.\n2. The function amdgpu_amdkfd_free_gtt_mem cannot safely handle gang_ctx_bo due to incorrect argument type.",
  "vulnerability_related_variables": {
    "gang_ctx_bo": "This variable stores a reference to a memory buffer object allocated for managing gang context data in a queue management system. It is initialized to zero to ensure a clean state and is mapped to both GPU and CPU address spaces for hardware interaction. In case of errors, the memory is deallocated to prevent resource leaks."
  },
  "vulnerability_related_functions": {
    "amdgpu_amdkfd_free_gtt_mem": "This function takes a reference to a memory object and a device context. It reserves the memory object to ensure exclusive access, unmaps it from the kernel's address space, unpins it to make it eligible for relocation, releases the reservation, and decrements the reference count, potentially freeing the memory object if no other references exist."
  },
  "root_cause": "Incorrect type conversion or cast of gang_ctx_bo when calling amdgpu_amdkfd_free_gtt_mem.",
  "patch_cot": "To patch the vulnerability, first identify where gang_ctx_bo is being passed to amdgpu_amdkfd_free_gtt_mem. Ensure that it is passed as &gang_ctx_bo instead of gang_ctx_bo. This corrects the type conversion issue by matching the expected void ** type. After making the change, test the code to confirm the issue is resolved."
}