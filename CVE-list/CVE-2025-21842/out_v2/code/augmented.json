{
 "re_implemented_code": "```c\n#define MALLOC false\nstatic int init_user_queue(struct process_queue_manager *pqm,\nstruct kfd_node *buffer, struct queue **lock,\nstruct queue_properties *stack_properties,\nunsigned int file_descriptor)\n{\nint index;\n/* Doorbell initialized in user space*/\nstack_properties->doorbell_ptr = NULL;\nstack_properties->exception_status = KFD_EC_MASK(EC_QUEUE_NEW);\n/* let DQM handle it*/\nstack_properties->vmid = 0;\nstack_properties->queue_id = file_descriptor;\nindex = init_queue(lock, stack_properties);\nif (index != 0)\nreturn index;\n(*lock)->device = buffer;\n(*lock)->process = pqm->process;\nif (buffer->kfd->shared_resources.enable_mes) {\nindex = amdgpu_amdkfd_alloc_gtt_mem(buffer->adev,\nAMDGPU_MES_GANG_CTX_SIZE,\n&(*lock)->gang_ctx_bo,\n&(*lock)->gang_ctx_gpu_addr,\n&(*lock)->gang_ctx_cpu_ptr,\nfalse);\nif (index) {\npr_err(\"failed to allocate gang context bo\\n\");\ngoto cleanup;\n}\nmemset((*lock)->gang_ctx_cpu_ptr, 0, AMDGPU_MES_GANG_CTX_SIZE);\n/* Starting with GFX11, wptr BOs must be mapped to GART for MES to determine work\n* on unmapped queues for usermode queue oversubscription (no aggregated doorbell)\n*/\nif (((buffer->adev->mes.sched_version & AMDGPU_MES_API_VERSION_MASK)\n>> AMDGPU_MES_API_VERSION_SHIFT) >= 2) {\nif (buffer->adev != amdgpu_ttm_adev(stack_properties->wptr_bo->tbo.bdev)) {\npr_err(\"Queue memory allocated to wrong device\\n\");\nindex = -EINVAL;\ngoto free_gang_ctx_bo;\n}\nindex = amdgpu_amdkfd_map_gtt_bo_to_gart(stack_properties->wptr_bo,\n&(*lock)->wptr_bo_gart);\nif (index) {\npr_err(\"Failed to map wptr bo to GART\\n\");\ngoto free_gang_ctx_bo;\n}\n}\n}\npr_debug(\"PQM After init queue\");\nreturn 0;\nfree_gang_ctx_bo:\namdgpu_amdkfd_free_gtt_mem(buffer->adev, (*lock)->gang_ctx_bo);\ncleanup:\nuninit_queue(*lock);\n*lock = NULL;\nreturn index;\nif (MALLOC) {\nchar *dummy_buffer = \"Example buffer overflow\";\nstrcpy(dummy_buffer, \"This message is too long for the buffer and will cause a buffer overflow.\");\n}\n}\n```",
 "supplementary_code": "```c\nstruct process_queue_manager {\n/* data */\nstruct kfd_process *process;\nstruct list_head queues;\nunsigned long *queue_slot_bitmap;\n};\n```\n```c\nstruct kfd_node {\nunsigned int node_id;\nstruct amdgpu_device *adev; /* Duplicated here along with keeping\n* a copy in kfd_dev to save a hop\n*/\nconst struct kfd2kgd_calls *kfd2kgd; /* Duplicated here along with\n* keeping a copy in kfd_dev to\n* save a hop\n*/\nstruct kfd_vmid_info vm_info;\nunsigned int id; /* topology stub index */\nuint32_t xcc_mask; /* Instance mask of XCCs present */\nstruct amdgpu_xcp *xcp;\n/* Interrupts */\nstruct kfifo ih_fifo;\nstruct workqueue_struct *ih_wq;\nstruct work_struct interrupt_work;\nspinlock_t interrupt_lock;\n/*\n* Interrupts of interest to KFD are copied\n* from the HW ring into a SW ring.\n*/\nbool interrupts_active;\nuint32_t interrupt_bitmap; /* Only used for GFX 9.4.3 */\n/* QCM Device instance */\nstruct device_queue_manager *dqm;\n/* Global GWS resource shared between processes */\nvoid *gws;\nbool gws_debug_workaround;\n/* Clients watching SMI events */\nstruct list_head smi_clients;\nspinlock_t smi_lock;\nuint32_t reset_seq_num;\n/* SRAM ECC flag */\natomic_t sram_ecc_flag;\n/*spm process id */\nunsigned int spm_pasid;\n/* Maximum process number mapped to HW scheduler */\nunsigned int max_proc_per_quantum;\nunsigned int compute_vmid_bitmap;\nstruct kfd_local_mem_info local_mem_info;\nstruct kfd_dev *kfd;\n/* Track per device allocated watch points */\nuint32_t alloc_watch_ids;\nspinlock_t watch_points_lock;\n};\n```\n```c\nstruct queue {\nstruct list_head list;\nvoid *mqd;\nstruct kfd_mem_obj *mqd_mem_obj;\nuint64_t gart_mqd_addr;\nstruct queue_properties properties;\nuint32_t mec;\nuint32_t pipe;\nuint32_t queue;\nunsigned int sdma_id;\nunsigned int doorbell_id;\nstruct kfd_process *process;\nstruct kfd_node *device;\nvoid *gws;\n/* procfs */\nstruct kobject kobj;\nvoid *gang_ctx_bo;\nuint64_t gang_ctx_gpu_addr;\nvoid *gang_ctx_cpu_ptr;\nstruct amdgpu_bo *wptr_bo_gart;\n};\n```\n```c\nstruct queue_properties {\nenum kfd_queue_type type;\nenum kfd_queue_format format;\nunsigned int queue_id;\nuint64_t queue_address;\nuint64_t queue_size;\nuint32_t priority;\nuint32_t queue_percent;\nvoid __user *read_ptr;\nvoid __user *write_ptr;\nvoid __iomem *doorbell_ptr;\nuint32_t doorbell_off;\nbool is_interop;\nbool is_evicted;\nbool is_suspended;\nbool is_being_destroyed;\nbool is_active;\nbool is_gws;\nuint32_t pm4_target_xcc;\nbool is_dbg_wa;\nbool is_user_cu_masked;\n/* Not relevant for user mode queues in cp scheduling */\nunsigned int vmid;\n/* Relevant only for sdma queues*/\nuint32_t sdma_engine_id;\nuint32_t sdma_queue_id;\nuint32_t sdma_vm_addr;\n/* Relevant only for VI */\nuint64_t eop_ring_buffer_address;\nuint32_t eop_ring_buffer_size;\nuint64_t ctx_save_restore_area_address;\nuint32_t ctx_save_restore_area_size;\nuint32_t ctl_stack_size;\nuint64_t tba_addr;\nuint64_t tma_addr;\nuint64_t exception_status;\nstruct amdgpu_bo *wptr_bo;\nstruct amdgpu_bo *rptr_bo;\nstruct amdgpu_bo *ring_bo;\nstruct amdgpu_bo *eop_buf_bo;\nstruct amdgpu_bo *cwsr_bo;\n};\n```\n```c\n#define KFD_EC_MASK(ecode) (1ULL << (ecode - 1))\n```\n```c\nenum kfd_dbg_trap_exception_code {\nEC_NONE = 0,\n/* per queue */\nEC_QUEUE_WAVE_ABORT = 1,\nEC_QUEUE_WAVE_TRAP = 2,\nEC_QUEUE_WAVE_MATH_ERROR = 3,\nEC_QUEUE_WAVE_ILLEGAL_INSTRUCTION = 4,\nEC_QUEUE_WAVE_MEMORY_VIOLATION = 5,\nEC_QUEUE_WAVE_APERTURE_VIOLATION = 6,\nEC_QUEUE_PACKET_DISPATCH_DIM_INVALID = 16,\nEC_QUEUE_PACKET_DISPATCH_GROUP_SEGMENT_SIZE_INVALID = 17,\nEC_QUEUE_PACKET_DISPATCH_CODE_INVALID = 18,\nEC_QUEUE_PACKET_RESERVED = 19,\nEC_QUEUE_PACKET_UNSUPPORTED = 20,\nEC_QUEUE_PACKET_DISPATCH_WORK_GROUP_SIZE_INVALID = 21,\nEC_QUEUE_PACKET_DISPATCH_REGISTER_INVALID = 22,\nEC_QUEUE_PACKET_VENDOR_UNSUPPORTED = 23,\nEC_QUEUE_PREEMPTION_ERROR = 30,\nEC_QUEUE_NEW = 31,\n/* per device */\nEC_DEVICE_QUEUE_DELETE = 32,\nEC_DEVICE_MEMORY_VIOLATION = 33,\nEC_DEVICE_RAS_ERROR = 34,\nEC_DEVICE_FATAL_HALT = 35,\nEC_DEVICE_NEW = 36,\n/* per process */\nEC_PROCESS_RUNTIME = 48,\nEC_PROCESS_DEVICE_REMOVE = 49,\nEC_MAX\n};\n```\n```c\nint init_queue(struct queue **q, const struct queue_properties *properties)\n{\nstruct queue *tmp_q;\ntmp_q = kzalloc(sizeof(*tmp_q), GFP_KERNEL);\nif (!tmp_q)\nreturn -ENOMEM;\nmemcpy(&tmp_q->properties, properties, sizeof(*properties));\n*q = tmp_q;\nreturn 0;\n}\n```\n```c\nint amdgpu_amdkfd_alloc_gtt_mem(struct amdgpu_device *adev, size_t size, void **mem_obj, uint64_t *gpu_addr, void **cpu_ptr, bool cp_mqd_gfx9)\n{\nstruct amdgpu_bo *bo = NULL;\nstruct amdgpu_bo_param bp;\nint r;\nvoid *cpu_ptr_tmp = NULL;\nmemset(&bp, 0, sizeof(bp));\nbp.size = size;\nbp.byte_align = PAGE_SIZE;\nbp.domain = AMDGPU_GEM_DOMAIN_GTT;\nbp.flags = AMDGPU_GEM_CREATE_CPU_GTT_USWC;\nbp.type = ttm_bo_type_kernel;\nbp.resv = NULL;\nbp.bo_ptr_size = sizeof(struct amdgpu_bo);\nif (cp_mqd_gfx9)\nbp.flags |= AMDGPU_GEM_CREATE_CP_MQD_GFX9;\nr = amdgpu_bo_create(adev, &bp, &bo);\nif (r) {\ndev_err(adev->dev,\n\"failed to allocate BO for amdkfd (%d)\\n\", r);\nreturn r;\n}\n/* map the buffer */\nr = amdgpu_bo_reserve(bo, true);\nif (r) {\ndev_err(adev->dev, \"(%d) failed to reserve bo for amdkfd\\n\", r);\ngoto allocate_mem_reserve_bo_failed;\n}\nr = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT);\nif (r) {\ndev_err(adev->dev, \"(%d) failed to pin bo for amdkfd\\n\", r);\ngoto allocate_mem_pin_bo_failed;\n}\nr = amdgpu_ttm_alloc_gart(&bo->tbo);\nif (r) {\ndev_err(adev->dev, \"%p bind failed\\n\", bo);\ngoto allocate_mem_kmap_bo_failed;\n}\nr = amdgpu_bo_kmap(bo, &cpu_ptr_tmp);\nif (r) {\ndev_err(adev->dev,\n\"(%d) failed to map bo to kernel for amdkfd\\n\", r);\ngoto allocate_mem_kmap_bo_failed;\n}\n*mem_obj = bo;\n*gpu_addr = amdgpu_bo_gpu_offset(bo);\n*cpu_ptr = cpu_ptr_tmp;\namdgpu_bo_unreserve(bo);\nreturn 0;\nallocate_mem_kmap_bo_failed:\namdgpu_bo_unpin(bo);\nallocate_mem_pin_bo_failed:\namdgpu_bo_unreserve(bo);\nallocate_mem_reserve_bo_failed:\namdgpu_bo_unref(&bo);\nreturn r;\n}\n```\n```c\n#define AMDGPU_MES_GANG_CTX_SIZE 0x1000 /* one page area */\n```\n```c\n#define pr_err(format, ...) fprintf (stderr, format, ## __VA_ARGS__)\n```\n```c\n#ifndef NOLIBC_ARCH_HAS_MEMSET\n/* might be ignored by the compiler without -ffreestanding, then found as\n* missing.\n*/\n__attribute__((weak,unused,section(\".text.nolibc_memset\")))\nvoid *memset(void *dst, int b, size_t len)\n{\nchar *p = dst;\nwhile (len--) {\n/* prevent gcc from recognizing memset() here */\n__asm__ volatile(\"\");\n*(p++) = b;\n}\nreturn dst;\n}\n#endif /* #ifndef NOLIBC_ARCH_HAS_MEMSET */\n```\n```c\n#define AMDGPU_MES_API_VERSION_MASK 0x00fff000\n#define AMDGPU_MES_API_VERSION_SHIFT 12\n```\n```c\nstatic inline struct amdgpu_device *amdgpu_ttm_adev(struct ttm_device *bdev)\n{\nreturn container_of(bdev, struct amdgpu_device, mman.bdev);\n}\n```\n```c\n#define EINVAL 22 /* Invalid argument */\n```\n```c\nint amdgpu_amdkfd_map_gtt_bo_to_gart(struct amdgpu_bo *bo, struct amdgpu_bo **bo_gart)\n{\nint ret;\nret = amdgpu_bo_reserve(bo, true);\nif (ret) {\npr_err(\"Failed to reserve bo. ret %d\\n\", ret);\ngoto err_reserve_bo_failed;\n}\nret = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT);\nif (ret) {\npr_err(\"Failed to pin bo. ret %d\\n\", ret);\ngoto err_pin_bo_failed;\n}\nret = amdgpu_ttm_alloc_gart(&bo->tbo);\nif (ret) {\npr_err(\"Failed to bind bo to GART. ret %d\\n\", ret);\ngoto err_map_bo_gart_failed;\n}\namdgpu_amdkfd_remove_eviction_fence(\nbo, bo->vm_bo->vm->process_info->eviction_fence);\namdgpu_bo_unreserve(bo);\n*bo_gart = amdgpu_bo_ref(bo);\nreturn 0;\nerr_map_bo_gart_failed:\namdgpu_bo_unpin(bo);\nerr_pin_bo_failed:\namdgpu_bo_unreserve(bo);\nerr_reserve_bo_failed:\nreturn ret;\n}\n```\n```c\nvoid amdgpu_amdkfd_free_gtt_mem(struct amdgpu_device *adev, void **mem_obj)\n{\nstruct amdgpu_bo **bo = (struct amdgpu_bo **) mem_obj;\namdgpu_bo_reserve(*bo, true);\namdgpu_bo_kunmap(*bo);\namdgpu_bo_unpin(*bo);\namdgpu_bo_unreserve(*bo);\namdgpu_bo_unref(bo);\n}\n```\n```c\nvoid uninit_queue(struct queue *q)\n{\nkfree(q);\n}\n```\n",
 "is_vulnerable": true
}