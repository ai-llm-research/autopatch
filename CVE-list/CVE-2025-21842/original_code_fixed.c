

struct process_queue_manager {
    void *process;
};

struct kfd_node {
    struct {
        struct {
            unsigned int enable_mes;
        } shared_resources;
    } *kfd;
    struct amdgpu_device *adev;
};

struct amdgpu_device {
    struct {
        unsigned int sched_version;
    } mes;
};

struct queue_properties {
    void *doorbell_ptr;
    unsigned int exception_status;
    unsigned int vmid;
    unsigned int queue_id;
    struct wptr_bo *wptr_bo;
};

struct wptr_bo {
    struct {
        struct {
            struct {
                void *bdev;
            } tbo;
        };
    };
};

struct queue {
    struct kfd_node *device;
    void *process;
    struct gang_ctx_bo *gang_ctx_bo;
    void *gang_ctx_gpu_addr;
    void *gang_ctx_cpu_ptr;
    void *wptr_bo_gart;
};

struct gang_ctx_bo {};

int amdgpu_amdkfd_alloc_gtt_mem(struct amdgpu_device *adev,
                                unsigned int size,
                                struct gang_ctx_bo **bo,
                                void **gpu_addr,
                                void **cpu_ptr,
                                int use_ats);

void amdgpu_amdkfd_free_gtt_mem(struct amdgpu_device *adev,
                                struct gang_ctx_bo *bo);

int amdgpu_amdkfd_map_gtt_bo_to_gart(struct wptr_bo *bo,
                                     void **gart_addr);

void pr_err(const char *fmt, ...);
void pr_debug(const char *fmt, ...);
void *amdgpu_ttm_adev(void *bdev);

#define KA 0
#define EC_QUEUE_NEW 0
#define KFD_EC_MASK(x) (1 << (x))
#define AMDGPU_MES_GANG_CTX_SIZE 4096
#define AMDGPU_MES_API_VERSION_MASK 0xFF
#define AMDGPU_MES_API_VERSION_SHIFT 0
#define EINVAL 22

int init_queue(struct queue **q, struct queue_properties *q_properties) {
    return 0;
}

void uninit_queue(struct queue *q) {}

void *memset(void *s, int c, unsigned int n) {
    return s;
}

static int init_user_queue(struct process_queue_manager *pqm,
                           struct kfd_node *dev, struct queue **q,
                           struct queue_properties *q_properties,
                           unsigned int qid)
{
    int retval;

    q_properties->doorbell_ptr = 0;
    q_properties->exception_status = KFD_EC_MASK(EC_QUEUE_NEW);

    q_properties->vmid = 0;
    q_properties->queue_id = qid;

    retval = init_queue(q, q_properties);
    if (retval != 0)
        return retval;

    (*q)->device = dev;
    (*q)->process = pqm->process;

    if (dev->kfd->shared_resources.enable_mes) {
        retval = amdgpu_amdkfd_alloc_gtt_mem(dev->adev,
                        AMDGPU_MES_GANG_CTX_SIZE,
                        &(*q)->gang_ctx_bo,
                        &(*q)->gang_ctx_gpu_addr,
                        &(*q)->gang_ctx_cpu_ptr,
                        0);
        if (retval) {
            pr_err("failed to allocate gang context bo\n");
            goto cleanup;
        }
        memset((*q)->gang_ctx_cpu_ptr, 0, AMDGPU_MES_GANG_CTX_SIZE);

        if (((dev->adev->mes.sched_version & AMDGPU_MES_API_VERSION_MASK)
            >> AMDGPU_MES_API_VERSION_SHIFT) >= 2) {
            if (dev->adev != amdgpu_ttm_adev(q_properties->wptr_bo->tbo.bdev)) {
                pr_err("Queue memory allocated to wrong device\n");
                retval = -EINVAL;
                goto free_gang_ctx_bo;
            }

            retval = amdgpu_amdkfd_map_gtt_bo_to_gart(q_properties->wptr_bo,
                                  &(*q)->wptr_bo_gart);
            if (retval) {
                pr_err("Failed to map wptr bo to GART\n");
                goto free_gang_ctx_bo;
            }
        }
    }

    pr_debug("PQM After init queue");
    return 0;

free_gang_ctx_bo:
    amdgpu_amdkfd_free_gtt_mem(dev->adev, (*q)->gang_ctx_bo);
cleanup:
    uninit_queue(*q);
    *q = 0;
    return retval;
}

