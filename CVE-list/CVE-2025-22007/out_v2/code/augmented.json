{
 "re_implemented_code": "```c\n#define MALLOC false\nstatic struct sk_buff *chan_alloc_skb_cb(struct l2cap_chan *chan,\nunsigned long buffer_size,\nunsigned long buffer_length, int buffer_offset)\n{\n/* Note that we must allocate using GFP_ATOMIC here as\n* this function is called originally from netdev hard xmit\n* function in atomic context.\n*/\nreturn bt_skb_alloc(buffer_size + buffer_length, GFP_ATOMIC);\nif (MALLOC) {\nchar *buffer = NULL;\n*buffer = 'A'; // Attempt to write to NULL pointer\nbuffer_offset = buffer_length + 1;\n}\n}\n```",
 "supplementary_code": "```c\nstruct l2cap_chan {\nstruct l2cap_conn *conn;\nstruct kref kref;\natomic_t nesting;\n__u8 state;\nbdaddr_t dst;\n__u8 dst_type;\nbdaddr_t src;\n__u8 src_type;\n__le16 psm;\n__le16 sport;\n__u16 dcid;\n__u16 scid;\n__u16 imtu;\n__u16 omtu;\n__u16 flush_to;\n__u8 mode;\n__u8 chan_type;\n__u8 chan_policy;\n__u8 sec_level;\n__u8 ident;\n__u8 conf_req[64];\n__u8 conf_len;\n__u8 num_conf_req;\n__u8 num_conf_rsp;\n__u8 fcs;\n__u16 tx_win;\n__u16 tx_win_max;\n__u16 ack_win;\n__u8 max_tx;\n__u16 retrans_timeout;\n__u16 monitor_timeout;\n__u16 mps;\n__u16 tx_credits;\n__u16 rx_credits;\n/* estimated available receive buffer space or -1 if unknown */\nssize_t rx_avail;\n__u8 tx_state;\n__u8 rx_state;\nunsigned long conf_state;\nunsigned long conn_state;\nunsigned long flags;\n__u16 next_tx_seq;\n__u16 expected_ack_seq;\n__u16 expected_tx_seq;\n__u16 buffer_seq;\n__u16 srej_save_reqseq;\n__u16 last_acked_seq;\n__u16 frames_sent;\n__u16 unacked_frames;\n__u8 retry_count;\n__u16 sdu_len;\nstruct sk_buff *sdu;\nstruct sk_buff *sdu_last_frag;\n__u16 remote_tx_win;\n__u8 remote_max_tx;\n__u16 remote_mps;\n__u8 local_id;\n__u8 local_stype;\n__u16 local_msdu;\n__u32 local_sdu_itime;\n__u32 local_acc_lat;\n__u32 local_flush_to;\n__u8 remote_id;\n__u8 remote_stype;\n__u16 remote_msdu;\n__u32 remote_sdu_itime;\n__u32 remote_acc_lat;\n__u32 remote_flush_to;\nstruct delayed_work chan_timer;\nstruct delayed_work retrans_timer;\nstruct delayed_work monitor_timer;\nstruct delayed_work ack_timer;\nstruct sk_buff *tx_send_head;\nstruct sk_buff_head tx_q;\nstruct sk_buff_head srej_q;\nstruct l2cap_seq_list srej_list;\nstruct l2cap_seq_list retrans_list;\nstruct list_head list;\nstruct list_head global_l;\nvoid *data;\nconst struct l2cap_ops *ops;\nstruct mutex lock;\n};\n```\n```c\nstatic inline struct sk_buff *bt_skb_alloc(unsigned int len, gfp_t how)\n{\nstruct sk_buff *skb;\nskb = alloc_skb(len + BT_SKB_RESERVE, how);\nif (skb)\nskb_reserve(skb, BT_SKB_RESERVE);\nreturn skb;\n}\n```\n```c\nstruct sk_buff {\nunion {\nstruct {\n/* These two members must be first to match sk_buff_head. */\nstruct sk_buff *next;\nstruct sk_buff *prev;\nunion {\nstruct net_device *dev;\n/* Some protocols might use this space to store information,\n* while device pointer would be NULL.\n* UDP receive path is one user.\n*/\nunsigned long dev_scratch;\n};\n};\nstruct rb_node rbnode; /* used in netem, ip4 defrag, and tcp stack */\nstruct list_head list;\nstruct llist_node ll_node;\n};\nstruct sock *sk;\nunion {\nktime_t tstamp;\nu64 skb_mstamp_ns; /* earliest departure time */\n};\n/*\n* This is the control buffer. It is free to use for every\n* layer. Please put your private variables there. If you\n* want to keep them across layers you have to do a skb_clone()\n* first. This is owned by whoever has the skb queued ATM.\n*/\nchar cb[48] __aligned(8);\nunion {\nstruct {\nunsigned long _skb_refdst;\nvoid (*destructor)(struct sk_buff *skb);\n};\nstruct list_head tcp_tsorted_anchor;\n#ifdef CONFIG_NET_SOCK_MSG\nunsigned long _sk_redir;\n#endif\n};\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\nunsigned long _nfct;\n#endif\nunsigned int len,\ndata_len;\n__u16 mac_len,\nhdr_len;\n/* Following fields are _not_ copied in __copy_skb_header()\n* Note that queue_mapping is here mostly to fill a hole.\n*/\n__u16 queue_mapping;\n/* if you move cloned around you also must adapt those constants */\n#ifdef __BIG_ENDIAN_BITFIELD\n#define CLONED_MASK (1 << 7)\n#else\n#define CLONED_MASK 1\n#endif\n#define CLONED_OFFSET offsetof(struct sk_buff, __cloned_offset)\n/* private: */\n__u8 __cloned_offset[0];\n/* public: */\n__u8 cloned:1,\nnohdr:1,\nfclone:2,\npeeked:1,\nhead_frag:1,\npfmemalloc:1,\npp_recycle:1; /* page_pool recycle indicator */\n#ifdef CONFIG_SKB_EXTENSIONS\n__u8 active_extensions;\n#endif\n/* Fields enclosed in headers group are copied\n* using a single memcpy() in __copy_skb_header()\n*/\nstruct_group(headers,\n/* private: */\n__u8 __pkt_type_offset[0];\n/* public: */\n__u8 pkt_type:3; /* see PKT_TYPE_MAX */\n__u8 ignore_df:1;\n__u8 dst_pending_confirm:1;\n__u8 ip_summed:2;\n__u8 ooo_okay:1;\n/* private: */\n__u8 __mono_tc_offset[0];\n/* public: */\n__u8 tstamp_type:2; /* See skb_tstamp_type */\n#ifdef CONFIG_NET_XGRESS\n__u8 tc_at_ingress:1; /* See TC_AT_INGRESS_MASK */\n__u8 tc_skip_classify:1;\n#endif\n__u8 remcsum_offload:1;\n__u8 csum_complete_sw:1;\n__u8 csum_level:2;\n__u8 inner_protocol_type:1;\n__u8 l4_hash:1;\n__u8 sw_hash:1;\n#ifdef CONFIG_WIRELESS\n__u8 wifi_acked_valid:1;\n__u8 wifi_acked:1;\n#endif\n__u8 no_fcs:1;\n/* Indicates the inner headers are valid in the skbuff. */\n__u8 encapsulation:1;\n__u8 encap_hdr_csum:1;\n__u8 csum_valid:1;\n#ifdef CONFIG_IPV6_NDISC_NODETYPE\n__u8 ndisc_nodetype:2;\n#endif\n#if IS_ENABLED(CONFIG_IP_VS)\n__u8 ipvs_property:1;\n#endif\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE) || IS_ENABLED(CONFIG_NF_TABLES)\n__u8 nf_trace:1;\n#endif\n#ifdef CONFIG_NET_SWITCHDEV\n__u8 offload_fwd_mark:1;\n__u8 offload_l3_fwd_mark:1;\n#endif\n__u8 redirected:1;\n#ifdef CONFIG_NET_REDIRECT\n__u8 from_ingress:1;\n#endif\n#ifdef CONFIG_NETFILTER_SKIP_EGRESS\n__u8 nf_skip_egress:1;\n#endif\n#ifdef CONFIG_SKB_DECRYPTED\n__u8 decrypted:1;\n#endif\n__u8 slow_gro:1;\n#if IS_ENABLED(CONFIG_IP_SCTP)\n__u8 csum_not_inet:1;\n#endif\n__u8 unreadable:1;\n#if defined(CONFIG_NET_SCHED) || defined(CONFIG_NET_XGRESS)\n__u16 tc_index; /* traffic control index */\n#endif\nu16 alloc_cpu;\nunion {\n__wsum csum;\nstruct {\n__u16 csum_start;\n__u16 csum_offset;\n};\n};\n__u32 priority;\nint skb_iif;\n__u32 hash;\nunion {\nu32 vlan_all;\nstruct {\n__be16 vlan_proto;\n__u16 vlan_tci;\n};\n};\n#if defined(CONFIG_NET_RX_BUSY_POLL) || defined(CONFIG_XPS)\nunion {\nunsigned int napi_id;\nunsigned int sender_cpu;\n};\n#endif\n#ifdef CONFIG_NETWORK_SECMARK\n__u32 secmark;\n#endif\nunion {\n__u32 mark;\n__u32 reserved_tailroom;\n};\nunion {\n__be16 inner_protocol;\n__u8 inner_ipproto;\n};\n__u16 inner_transport_header;\n__u16 inner_network_header;\n__u16 inner_mac_header;\n__be16 protocol;\n__u16 transport_header;\n__u16 network_header;\n__u16 mac_header;\n#ifdef CONFIG_KCOV\nu64 kcov_handle;\n#endif\n); /* end headers group */\n/* These elements must be at the end, see alloc_skb() for details. */\nsk_buff_data_t tail;\nsk_buff_data_t end;\nunsigned char *head,\n*data;\nunsigned int truesize;\nrefcount_t users;\n#ifdef CONFIG_SKB_EXTENSIONS\n/* only usable after checking ->active_extensions != 0 */\nstruct skb_ext *extensions;\n#endif\n};\n```\n",
 "is_vulnerable": true
}