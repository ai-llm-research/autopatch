{
  "cwe_type": "Improper Locking",
  "cve_id": "CVE-2025-21809",
  "supplementary_code": "```c\nstruct rxrpc_peer {\nstruct rcu_head rcu; /* This must be first */\nrefcount_t ref;\nunsigned long hash_key;\nstruct hlist_node hash_link;\nstruct rxrpc_local *local;\nstruct hlist_head error_targets; /* targets for net error distribution */\nstruct rb_root service_conns; /* Service connections */\nstruct list_head keepalive_link; /* Link in net->peer_keepalive[] */\ntime64_t last_tx_at; /* Last time packet sent here */\nseqlock_t service_conn_lock;\nspinlock_t lock; /* access lock */\nunsigned int if_mtu; /* interface MTU for this peer */\nunsigned int mtu; /* network MTU for this peer */\nunsigned int maxdata; /* data size (MTU - hdrsize) */\nunsigned short hdrsize; /* header size (IP + UDP + RxRPC) */\nint debug_id; /* debug ID for printks */\nstruct sockaddr_rxrpc srx; /* remote address */\n/* calculated RTT cache */\n#define RXRPC_RTT_CACHE_SIZE 32\nspinlock_t rtt_input_lock; /* RTT lock for input routine */\nktime_t rtt_last_req; /* Time of last RTT request */\nunsigned int rtt_count; /* Number of samples we've got */\nu32 srtt_us; /* smoothed round trip time << 3 in usecs */\nu32 mdev_us; /* medium deviation */\nu32 mdev_max_us; /* maximal mdev for the last rtt period */\nu32 rttvar_us; /* smoothed mdev_max */\nu32 rto_us; /* Retransmission timeout in usec */\nu8 backoff; /* Backoff timeout (as shift) */\nu8 cong_ssthresh; /* Congestion slow-start threshold */\n};\n```\n```c\nstruct rxrpc_net {\nstruct proc_dir_entry *proc_net; /* Subdir in /proc/net */\nu32 epoch; /* Local epoch for detecting local-end reset */\nstruct list_head calls; /* List of calls active in this namespace */\nspinlock_t call_lock; /* Lock for ->calls */\natomic_t nr_calls; /* Count of allocated calls */\natomic_t nr_conns;\nstruct list_head bundle_proc_list; /* List of bundles for proc */\nstruct list_head conn_proc_list; /* List of conns in this namespace for proc */\nstruct list_head service_conns; /* Service conns in this namespace */\nrwlock_t conn_lock; /* Lock for ->conn_proc_list, ->service_conns */\nstruct work_struct service_conn_reaper;\nstruct timer_list service_conn_reap_timer;\nbool live;\natomic_t nr_client_conns;\nstruct hlist_head local_endpoints;\nstruct mutex local_mutex; /* Lock for ->local_endpoints */\nDECLARE_HASHTABLE (peer_hash, 10);\nspinlock_t peer_hash_lock; /* Lock for ->peer_hash */\n#define RXRPC_KEEPALIVE_TIME 20 /* NAT keepalive time in seconds */\nu8 peer_keepalive_cursor;\ntime64_t peer_keepalive_base;\nstruct list_head peer_keepalive[32];\nstruct list_head peer_keepalive_new;\nstruct timer_list peer_keepalive_timer;\nstruct work_struct peer_keepalive_work;\natomic_t stat_tx_data;\natomic_t stat_tx_data_retrans;\natomic_t stat_tx_data_send;\natomic_t stat_tx_data_send_frag;\natomic_t stat_tx_data_send_fail;\natomic_t stat_tx_data_underflow;\natomic_t stat_tx_data_cwnd_reset;\natomic_t stat_rx_data;\natomic_t stat_rx_data_reqack;\natomic_t stat_rx_data_jumbo;\natomic_t stat_tx_ack_fill;\natomic_t stat_tx_ack_send;\natomic_t stat_tx_ack_skip;\natomic_t stat_tx_acks[256];\natomic_t stat_rx_acks[256];\natomic_t stat_why_req_ack[8];\natomic_t stat_io_loop;\n};\n```\n```c\n#define ASSERT(X) \\\ndo { \\\nif (unlikely(!(X))) { \\\npr_err(\"Assertion failed\\n\"); \\\nBUG(); \\\n} \\\n} while (0)\n```\n```c\nstatic inline int hlist_empty(const struct hlist_head *h)\n{\nreturn !READ_ONCE(h->first);\n}\n```\n```c\nstatic inline void spin_lock(spinlock_t *lock)\n{\nint ret = pthread_spin_lock(lock);\nassert(!ret);\n}\n```\n```c\nstatic inline void hash_del_rcu(struct hlist_node *node)\n{\nhlist_del_init_rcu(node);\n}\n```\n```c\nstatic inline void list_del_init(struct list_head *entry)\n{\n__list_del_entry(entry);\nINIT_LIST_HEAD(entry);\n}\n```\n```c\nstatic inline void spin_unlock(spinlock_t *lock)\n{\nint ret = pthread_spin_unlock(lock);\nassert(!ret);\n}\n```\n```c\nstatic void rxrpc_free_peer(struct rxrpc_peer *peer)\n{\ntrace_rxrpc_peer(peer->debug_id, 0, rxrpc_peer_free);\nrxrpc_put_local(peer->local, rxrpc_local_put_peer);\nkfree_rcu(peer, rcu);\n}\n```",
  "original_code": "```c\nstatic void __rxrpc_put_peer(struct rxrpc_peer *peer)\n{\nstruct rxrpc_net *rxnet = peer->local->rxnet;\nASSERT(hlist_empty(&peer->error_targets));\nspin_lock(&rxnet->peer_hash_lock);\nhash_del_rcu(&peer->hash_link);\nlist_del_init(&peer->keepalive_link);\nspin_unlock(&rxnet->peer_hash_lock);\nrxrpc_free_peer(peer);\n}\n```",
  "vuln_patch": "```c\nstatic void __rxrpc_put_peer(struct rxrpc_peer *peer)\n{\nstruct rxrpc_net *rxnet = peer->local->rxnet;\nASSERT(hlist_empty(&peer->error_targets));\nspin_lock_bh(&rxnet->peer_hash_lock);\nhash_del_rcu(&peer->hash_link);\nlist_del_init(&peer->keepalive_link);\nspin_unlock_bh(&rxnet->peer_hash_lock);\nrxrpc_free_peer(peer);\n}\n```",
  "function_name": "__rxrpc_put_peer",
  "function_prototype": "static void __rxrpc_put_peer(struct rxrpc_peer *peer)",
  "code_semantics": "The function checks that a specific list associated with a network peer is empty. It then acquires a lock to ensure exclusive access to shared data structures. While holding the lock, it removes the peer from a hash table and a linked list. After these operations, it releases the lock. Finally, it calls another function to free the resources associated with the peer.",
  "safe_verification_cot": "The function spin_lock_bh is used to lock rxnet->peer_hash_lock, which disables bottom halves, preventing any race conditions with interrupt handlers or bottom halves. The function spin_unlock_bh is used to unlock rxnet->peer_hash_lock, which re-enables bottom halves, ensuring that the system remains in a consistent state. The peer->hash_link and peer->keepalive_link are modified within the critical section, and with bottom halves disabled, these modifications are fully protected against concurrent access.",
  "verification_cot": "The function spin_lock is used to lock rxnet->peer_hash_lock, but it does not disable bottom halves. This means that if an interrupt or bottom half accesses the same data structure, it can lead to a race condition. The function spin_unlock is used to unlock rxnet->peer_hash_lock, but it does not re-enable bottom halves, which can leave the system in an inconsistent state if a bottom half was preempted. The peer->hash_link and peer->keepalive_link are modified within the critical section, but without disabling bottom halves, these modifications are not fully protected against concurrent access.",
  "vulnerability_related_variables": {
    "rxnet->peer_hash_lock": "This is a synchronization mechanism used to ensure exclusive access to a shared resource, preventing concurrent modifications and ensuring data consistency.",
    "peer->hash_link": "This is a data structure element used to manage membership in a collection, allowing for efficient insertion, deletion, and traversal operations.",
    "peer->keepalive_link": "This is a data structure element used to manage membership in a collection, allowing for efficient insertion, deletion, and traversal operations."
  },
  "vulnerability_related_functions": {
    "spin_lock": "Acquires a lock to ensure exclusive access to a shared resource, preventing concurrent access by other threads.",
    "spin_unlock": "Releases a previously acquired lock, allowing other threads to access the shared resource.",
    "spin_lock_bh": "Acquires a lock and disables bottom halves to prevent deadlocks in interrupt context.",
    "spin_unlock_bh": "Releases a lock and re-enables bottom halves, allowing interrupt handling to continue.",
    "hash_del_rcu": "Removes an element from a hash table safely for concurrent readers using Read-Copy-Update mechanisms.",
    "list_del_init": "Removes an element from a linked list and reinitializes the list node to prevent accidental reuse."
  },
  "root_cause": "Improper locking mechanism using spin_lock and spin_unlock without disabling bottom halves, leading to potential race conditions.",
  "patch_cot": "First, identify the critical section in the code where shared resources are accessed or modified. In this case, it is the section where peer->hash_link and peer->keepalive_link are modified. Replace spin_lock(&rxnet->peer_hash_lock) with spin_lock_bh(&rxnet->peer_hash_lock) to ensure that bottom halves are disabled when acquiring the lock. Replace spin_unlock(&rxnet->peer_hash_lock) with spin_unlock_bh(&rxnet->peer_hash_lock) to ensure that bottom halves are re-enabled when releasing the lock. This change ensures that the critical section is protected from race conditions caused by interrupt handlers accessing the same resources."
}