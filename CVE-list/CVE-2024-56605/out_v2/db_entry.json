{
  "cwe_type": "Use After Free",
  "cve_id": "CVE-2024-56605",
  "supplementary_code": "```c\n/**\n*\tstruct sock - network layer representation of sockets\n*\t@__sk_common: shared layout with inet_timewait_sock\n*\t@sk_shutdown: mask of %SEND_SHUTDOWN and/or %RCV_SHUTDOWN\n*\t@sk_userlocks: %SO_SNDBUF and %SO_RCVBUF settings\n*\t@sk_lock:\tsynchronizer\n*\t@sk_kern_sock: True if sock is using kernel lock classes\n*\t@sk_rcvbuf: size of receive buffer in bytes\n*\t@sk_wq: sock wait queue and async head\n*\t@sk_rx_dst: receive input route used by early demux\n*\t@sk_rx_dst_ifindex: ifindex for @sk_rx_dst\n*\t@sk_rx_dst_cookie: cookie for @sk_rx_dst\n*\t@sk_dst_cache: destination cache\n*\t@sk_dst_pending_confirm: need to confirm neighbour\n*\t@sk_policy: flow policy\n*\t@sk_receive_queue: incoming packets\n*\t@sk_wmem_alloc: transmit queue bytes committed\n*\t@sk_tsq_flags: TCP Small Queues flags\n*\t@sk_write_queue: Packet sending queue\n*\t@sk_omem_alloc: \"o\" is \"option\" or \"other\"\n*\t@sk_wmem_queued: persistent queue size\n*\t@sk_forward_alloc: space allocated forward\n*\t@sk_reserved_mem: space reserved and non-reclaimable for the socket\n*\t@sk_napi_id: id of the last napi context to receive data for sk\n*\t@sk_ll_usec: usecs to busypoll when there is no data\n*\t@sk_allocation: allocation mode\n*\t@sk_pacing_rate: Pacing rate (if supported by transport/packet scheduler)\n*\t@sk_pacing_status: Pacing status (requested, handled by sch_fq)\n*\t@sk_max_pacing_rate: Maximum pacing rate (%SO_MAX_PACING_RATE)\n*\t@sk_sndbuf: size of send buffer in bytes\n*\t@sk_no_check_tx: %SO_NO_CHECK setting, set checksum in TX packets\n*\t@sk_no_check_rx: allow zero checksum in RX packets\n*\t@sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)\n*\t@sk_gso_disabled: if set, NETIF_F_GSO_MASK is forbidden.\n*\t@sk_gso_type: GSO type (e.g. %SKB_GSO_TCPV4)\n*\t@sk_gso_max_size: Maximum GSO segment size to build\n*\t@sk_gso_max_segs: Maximum number of GSO segments\n*\t@sk_pacing_shift: scaling factor for TCP Small Queues\n*\t@sk_lingertime: %SO_LINGER l_linger setting\n*\t@sk_backlog: always used with the per-socket spinlock held\n*\t@sk_callback_lock: used with the callbacks in the end of this struct\n*\t@sk_error_queue: rarely used\n*\t@sk_prot_creator: sk_prot of original sock creator (see ipv6_setsockopt,\n*\tIPV6_ADDRFORM for instance)\n*\t@sk_err: last error\n*\t@sk_err_soft: errors that don't cause failure but are the cause of a\n*\tpersistent failure not just 'timed out'\n*\t@sk_drops: raw/udp drops counter\n*\t@sk_ack_backlog: current listen backlog\n*\t@sk_max_ack_backlog: listen backlog set in listen()\n*\t@sk_uid: user id of owner\n*\t@sk_prefer_busy_poll: prefer busypolling over softirq processing\n*\t@sk_busy_poll_budget: napi processing budget when busypolling\n*\t@sk_priority: %SO_PRIORITY setting\n*\t@sk_type: socket type (%SOCK_STREAM, etc)\n*\t@sk_protocol: which protocol this socket belongs in this network family\n*\t@sk_peer_lock: lock protecting @sk_peer_pid and @sk_peer_cred\n*\t@sk_peer_pid: &struct pid for this socket's peer\n*\t@sk_peer_cred: %SO_PEERCRED setting\n*\t@sk_rcvlowat: %SO_RCVLOWAT setting\n*\t@sk_rcvtimeo: %SO_RCVTIMEO setting\n*\t@sk_sndtimeo: %SO_SNDTIMEO setting\n*\t@sk_txhash: computed flow hash for use on transmit\n*\t@sk_txrehash: enable TX hash rethink\n*\t@sk_filter: socket filtering instructions\n*\t@sk_timer: sock cleanup timer\n*\t@sk_stamp: time stamp of last packet received\n*\t@sk_stamp_seq: lock for accessing sk_stamp on 32 bit architectures only\n*\t@sk_tsflags: SO_TIMESTAMPING flags\n*\t@sk_use_task_frag: allow sk_page_frag() to use current->task_frag.\n*\tSockets that can be used under memory reclaim should\n*\tset this to false.\n*\t@sk_bind_phc: SO_TIMESTAMPING bind PHC index of PTP virtual clock\n*\tfor timestamping\n*\t@sk_tskey: counter to disambiguate concurrent tstamp requests\n*\t@sk_zckey: counter to order MSG_ZEROCOPY notifications\n*\t@sk_socket: Identd and reporting IO signals\n*\t@sk_user_data: RPC layer private data. Write-protected by @sk_callback_lock.\n*\t@sk_frag: cached page frag\n*\t@sk_peek_off: current peek_offset value\n*\t@sk_send_head: front of stuff to transmit\n*\t@tcp_rtx_queue: TCP re-transmit queue [union with @sk_send_head]\n*\t@sk_security: used by security modules\n*\t@sk_mark: generic packet mark\n*\t@sk_cgrp_data: cgroup data for this cgroup\n*\t@sk_memcg: this socket's memory cgroup association\n*\t@sk_write_pending: a write to stream socket waits to start\n*\t@sk_disconnects: number of disconnect operations performed on this sock\n*\t@sk_state_change: callback to indicate change in the state of the sock\n*\t@sk_data_ready: callback to indicate there is data to be processed\n*\t@sk_write_space: callback to indicate there is bf sending space available\n*\t@sk_error_report: callback to indicate errors (e.g. %MSG_ERRQUEUE)\n*\t@sk_backlog_rcv: callback to process the backlog\n*\t@sk_validate_xmit_skb: ptr to an optional validate function\n*\t@sk_destruct: called at sock freeing time, i.e. when all refcnt == 0\n*\t@sk_reuseport_cb: reuseport group container\n*\t@sk_bpf_storage: ptr to cache and control for bpf_sk_storage\n*\t@sk_rcu: used during RCU grace period\n*\t@sk_clockid: clockid used by time-based scheduling (SO_TXTIME)\n*\t@sk_txtime_deadline_mode: set deadline mode for SO_TXTIME\n*\t@sk_txtime_report_errors: set report errors mode for SO_TXTIME\n*\t@sk_txtime_unused: unused txtime flags\n*\t@ns_tracker: tracker for netns reference\n*\t@sk_user_frags: xarray of pages the user is holding a reference on.\n*/\nstruct sock {\n/*\n* Now struct inet_timewait_sock also uses sock_common, so please just\n* don't add nothing before this first member (__sk_common) --acme\n*/\nstruct sock_common\t__sk_common;\n#define sk_node\t__sk_common.skc_node\n#define sk_nulls_node\t__sk_common.skc_nulls_node\n#define sk_refcnt\t__sk_common.skc_refcnt\n#define sk_tx_queue_mapping\t__sk_common.skc_tx_queue_mapping\n#ifdef CONFIG_SOCK_RX_QUEUE_MAPPING\n#define sk_rx_queue_mapping\t__sk_common.skc_rx_queue_mapping\n#endif\n#define sk_dontcopy_begin\t__sk_common.skc_dontcopy_begin\n#define sk_dontcopy_end\t__sk_common.skc_dontcopy_end\n#define sk_hash\t__sk_common.skc_hash\n#define sk_portpair\t__sk_common.skc_portpair\n#define sk_num\t__sk_common.skc_num\n#define sk_dport\t__sk_common.skc_dport\n#define sk_addrpair\t__sk_common.skc_addrpair\n#define sk_daddr\t__sk_common.skc_daddr\n#define sk_rcv_saddr\t__sk_common.skc_rcv_saddr\n#define sk_family\t__sk_common.skc_family\n#define sk_state\t__sk_common.skc_state\n#define sk_reuse\t__sk_common.skc_reuse\n#define sk_reuseport\t__sk_common.skc_reuseport\n#define sk_ipv6only\t__sk_common.skc_ipv6only\n#define sk_net_refcnt\t__sk_common.skc_net_refcnt\n#define sk_bound_dev_if\t__sk_common.skc_bound_dev_if\n#define sk_bind_node\t__sk_common.skc_bind_node\n#define sk_prot\t__sk_common.skc_prot\n#define sk_net\t__sk_common.skc_net\n#define sk_v6_daddr\t__sk_common.skc_v6_daddr\n#define sk_v6_rcv_saddr\t__sk_common.skc_v6_rcv_saddr\n#define sk_cookie\t__sk_common.skc_cookie\n#define sk_incoming_cpu\t__sk_common.skc_incoming_cpu\n#define sk_flags\t__sk_common.skc_flags\n#define sk_rxhash\t__sk_common.skc_rxhash\n__cacheline_group_begin(sock_write_rx);\natomic_t\tsk_drops;\n__s32\tsk_peek_off;\nstruct sk_buff_head\tsk_error_queue;\nstruct sk_buff_head\tsk_receive_queue;\n/*\n* The backlog queue is special, it is always used with\n* the per-socket spinlock held and requires low latency\n* access. Therefore we special case it's implementation.\n* Note : rmem_alloc is in this structure to fill a hole\n* on 64bit arches, not because its logically part of\n* backlog.\n*/\nstruct {\natomic_t\trmem_alloc;\nint\tlen;\nstruct sk_buff\t*head;\nstruct sk_buff\t*tail;\n} sk_backlog;\n#define sk_rmem_alloc sk_backlog.rmem_alloc\n__cacheline_group_end(sock_write_rx);\n__cacheline_group_begin(sock_read_rx);\n/* early demux fields */\nstruct dst_entry __rcu\t*sk_rx_dst;\nint\tsk_rx_dst_ifindex;\nu32\tsk_rx_dst_cookie;\n#ifdef CONFIG_NET_RX_BUSY_POLL\nunsigned int\tsk_ll_usec;\nunsigned int\tsk_napi_id;\nu16\tsk_busy_poll_budget;\nu8\tsk_prefer_busy_poll;\n#endif\nu8\tsk_userlocks;\nint\tsk_rcvbuf;\nstruct sk_filter __rcu\t*sk_filter;\nunion {\nstruct socket_wq __rcu\t*sk_wq;\n/* private: */\nstruct socket_wq\t*sk_wq_raw;\n/* public: */\n};\nvoid\t(*sk_data_ready)(struct sock *sk);\nlong\tsk_rcvtimeo;\nint\tsk_rcvlowat;\n__cacheline_group_end(sock_read_rx);\n__cacheline_group_begin(sock_read_rxtx);\nint\tsk_err;\nstruct socket\t*sk_socket;\nstruct mem_cgroup\t*sk_memcg;\n#ifdef CONFIG_XFRM\nstruct xfrm_policy __rcu *sk_policy[2];\n#endif\n__cacheline_group_end(sock_read_rxtx);\n__cacheline_group_begin(sock_write_rxtx);\nsocket_lock_t\tsk_lock;\nu32\tsk_reserved_mem;\nint\tsk_forward_alloc;\nu32\tsk_tsflags;\n__cacheline_group_end(sock_write_rxtx);\n__cacheline_group_begin(sock_write_tx);\nint\tsk_write_pending;\natomic_t\tsk_omem_alloc;\nint\tsk_sndbuf;\nint\tsk_wmem_queued;\nrefcount_t\tsk_wmem_alloc;\nunsigned long\tsk_tsq_flags;\nunion {\nstruct sk_buff\t*sk_send_head;\nstruct rb_root\ttcp_rtx_queue;\n};\nstruct sk_buff_head\tsk_write_queue;\nu32\tsk_dst_pending_confirm;\nu32\tsk_pacing_status; /* see enum sk_pacing */\nstruct page_frag\tsk_frag;\nstruct timer_list\tsk_timer;\nunsigned long\tsk_pacing_rate; /* bytes per second */\natomic_t\tsk_zckey;\natomic_t\tsk_tskey;\n__cacheline_group_end(sock_write_tx);\n__cacheline_group_begin(sock_read_tx);\nunsigned long\tsk_max_pacing_rate;\nlong\tsk_sndtimeo;\nu32\tsk_priority;\nu32\tsk_mark;\nstruct dst_entry __rcu\t*sk_dst_cache;\nnetdev_features_t\tsk_route_caps;\n#ifdef CONFIG_SOCK_VALIDATE_XMIT\nstruct sk_buff*\t(*sk_validate_xmit_skb)(struct sock *sk,\nstruct net_device *dev,\nstruct sk_buff *skb);\n#endif\nu16\tsk_gso_type;\nu16\tsk_gso_max_segs;\nunsigned int\tsk_gso_max_size;\ngfp_t\tsk_allocation;\nu32\tsk_txhash;\nu8\tsk_pacing_shift;\nbool\tsk_use_task_frag;\n__cacheline_group_end(sock_read_tx);\n/*\n* Because of non atomicity rules, all\n* changes are protected by socket lock.\n*/\nu8\tsk_gso_disabled : 1,\nsk_kern_sock : 1,\nsk_no_check_tx : 1,\nsk_no_check_rx : 1;\nu8\tsk_shutdown;\nu16\tsk_type;\nu16\tsk_protocol;\nunsigned long\tsk_lingertime;\nstruct proto\t*sk_prot_creator;\nrwlock_t\tsk_callback_lock;\nint\tsk_err_soft;\nu32\tsk_ack_backlog;\nu32\tsk_max_ack_backlog;\nkuid_t\tsk_uid;\nspinlock_t\tsk_peer_lock;\nint\tsk_bind_phc;\nstruct pid\t*sk_peer_pid;\nconst struct cred\t*sk_peer_cred;\nktime_t\tsk_stamp;\n#if BITS_PER_LONG==32\nseqlock_t\tsk_stamp_seq;\n#endif\nint\tsk_disconnects;\nu8\tsk_txrehash;\nu8\tsk_clockid;\nu8\tsk_txtime_deadline_mode : 1,\nsk_txtime_report_errors : 1,\nsk_txtime_unused : 6;\nvoid\t*sk_user_data;\n#ifdef CONFIG_SECURITY\nvoid\t*sk_security;\n#endif\nstruct sock_cgroup_data\tsk_cgrp_data;\nvoid\t(*sk_state_change)(struct sock *sk);\nvoid\t(*sk_write_space)(struct sock *sk);\nvoid\t(*sk_error_report)(struct sock *sk);\nint\t(*sk_backlog_rcv)(struct sock *sk,\nstruct sk_buff *skb);\nvoid (*sk_destruct)(struct sock *sk);\nstruct sock_reuseport __rcu\t*sk_reuseport_cb;\n#ifdef CONFIG_BPF_SYSCALL\nstruct bpf_local_storage __rcu\t*sk_bpf_storage;\n#endif\nstruct rcu_head\tsk_rcu;\nnetns_tracker\tns_tracker;\nstruct xarray\tsk_user_frags;\n};\n```\n```c\nstruct net {\n/* First cache line can be often dirtied.\n* Do not place here read-mostly fields.\n*/\nrefcount_t\tpassive;\t/* To decide when the network\n* namespace should be freed.\n*/\nspinlock_t\trules_mod_lock;\nunsigned int\tdev_base_seq;\t/* protected by rtnl_mutex */\nu32\tifindex;\nspinlock_t\tnsid_lock;\natomic_t\tfnhe_genid;\nstruct list_head\tlist;\t/* list of network namespaces */\nstruct list_head\texit_list;\t/* To linked to call pernet exit\n* methods on dead net (\n* pernet_ops_rwsem read locked),\n* or to unregister pernet ops\n* (pernet_ops_rwsem write locked).\n*/\nstruct llist_node\tdefer_free_list;\nstruct llist_node\tcleanup_list;\t/* namespaces on death row */\n#ifdef CONFIG_KEYS\nstruct key_tag\t*key_domain;\t/* Key domain of operation tag */\n#endif\nstruct user_namespace *user_ns;\t/* Owning user namespace */\nstruct ucounts\t*ucounts;\nstruct idr\tnetns_ids;\nstruct ns_common\tns;\nstruct ref_tracker_dir refcnt_tracker;\nstruct ref_tracker_dir notrefcnt_tracker; /* tracker for objects not\n* refcounted against netns\n*/\nstruct list_head\tdev_base_head;\nstruct proc_dir_entry\t*proc_net;\nstruct proc_dir_entry\t*proc_net_stat;\n#ifdef CONFIG_SYSCTL\nstruct ctl_table_set\tsysctls;\n#endif\nstruct sock\t*rtnl;\t/* rtnetlink socket */\nstruct sock\t*genl_sock;\nstruct uevent_sock\t*uevent_sock;\t/* uevent socket */\nstruct hlist_head\t*dev_name_head;\nstruct hlist_head\t*dev_index_head;\nstruct xarray\tdev_by_index;\nstruct raw_notifier_head\tnetdev_chain;\n/* Note that @hash_mix can be read millions times per second,\n* it is critical that it is on a read_mostly cache line.\n*/\nu32\thash_mix;\nstruct net_device *loopback_dev; /* The loopback */\n/* core fib_rules */\nstruct list_head\trules_ops;\nstruct netns_core\tcore;\nstruct netns_mib\tmib;\nstruct netns_packet\tpacket;\n#if IS_ENABLED(CONFIG_UNIX)\nstruct netns_unix\tunx;\n#endif\nstruct netns_nexthop\tnexthop;\nstruct netns_ipv4\tipv4;\n#if IS_ENABLED(CONFIG_IPV6)\nstruct netns_ipv6\tipv6;\n#endif\n#if IS_ENABLED(CONFIG_IEEE802154_6LOWPAN)\nstruct netns_ieee802154_lowpan\tieee802154_lowpan;\n#endif\n#if defined(CONFIG_IP_SCTP) || defined(CONFIG_IP_SCTP_MODULE)\nstruct netns_sctp\tsctp;\n#endif\n#ifdef CONFIG_NETFILTER\nstruct netns_nf\tnf;\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\nstruct netns_ct\tct;\n#endif\n#if defined(CONFIG_NF_TABLES) || defined(CONFIG_NF_TABLES_MODULE)\nstruct netns_nftables\tnft;\n#endif\n#if IS_ENABLED(CONFIG_NF_FLOW_TABLE)\nstruct netns_ft ft;\n#endif\n#endif\n#ifdef CONFIG_WEXT_CORE\nstruct sk_buff_head\twext_nlevents;\n#endif\nstruct net_generic __rcu\t*gen;\n/* Used to store attached BPF programs */\nstruct netns_bpf\tbpf;\n/* Note : following structs are cache line aligned */\n#ifdef CONFIG_XFRM\nstruct netns_xfrm\txfrm;\n#endif\nu64\tnet_cookie; /* written once */\n#if IS_ENABLED(CONFIG_IP_VS)\nstruct netns_ipvs\t*ipvs;\n#endif\n#if IS_ENABLED(CONFIG_MPLS)\nstruct netns_mpls\tmpls;\n#endif\n#if IS_ENABLED(CONFIG_CAN)\nstruct netns_can\tcan;\n#endif\n#ifdef CONFIG_XDP_SOCKETS\nstruct netns_xdp\txdp;\n#endif\n#if IS_ENABLED(CONFIG_MCTP)\nstruct netns_mctp\tmctp;\n#endif\n#if IS_ENABLED(CONFIG_CRYPTO_USER)\nstruct sock\t*crypto_nlsk;\n#endif\nstruct sock\t*diag_nlsk;\n#if IS_ENABLED(CONFIG_SMC)\nstruct netns_smc\tsmc;\n#endif\n#ifdef CONFIG_DEBUG_NET_SMALL_RTNL\n/* Move to a better place when the config guard is removed. */\nstruct mutex\trtnl_mutex;\n#endif\n} __randomize_layout;\n```\n```c\ntypedef enum {\nGFP_KERNEL,\nGFP_ATOMIC,\n__GFP_HIGHMEM,\n__GFP_HIGH\n} gfp_t;\n```\n```c\n/**\n*\tsk_alloc - All socket objects are allocated here\n*\t@net: the applicable net namespace\n*\t@family: protocol family\n*\t@priority: for allocation (%GFP_KERNEL, %GFP_ATOMIC, etc)\n*\t@prot: struct proto associated with this new sock instance\n*\t@kern: is this to be a kernel socket?\n*/\nstruct sock *sk_alloc(struct net *net, int family, gfp_t priority,\nstruct proto *prot, int kern)\n{\nstruct sock *sk;\nsk = sk_prot_alloc(prot, priority | __GFP_ZERO, family);\nif (sk) {\nsk->sk_family = family;\n/*\n* See comment in struct sock definition to understand\n* why we need sk_prot_creator -acme\n*/\nsk->sk_prot = sk->sk_prot_creator = prot;\nsk->sk_kern_sock = kern;\nsock_lock_init(sk);\nsk->sk_net_refcnt = kern ? 0 : 1;\nif (likely(sk->sk_net_refcnt)) {\nget_net_track(net, &sk->ns_tracker, priority);\nsock_inuse_add(net, 1);\n} else {\nnet_passive_inc(net);\n__netns_tracker_alloc(net, &sk->ns_tracker,\nfalse, priority);\n}\nsock_net_set(sk, net);\nrefcount_set(&sk->sk_wmem_alloc, 1);\nmem_cgroup_sk_alloc(sk);\ncgroup_sk_alloc(&sk->sk_cgrp_data);\nsock_update_classid(&sk->sk_cgrp_data);\nsock_update_netprioidx(&sk->sk_cgrp_data);\nsk_tx_queue_clear(sk);\n}\nreturn sk;\n}\n```\n```c\n/* Protocol families, same as address families. */\n#define PF_UNSPEC\tAF_UNSPEC\n#define PF_UNIX\tAF_UNIX\n#define PF_LOCAL\tAF_LOCAL\n#define PF_INET\tAF_INET\n#define PF_AX25\tAF_AX25\n#define PF_IPX\tAF_IPX\n#define PF_APPLETALK\tAF_APPLETALK\n#define\tPF_NETROM\tAF_NETROM\n#define PF_BRIDGE\tAF_BRIDGE\n#define PF_ATMPVC\tAF_ATMPVC\n#define PF_X25\tAF_X25\n#define PF_INET6\tAF_INET6\n#define PF_ROSE\tAF_ROSE\n#define PF_DECnet\tAF_DECnet\n#define PF_NETBEUI\tAF_NETBEUI\n#define PF_SECURITY\tAF_SECURITY\n#define PF_KEY\tAF_KEY\n#define PF_NETLINK\tAF_NETLINK\n#define PF_ROUTE\tAF_ROUTE\n#define PF_PACKET\tAF_PACKET\n#define PF_ASH\tAF_ASH\n#define PF_ECONET\tAF_ECONET\n#define PF_ATMSVC\tAF_ATMSVC\n#define PF_RDS\tAF_RDS\n#define PF_SNA\tAF_SNA\n#define PF_IRDA\tAF_IRDA\n#define PF_PPPOX\tAF_PPPOX\n#define PF_WANPIPE\tAF_WANPIPE\n#define PF_LLC\tAF_LLC\n#define PF_IB\tAF_IB\n#define PF_MPLS\tAF_MPLS\n#define PF_CAN\tAF_CAN\n#define PF_TIPC\tAF_TIPC\n#define PF_BLUETOOTH\tAF_BLUETOOTH\n#define PF_IUCV\tAF_IUCV\n#define PF_RXRPC\tAF_RXRPC\n#define PF_ISDN\tAF_ISDN\n#define PF_PHONET\tAF_PHONET\n#define PF_IEEE802154\tAF_IEEE802154\n#define PF_CAIF\tAF_CAIF\n#define PF_ALG\tAF_ALG\n#define PF_NFC\tAF_NFC\n#define PF_VSOCK\tAF_VSOCK\n#define PF_KCM\tAF_KCM\n#define PF_QIPCRTR\tAF_QIPCRTR\n#define PF_SMC\tAF_SMC\n#define PF_XDP\tAF_XDP\n#define PF_MCTP\tAF_MCTP\n#define PF_MAX\tAF_MAX\n```\n```c\nvoid sock_init_data_uid(struct socket *sock, struct sock *sk, kuid_t uid)\n{\nsk_init_common(sk);\nsk->sk_send_head\t=\tNULL;\ntimer_setup(&sk->sk_timer, NULL, 0);\nsk->sk_allocation\t=\tGFP_KERNEL;\nsk->sk_rcvbuf\t=\tREAD_ONCE(sysctl_rmem_default);\nsk->sk_sndbuf\t=\tREAD_ONCE(sysctl_wmem_default);\nsk->sk_state\t=\tTCP_CLOSE;\nsk->sk_use_task_frag\t=\ttrue;\nsk_set_socket(sk, sock);\nsock_set_flag(sk, SOCK_ZAPPED);\nif (sock) {\nsk->sk_type\t=\tsock->type;\nRCU_INIT_POINTER(sk->sk_wq, &sock->wq);\nsock->sk\t=\tsk;\n} else {\nRCU_INIT_POINTER(sk->sk_wq, NULL);\n}\nsk->sk_uid\t=\tuid;\nsk->sk_state_change\t=\tsock_def_wakeup;\nsk->sk_data_ready\t=\tsock_def_readable;\nsk->sk_write_space\t=\tsock_def_write_space;\nsk->sk_error_report\t=\tsock_def_error_report;\nsk->sk_destruct\t=\tsock_def_destruct;\nsk->sk_frag.page\t=\tNULL;\nsk->sk_frag.offset\t=\t0;\nsk->sk_peek_off\t=\t-1;\nsk->sk_peer_pid\t=\tNULL;\nsk->sk_peer_cred\t=\tNULL;\nspin_lock_init(&sk->sk_peer_lock);\nsk->sk_write_pending\t=\t0;\nsk->sk_rcvlowat\t=\t1;\nsk->sk_rcvtimeo\t=\tMAX_SCHEDULE_TIMEOUT;\nsk->sk_sndtimeo\t=\tMAX_SCHEDULE_TIMEOUT;\nsk->sk_stamp = SK_DEFAULT_STAMP;\n#if BITS_PER_LONG==32\nseqlock_init(&sk->sk_stamp_seq);\n#endif\natomic_set(&sk->sk_zckey, 0);\n#ifdef CONFIG_NET_RX_BUSY_POLL\nsk->sk_napi_id\t=\t0;\nsk->sk_ll_usec\t=\tREAD_ONCE(sysctl_net_busy_read);\n#endif\nsk->sk_max_pacing_rate = ~0UL;\nsk->sk_pacing_rate = ~0UL;\nWRITE_ONCE(sk->sk_pacing_shift, 10);\nsk->sk_incoming_cpu = -1;\nsk_rx_queue_clear(sk);\n/*\n* Before updating sk_refcnt, we must commit prior changes to memory\n* (Documentation/RCU/rculist_nulls.rst for details)\n*/\nsmp_wmb();\nrefcount_set(&sk->sk_refcnt, 1);\natomic_set(&sk->sk_drops, 0);\n}\n```\n```c\nstatic void l2cap_sock_destruct(struct sock *sk)\n{\nstruct l2cap_rx_busy *rx_busy, *next;\nBT_DBG(\"sk %p\", sk);\nif (l2cap_pi(sk)->chan) {\nl2cap_pi(sk)->chan->data = NULL;\nl2cap_chan_put(l2cap_pi(sk)->chan);\n}\nlist_for_each_entry_safe(rx_busy, next, &l2cap_pi(sk)->rx_busy, list) {\nkfree_skb(rx_busy->skb);\nlist_del(&rx_busy->list);\nkfree(rx_busy);\n}\nskb_queue_purge(&sk->sk_receive_queue);\nskb_queue_purge(&sk->sk_write_queue);\n}\n```\n```c\n#define L2CAP_CONN_TIMEOUT\tmsecs_to_jiffies(40000)\n```\n```c\n/* Sock flags */\nenum sock_flags {\nSOCK_DEAD,\nSOCK_DONE,\nSOCK_URGINLINE,\nSOCK_KEEPOPEN,\nSOCK_LINGER,\nSOCK_DESTROY,\nSOCK_BROADCAST,\nSOCK_TIMESTAMP,\nSOCK_ZAPPED,\nSOCK_USE_WRITE_QUEUE, /* whether to call sk->sk_write_space in sock_wfree */\nSOCK_DBG, /* %SO_DEBUG setting */\nSOCK_RCVTSTAMP, /* %SO_TIMESTAMP setting */\nSOCK_RCVTSTAMPNS, /* %SO_TIMESTAMPNS setting */\nSOCK_LOCALROUTE, /* route locally only, %SO_DONTROUTE setting */\nSOCK_MEMALLOC, /* VM depends on this socket for swapping */\nSOCK_TIMESTAMPING_RX_SOFTWARE, /* %SOF_TIMESTAMPING_RX_SOFTWARE */\nSOCK_FASYNC, /* fasync() active */\nSOCK_RXQ_OVFL,\nSOCK_ZEROCOPY, /* buffers from userspace */\nSOCK_WIFI_STATUS, /* push wifi status to userspace */\nSOCK_NOFCS, /* Tell NIC not to do the Ethernet FCS.\n* Will use last 4 bytes of packet sent from\n* user-space instead.\n*/\nSOCK_FILTER_LOCKED, /* Filter cannot be changed anymore */\nSOCK_SELECT_ERR_QUEUE, /* Wake select on error queue */\nSOCK_RCU_FREE, /* wait rcu grace period in sk_destruct() */\nSOCK_TXTIME,\nSOCK_XDP, /* XDP is attached */\nSOCK_TSTAMP_NEW, /* Indicates 64 bit timestamps always */\nSOCK_RCVMARK, /* Receive SO_MARK ancillary data with packet */\nSOCK_RCVPRIORITY, /* Receive SO_PRIORITY ancillary data with packet */\n};\n```\n```c\nstruct l2cap_chan *l2cap_chan_create(void)\n{\nstruct l2cap_chan *chan;\nchan = kzalloc(sizeof(*chan), GFP_ATOMIC);\nif (!chan)\nreturn NULL;\nskb_queue_head_init(&chan->tx_q);\nskb_queue_head_init(&chan->srej_q);\nmutex_init(&chan->lock);\n/* Set default lock nesting level */\natomic_set(&chan->nesting, L2CAP_NESTING_NORMAL);\n/* Available receive buffer space is initially unknown */\nchan->rx_avail = -1;\nwrite_lock(&chan_list_lock);\nlist_add(&chan->global_l, &chan_list);\nwrite_unlock(&chan_list_lock);\nINIT_DELAYED_WORK(&chan->chan_timer, l2cap_chan_timeout);\nINIT_DELAYED_WORK(&chan->retrans_timer, l2cap_retrans_timeout);\nINIT_DELAYED_WORK(&chan->monitor_timer, l2cap_monitor_timeout);\nINIT_DELAYED_WORK(&chan->ack_timer, l2cap_ack_timeout);\nchan->state = BT_OPEN;\nkref_init(&chan->kref);\n/* This flag is cleared in l2cap_chan_ready() */\nset_bit(CONF_NOT_COMPLETE, &chan->conf_state);\nBT_DBG(\"chan %p\", chan);\nreturn chan;\n}\n```\n```c\nvoid l2cap_chan_hold(struct l2cap_chan *c)\n{\nBT_DBG(\"chan %p orig refcnt %u\", c, kref_read(&c->kref));\nkref_get(&c->kref);\n}\n```\n```c\nstatic void __sk_free(struct sock *sk)\n{\nif (likely(sk->sk_net_refcnt))\nsock_inuse_add(sock_net(sk), -1);\nif (unlikely(sk->sk_net_refcnt && sock_diag_has_destroy_listeners(sk)))\nsock_diag_broadcast_destroy(sk);\nelse\nsk_destruct(sk);\n}\nvoid sk_free(struct sock *sk)\n{\n/*\n* We subtract one from sk_wmem_alloc and can know if\n* some packets are still in some tx queue.\n* If not null, sock_wfree() will call __sk_free(sk) later\n*/\nif (refcount_dec_and_test(&sk->sk_wmem_alloc))\n__sk_free(sk);\n}\nEXPORT_SYMBOL(sk_free);\n```\n```c\n/* ----- L2CAP socket info ----- */\n#define l2cap_pi(sk) ((struct l2cap_pinfo *) sk)\nstruct l2cap_rx_busy {\nstruct list_head\tlist;\nstruct sk_buff\t*skb;\n};\nstruct l2cap_pinfo {\nstruct bt_sock\tbt;\nstruct l2cap_chan\t*chan;\nstruct list_head\trx_busy;\n};\n```",
  "original_code": "```c\nstatic struct sock *l2cap_sock_alloc(struct net *net, struct socket *sock,\nint proto, gfp_t prio, int kern)\n{\nstruct sock *sk;\nstruct l2cap_chan *chan;\nsk = sk_alloc(net, PF_BLUETOOTH, prio, &l2cap_proto, kern);\nif (!sk)\nreturn NULL;\nsock_init_data(sock, sk);\nINIT_LIST_HEAD(&bt_sk(sk)->accept_q);\nsk->sk_destruct = l2cap_sock_destruct;\nsk->sk_sndtimeo = L2CAP_CONN_TIMEOUT;\nsock_reset_flag(sk, SOCK_ZAPPED);\nsk->sk_protocol = proto;\nsk->sk_state = BT_OPEN;\nchan = l2cap_chan_create();\nif (!chan) {\nsk_free(sk);\nreturn NULL;\n}\nl2cap_chan_hold(chan);\nl2cap_pi(sk)->chan = chan;\nreturn sk;\n}\n```",
  "vuln_patch": "```c\nstatic struct sock *l2cap_sock_alloc(struct net *net, struct socket *sock,\nint proto, gfp_t prio, int kern)\n{\nstruct sock *sk;\nstruct l2cap_chan *chan;\nsk = sk_alloc(net, PF_BLUETOOTH, prio, &l2cap_proto, kern);\nif (!sk)\nreturn NULL;\nsock_init_data(sock, sk);\nINIT_LIST_HEAD(&bt_sk(sk)->accept_q);\nsk->sk_destruct = l2cap_sock_destruct;\nsk->sk_sndtimeo = L2CAP_CONN_TIMEOUT;\nsock_reset_flag(sk, SOCK_ZAPPED);\nsk->sk_protocol = proto;\nsk->sk_state = BT_OPEN;\nchan = l2cap_chan_create();\nif (!chan) {\nsk_free(sk);\nsock->sk = NULL;\nreturn NULL;\n}\nl2cap_chan_hold(chan);\nl2cap_pi(sk)->chan = chan;\nreturn sk;\n}\n```",
  "function_name": "l2cap_sock_alloc",
  "function_prototype": "static struct sock *l2cap_sock_alloc(struct net *net, struct socket *sock, int proto, gfp_t prio, int kern)",
  "code_semantics": "The target code is a function that allocates and initializes a network communication structure for a specific protocol. It first attempts to allocate memory for the structure. If successful, it initializes various fields of the structure, including setting up a queue, assigning a cleanup function, and configuring timeout and protocol settings. It then creates an associated communication channel. If channel creation is successful, it links the channel to the structure and returns the structure. If any step fails, it cleans up and returns a null reference.",
  "safe_verification_cot": "1. l2cap_chan_create() can still return NULL, leading to the error handling path.\n2. sock->sk is explicitly set to NULL after sk_free(sk) is called, preventing a dangling pointer.\n3. sock_init_data(sock, sk) correctly initializes sock->sk, and the patch ensures that sock->sk is set to NULL if sk is freed.",
  "verification_cot": "1. l2cap_chan_create() can return NULL, leading to the error handling path.\n2. sock->sk is not set to NULL after sk_free(sk) is called, leaving a dangling pointer in sock->sk.\n3. sock_init_data(sock, sk) correctly initializes sock->sk, but the issue arises when sk is freed without updating sock->sk.",
  "vulnerability_related_variables": {
    "sock->sk": "This variable serves as a reference within a higher-level structure to a lower-level structure that contains detailed data and operations related to network communication. It allows the higher-level structure to access and manipulate the lower-level structure's data.",
    "sk": "This variable represents a complex data structure that encapsulates all necessary information and operations for managing a network communication endpoint. It includes state management, protocol settings, and lifecycle operations, enabling the establishment and maintenance of network connections."
  },
  "vulnerability_related_functions": {
    "l2cap_chan_create": "This function is responsible for creating and initializing a new communication channel. It allocates memory for the channel, sets up necessary data structures like queues and locks, initializes timers, and sets default states. The channel is then added to a global list for management.",
    "sk_free": "This function manages the deallocation of a network socket. It checks if the socket's memory allocation reference count has reached zero. If so, it proceeds to decrement the network usage count and calls a destructor function to clean up resources associated with the socket.",
    "sock_init_data": "This function initializes a network socket with default settings. It sets up various parameters such as buffer sizes, state, and timeout values. It also links the socket to a network structure and sets up callback functions for handling state changes, data readiness, and errors."
  },
  "root_cause": "The root cause of the vulnerability is a use-after-free condition where the sock->sk pointer is not set to NULL after the sk object is freed, leading to a dangling pointer.",
  "patch_cot": "1. Set sock->sk to NULL after freeing sk: After calling sk_free(sk), immediately set sock->sk = NULL; to ensure that the pointer does not become a dangling pointer. 2. Review sock_init_data for proper initialization: Ensure that sock_init_data correctly initializes sock->sk and other related fields to prevent inconsistent states. 3. Review l2cap_chan_create for proper lifecycle management: Ensure that l2cap_chan_create initializes chan correctly and that its reference count and lifecycle are managed to prevent premature freeing."
}