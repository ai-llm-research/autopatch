{
  "cwe_type": "Deadlock",
  "cve_id": "CVE-2025-21807",
  "supplementary_code": "```c\nstruct kobject {\nconst char *name;\nstruct list_head entry;\nstruct kobject *parent;\nstruct kset *kset;\nconst struct kobj_type *ktype;\nstruct kernfs_node *sd; /* sysfs directory entry */\nstruct kref kref;\nunsigned int state_initialized:1;\nunsigned int state_in_sysfs:1;\nunsigned int state_add_uevent_sent:1;\nunsigned int state_remove_uevent_sent:1;\nunsigned int uevent_suppress:1;\n#ifdef CONFIG_DEBUG_KOBJECT_RELEASE\nstruct delayed_work release;\n#endif\n};\n```\n```c\nstruct attribute {\nconst char *name;\numode_t mode;\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\nbool ignore_lockdep:1;\nstruct lock_class_key *key;\nstruct lock_class_key skey;\n#endif\n};\n```\n```c\nstruct queue_sysfs_entry {\nstruct attribute attr;\nssize_t (*show)(struct gendisk *disk, char *page);\nssize_t (*store)(struct gendisk *disk, const char *page, size_t count);\nvoid (*load_module)(struct gendisk *disk, const char *page, size_t count);\n};\n```\n```c\nstruct gendisk {\n/*\n* major/first_minor/minors should not be set by any new driver, the\n* block core will take care of allocating them automatically.\n*/\nint major;\nint first_minor;\nint minors;\nchar disk_name[DISK_NAME_LEN]; /* name of major driver */\nunsigned short events; /* supported events */\nunsigned short event_flags; /* flags related to event processing */\nstruct xarray part_tbl;\nstruct block_device *part0;\nconst struct block_device_operations *fops;\nstruct request_queue *queue;\nvoid *private_data;\nstruct bio_set bio_split;\nint flags;\nunsigned long state;\n#define GD_NEED_PART_SCAN 0\n#define GD_READ_ONLY 1\n#define GD_DEAD 2\n#define GD_NATIVE_CAPACITY 3\n#define GD_ADDED 4\n#define GD_SUPPRESS_PART_SCAN 5\n#define GD_OWNS_QUEUE 6\nstruct mutex open_mutex; /* open/close mutex */\nunsigned open_partitions; /* number of open partitions */\nstruct backing_dev_info *bdi;\nstruct kobject queue_kobj; /* the queue/ directory */\nstruct kobject *slave_dir;\n#ifdef CONFIG_BLOCK_HOLDER_DEPRECATED\nstruct list_head slave_bdevs;\n#endif\nstruct timer_rand_state *random;\natomic_t sync_io; /* RAID */\nstruct disk_events *ev;\n#ifdef CONFIG_BLK_DEV_ZONED\n/*\n* Zoned block device information. Reads of this information must be\n* protected with blk_queue_enter() / blk_queue_exit(). Modifying this\n* information is only allowed while no requests are being processed.\n* See also blk_mq_freeze_queue() and blk_mq_unfreeze_queue().\n*/\nunsigned int nr_zones;\nunsigned int zone_capacity;\nunsigned int last_zone_capacity;\nunsigned long __rcu *conv_zones_bitmap;\nunsigned int zone_wplugs_hash_bits;\nspinlock_t zone_wplugs_lock;\nstruct mempool_s *zone_wplugs_pool;\nstruct hlist_head *zone_wplugs_hash;\nstruct workqueue_struct *zone_wplugs_wq;\n#endif /* CONFIG_BLK_DEV_ZONED */\n#if IS_ENABLED(CONFIG_CDROM)\nstruct cdrom_device_info *cdi;\n#endif\nint node_id;\nstruct badblocks *bb;\nstruct lockdep_map lockdep_map;\nu64 diskseq;\nblk_mode_t open_mode;\n/*\n* Independent sector access ranges. This is always NULL for\n* devices that do not have multiple independent access ranges.\n*/\nstruct blk_independent_access_ranges *ia_ranges;\n};\n```\n```c\n#define to_queue(atr) container_of((atr), struct queue_sysfs_entry, attr)\n```\n```c\n#define container_of(ptr, type, member) ({ \\\nvoid *__mptr = (void *)(ptr); \\\nstatic_assert(__same_type(*(ptr), ((type *)0)->member) || \\\n__same_type(*(ptr), void), \\\n\"pointer type mismatch in container_of()\"); \\\n((type *)(__mptr - offsetof(type, member))); })\n```\n```c\nstruct request_queue {\n/*\n* The queue owner gets to use this for whatever they like.\n* ll_rw_blk doesn't touch it.\n*/\nvoid *queuedata;\nstruct elevator_queue *elevator;\nconst struct blk_mq_ops *mq_ops;\n/* sw queues */\nstruct blk_mq_ctx __percpu *queue_ctx;\n/*\n* various queue flags, see QUEUE_* below\n*/\nunsigned long queue_flags;\nunsigned int rq_timeout;\nunsigned int queue_depth;\nrefcount_t refs;\n/* hw dispatch queues */\nunsigned int nr_hw_queues;\nstruct xarray hctx_table;\nstruct percpu_ref q_usage_counter;\nstruct lock_class_key io_lock_cls_key;\nstruct lockdep_map io_lockdep_map;\nstruct lock_class_key q_lock_cls_key;\nstruct lockdep_map q_lockdep_map;\nstruct request *last_merge;\nspinlock_t queue_lock;\nint quiesce_depth;\nstruct gendisk *disk;\n/*\n* mq queue kobject\n*/\nstruct kobject *mq_kobj;\nstruct queue_limits limits;\n#ifdef CONFIG_PM\nstruct device *dev;\nenum rpm_status rpm_status;\n#endif\n/*\n* Number of contexts that have called blk_set_pm_only(). If this\n* counter is above zero then only RQF_PM requests are processed.\n*/\natomic_t pm_only;\nstruct blk_queue_stats *stats;\nstruct rq_qos *rq_qos;\nstruct mutex rq_qos_mutex;\n/*\n* ida allocated id for this queue. Used to index queues from\n* ioctx.\n*/\nint id;\n/*\n* queue settings\n*/\nunsigned long nr_requests; /* Max # of requests */\n#ifdef CONFIG_BLK_INLINE_ENCRYPTION\nstruct blk_crypto_profile *crypto_profile;\nstruct kobject *crypto_kobject;\n#endif\nstruct timer_list timeout;\nstruct work_struct timeout_work;\natomic_t nr_active_requests_shared_tags;\nstruct blk_mq_tags *sched_shared_tags;\nstruct list_head icq_list;\n#ifdef CONFIG_BLK_CGROUP\nDECLARE_BITMAP (blkcg_pols, BLKCG_MAX_POLS);\nstruct blkcg_gq *root_blkg;\nstruct list_head blkg_list;\nstruct mutex blkcg_mutex;\n#endif\nint node;\nspinlock_t requeue_lock;\nstruct list_head requeue_list;\nstruct delayed_work requeue_work;\n#ifdef CONFIG_BLK_DEV_IO_TRACE\nstruct blk_trace __rcu *blk_trace;\n#endif\n/*\n* for flush operations\n*/\nstruct blk_flush_queue *fq;\nstruct list_head flush_list;\nstruct mutex sysfs_lock;\nstruct mutex sysfs_dir_lock;\nstruct mutex limits_lock;\n/*\n* for reusing dead hctx instance in case of updating\n* nr_hw_queues\n*/\nstruct list_head unused_hctx_list;\nspinlock_t unused_hctx_lock;\nint mq_freeze_depth;\n#ifdef CONFIG_BLK_DEV_THROTTLING\n/* Throttle data */\nstruct throtl_data *td;\n#endif\nstruct rcu_head rcu_head;\n#ifdef CONFIG_LOCKDEP\nstruct task_struct *mq_freeze_owner;\nint mq_freeze_owner_depth;\n#endif\nwait_queue_head_t mq_freeze_wq;\n/*\n* Protect concurrent access to q_usage_counter by\n* percpu_ref_kill() and percpu_ref_reinit().\n*/\nstruct mutex mq_freeze_lock;\nstruct blk_mq_tag_set *tag_set;\nstruct list_head tag_set_list;\nstruct dentry *debugfs_dir;\nstruct dentry *sched_debugfs_dir;\nstruct dentry *rqos_debugfs_dir;\n/*\n* Serializes all debugfs metadata operations using the above dentries.\n*/\nstruct mutex debugfs_mutex;\nbool mq_sysfs_init_done;\n};\n```\n```c\nvoid __sched mutex_lock(struct mutex *lock)\n{\n__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);\n}\nEXPORT_SYMBOL(mutex_lock);\n```\n```c\nvoid blk_mq_freeze_queue(struct request_queue *q)\n{\nblk_freeze_queue_start(q);\nblk_mq_freeze_queue_wait(q);\n}\nEXPORT_SYMBOL_GPL(blk_mq_freeze_queue);\n```\n```c\nstruct queue_limits {\nblk_features_t features;\nblk_flags_t flags;\nunsigned long seg_boundary_mask;\nunsigned long virt_boundary_mask;\nunsigned int max_hw_sectors;\nunsigned int max_dev_sectors;\nunsigned int chunk_sectors;\nunsigned int max_sectors;\nunsigned int max_user_sectors;\nunsigned int max_segment_size;\nunsigned int physical_block_size;\nunsigned int logical_block_size;\nunsigned int alignment_offset;\nunsigned int io_min;\nunsigned int io_opt;\nunsigned int max_discard_sectors;\nunsigned int max_hw_discard_sectors;\nunsigned int max_user_discard_sectors;\nunsigned int max_secure_erase_sectors;\nunsigned int max_write_zeroes_sectors;\nunsigned int max_hw_zone_append_sectors;\nunsigned int max_zone_append_sectors;\nunsigned int discard_granularity;\nunsigned int discard_alignment;\nunsigned int zone_write_granularity;\n/* atomic write limits */\nunsigned int atomic_write_hw_max;\nunsigned int atomic_write_max_sectors;\nunsigned int atomic_write_hw_boundary;\nunsigned int atomic_write_boundary_sectors;\nunsigned int atomic_write_hw_unit_min;\nunsigned int atomic_write_unit_min;\nunsigned int atomic_write_hw_unit_max;\nunsigned int atomic_write_unit_max;\nunsigned short max_segments;\nunsigned short max_integrity_segments;\nunsigned short max_discard_segments;\nunsigned int max_open_zones;\nunsigned int max_active_zones;\n/*\n* Drivers that set dma_alignment to less than 511 must be prepared to\n* handle individual bvec's that are not a multiple of a SECTOR_SIZE\n* due to possible offsets.\n*/\nunsigned int dma_alignment;\nunsigned int dma_pad_mask;\nstruct blk_integrity integrity;\n};\n```\n```c\nstatic inline struct queue_limits queue_limits_start_update(struct request_queue *q)\n{\nmutex_lock(&q->limits_lock);\nreturn q->limits;\n}\n```\n```c\nssize_t (*store)(struct gendisk *disk, const char *page, size_t count);\n```\n```c\nstatic inline void queue_limits_cancel_update(struct request_queue *q)\n{\nmutex_unlock(&q->limits_lock);\n}\n```\n```c\nint queue_limits_commit_update(struct request_queue *q,\nstruct queue_limits *lim)\n{\nint error;\nerror = blk_validate_limits(lim);\nif (error)\ngoto out_unlock;\n#ifdef CONFIG_BLK_INLINE_ENCRYPTION\nif (q->crypto_profile && lim->integrity.tag_size) {\npr_warn(\"blk-integrity: Integrity and hardware inline encryption are not supported together.\\n\");\nerror = -EINVAL;\ngoto out_unlock;\n}\n#endif\nq->limits = *lim;\nif (q->disk)\nblk_apply_bdi_limits(q->disk->bdi, lim);\nout_unlock:\nmutex_unlock(&q->limits_lock);\nreturn error;\n}\nEXPORT_SYMBOL_GPL(queue_limits_commit_update);\n```\n```c\nvoid blk_mq_unfreeze_queue(struct request_queue *q)\n{\nif (__blk_mq_unfreeze_queue(q, false))\nblk_unfreeze_release_lock(q, false, false);\n}\nEXPORT_SYMBOL_GPL(blk_mq_unfreeze_queue);\n```\n```c\nvoid __sched mutex_unlock(struct mutex *lock)\n{\nmutex_release(&lock->dep_map, _RET_IP_);\n__rt_mutex_unlock(&lock->rtmutex);\n}\nEXPORT_SYMBOL(mutex_unlock);\n```",
  "original_code": "```c\nstatic ssize_t queue_attr_store(struct kobject *kobj, struct attribute *attr, const char *page, size_t length)\n{\nstruct queue_sysfs_entry *entry = to_queue(attr);\nstruct gendisk *disk = container_of(kobj, struct gendisk, queue_kobj);\nstruct request_queue *q = disk->queue;\nssize_t res;\nif (!entry->store_limit && !entry->store)\nreturn -EIO;\n/*\n* If the attribute needs to load a module, do it before freezing the\n* queue to ensure that the module file can be read when the request\n* queue is the one for the device storing the module file.\n*/\nif (entry->load_module)\nentry->load_module(disk, page, length);\nmutex_lock(&q->sysfs_lock);\nblk_mq_freeze_queue(q);\nif (entry->store_limit) {\nstruct queue_limits lim = queue_limits_start_update(q);\nres = entry->store_limit(disk, page, length, &lim);\nif (res < 0) {\nqueue_limits_cancel_update(q);\n} else {\nres = queue_limits_commit_update(q, &lim);\nif (!res)\nres = length;\n}\n} else {\nres = entry->store(disk, page, length);\n}\nblk_mq_unfreeze_queue(q);\nmutex_unlock(&q->sysfs_lock);\nreturn res;\n}\n```",
  "vuln_patch": "```c\nstatic ssize_t queue_attr_store(struct kobject *kobj, struct attribute *attr, const char *page, size_t length)\n{\nstruct queue_sysfs_entry *entry = to_queue(attr);\nstruct gendisk *disk = container_of(kobj, struct gendisk, queue_kobj);\nstruct request_queue *q = disk->queue;\nssize_t res;\nif (!entry->store_limit && !entry->store)\nreturn -EIO;\n/*\n* If the attribute needs to load a module, do it before freezing the\n* queue to ensure that the module file can be read when the request\n* queue is the one for the device storing the module file.\n*/\nif (entry->load_module)\nentry->load_module(disk, page, length);\nif (entry->store_limit) {\nstruct queue_limits lim = queue_limits_start_update(q);\nres = entry->store_limit(disk, page, length, &lim);\nif (res < 0) {\nqueue_limits_cancel_update(q);\nreturn res;\n}\nres = queue_limits_commit_update_frozen(q, &lim);\nif (res)\nreturn res;\nreturn length;\n}\nmutex_lock(&q->sysfs_lock);\nblk_mq_freeze_queue(q);\nres = entry->store(disk, page, length);\nblk_mq_unfreeze_queue(q);\nmutex_unlock(&q->sysfs_lock);\nreturn res;\n}\n```",
  "function_name": "queue_attr_store",
  "function_prototype": "static ssize_t queue_attr_store(struct kobject *kobj, struct attribute *attr, const char *page, size_t length)",
  "code_semantics": "The function handles the storage of attributes for a queue associated with a block device. It first converts the attribute to a specific entry type and retrieves the disk and queue. It checks if certain operations are available and returns an error if not. If a module needs to be loaded, it does so before freezing the queue. The function locks a mutex for exclusive access, freezes the queue, and checks if a limit update is needed. If so, it starts the update, attempts to store the new limits, and either cancels or commits the update based on the result. If no limit update is needed, it directly stores the attribute. Finally, it unfreezes the queue, unlocks the mutex, and returns the result.",
  "safe_verification_cot": "1. The mutex_lock(&q->sysfs_lock) is only called if entry->store_limit is false, avoiding unnecessary locking. 2. The blk_mq_freeze_queue(q) is only called if entry->store_limit is false, preventing potential deadlocks. 3. The queue_limits_start_update(q) and queue_limits_commit_update_frozen(q, &lim) are used without holding unnecessary locks, ensuring safe updates. 4. The queue_limits_cancel_update(q) is called if entry->store_limit returns an error, and the function returns early, avoiding further operations. 5. The blk_mq_unfreeze_queue(q) is called appropriately, ensuring the queue is unfrozen only when necessary.",
  "verification_cot": "1. The mutex_lock(&q->sysfs_lock) is called before checking entry->store_limit, which can lead to holding the lock unnecessarily. 2. The blk_mq_freeze_queue(q) is called regardless of whether entry->store_limit is true, potentially causing a deadlock if store_limit tries to acquire the same lock or if the queue is already frozen. 3. The queue_limits_start_update(q) and queue_limits_commit_update(q, &lim) are executed while holding the lock, which is unnecessary and risky. 4. The queue_limits_cancel_update(q) is correctly called if entry->store_limit returns an error, but the lock is still held during this process. 5. The blk_mq_unfreeze_queue(q) is called after the operations, but the initial locking and freezing logic is flawed.",
  "vulnerability_related_variables": {
    "q->sysfs_lock": "This is a synchronization mechanism used to ensure that only one thread can access a particular section of code at a time, preventing race conditions.",
    "q": "This is a data structure that manages a sequence of operations or tasks, allowing for control over their execution order and state.",
    "entry->store_limit": "This is a reference to a function that processes input data to update configuration limits or constraints on a system component.",
    "entry->store": "This is a reference to a function that processes input data to perform a specific operation or update a configuration."
  },
  "vulnerability_related_functions": {
    "mutex_lock": "Acquire a lock on a specified mutex to ensure exclusive access to a shared resource.",
    "blk_mq_freeze_queue": "Temporarily halt the processing of requests in a queue to perform safe updates or modifications.",
    "queue_limits_start_update": "Begin an update to the queue's operational limits by acquiring a lock and returning the current limits.",
    "queue_limits_commit_update": "Validate and apply new operational limits to a queue, ensuring they meet required constraints, and release the lock.",
    "queue_limits_cancel_update": "Abort an ongoing update to the queue's operational limits by releasing the lock without applying changes.",
    "blk_mq_unfreeze_queue": "Resume the processing of requests in a queue after a temporary halt."
  },
  "root_cause": "The root cause of the deadlock is the unnecessary locking and freezing of the queue when entry->store_limit is true, which could lead to a deadlock situation if the store_limit function itself tries to acquire the same locks or if the queue is already frozen.",
  "patch_cot": "First, check if entry->store_limit is true before acquiring any locks or freezing the queue. This prevents unnecessary operations that could lead to deadlock. If entry->store_limit is true, directly handle the queue limits update using queue_limits_commit_update_frozen() to ensure the queue state is managed safely. If entry->store_limit is false, proceed with acquiring the lock using mutex_lock(&q->sysfs_lock) and freezing the queue with blk_mq_freeze_queue(q). After performing the necessary operations, ensure that the queue is unfrozen and the lock is released only if they were acquired."
}