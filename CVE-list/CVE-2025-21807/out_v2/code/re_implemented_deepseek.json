{
 "supplementary_code": "```c\nstruct kobject {\nconst char *name;\nstruct list_head entry;\nstruct kobject *parent;\nstruct kset *kset;\nconst struct kobj_type *ktype;\nstruct kernfs_node *sd; /* sysfs directory entry */\nstruct kref kref;\nunsigned int state_initialized:1;\nunsigned int state_in_sysfs:1;\nunsigned int state_add_uevent_sent:1;\nunsigned int state_remove_uevent_sent:1;\nunsigned int uevent_suppress:1;\n#ifdef CONFIG_DEBUG_KOBJECT_RELEASE\nstruct delayed_work release;\n#endif\n};\n```\n```c\nstruct attribute {\nconst char *name;\numode_t mode;\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\nbool ignore_lockdep:1;\nstruct lock_class_key *key;\nstruct lock_class_key skey;\n#endif\n};\n```\n```c\nstruct queue_sysfs_entry {\nstruct attribute attr;\nssize_t (*show)(struct gendisk *disk, char *page);\nssize_t (*store)(struct gendisk *disk, const char *page, size_t count);\nvoid (*load_module)(struct gendisk *disk, const char *page, size_t count);\n};\n```\n```c\nstruct gendisk {\n/*\n* major/first_minor/minors should not be set by any new driver, the\n* block core will take care of allocating them automatically.\n*/\nint major;\nint first_minor;\nint minors;\nchar disk_name[DISK_NAME_LEN]; /* name of major driver */\nunsigned short events; /* supported events */\nunsigned short event_flags; /* flags related to event processing */\nstruct xarray part_tbl;\nstruct block_device *part0;\nconst struct block_device_operations *fops;\nstruct request_queue *queue;\nvoid *private_data;\nstruct bio_set bio_split;\nint flags;\nunsigned long state;\n#define GD_NEED_PART_SCAN 0\n#define GD_READ_ONLY 1\n#define GD_DEAD 2\n#define GD_NATIVE_CAPACITY 3\n#define GD_ADDED 4\n#define GD_SUPPRESS_PART_SCAN 5\n#define GD_OWNS_QUEUE 6\nstruct mutex open_mutex; /* open/close mutex */\nunsigned open_partitions; /* number of open partitions */\nstruct backing_dev_info *bdi;\nstruct kobject queue_kobj; /* the queue/ directory */\nstruct kobject *slave_dir;\n#ifdef CONFIG_BLOCK_HOLDER_DEPRECATED\nstruct list_head slave_bdevs;\n#endif\nstruct timer_rand_state *random;\natomic_t sync_io; /* RAID */\nstruct disk_events *ev;\n#ifdef CONFIG_BLK_DEV_ZONED\n/*\n* Zoned block device information. Reads of this information must be\n* protected with blk_queue_enter() / blk_queue_exit(). Modifying this\n* information is only allowed while no requests are being processed.\n* See also blk_mq_freeze_queue() and blk_mq_unfreeze_queue().\n*/\nunsigned int nr_zones;\nunsigned int zone_capacity;\nunsigned int last_zone_capacity;\nunsigned long __rcu *conv_zones_bitmap;\nunsigned int zone_wplugs_hash_bits;\nspinlock_t zone_wplugs_lock;\nstruct mempool_s *zone_wplugs_pool;\nstruct hlist_head *zone_wplugs_hash;\nstruct workqueue_struct *zone_wplugs_wq;\n#endif /* CONFIG_BLK_DEV_ZONED */\n#if IS_ENABLED(CONFIG_CDROM)\nstruct cdrom_device_info *cdi;\n#endif\nint node_id;\nstruct badblocks *bb;\nstruct lockdep_map lockdep_map;\nu64 diskseq;\nblk_mode_t open_mode;\n/*\n* Independent sector access ranges. This is always NULL for\n* devices that do not have multiple independent access ranges.\n*/\nstruct blk_independent_access_ranges *ia_ranges;\n};\n```\n```c\n#define to_queue(atr) container_of((atr), struct queue_sysfs_entry, attr)\n```\n```c\n#define container_of(ptr, type, member) ({ \\\nvoid *__mptr = (void *)(ptr); \\\nstatic_assert(__same_type(*(ptr), ((type *)0)->member) || \\\n__same_type(*(ptr), void), \\\n\"pointer type mismatch in container_of()\"); \\\n((type *)(__mptr - offsetof(type, member))); })\n```\n```c\nstruct request_queue {\n/*\n* The queue owner gets to use this for whatever they like.\n* ll_rw_blk doesn't touch it.\n*/\nvoid *queuedata;\nstruct elevator_queue *elevator;\nconst struct blk_mq_ops *mq_ops;\n/* sw queues */\nstruct blk_mq_ctx __percpu *queue_ctx;\n/*\n* various queue flags, see QUEUE_* below\n*/\nunsigned long queue_flags;\nunsigned int rq_timeout;\nunsigned int queue_depth;\nrefcount_t refs;\n/* hw dispatch queues */\nunsigned int nr_hw_queues;\nstruct xarray hctx_table;\nstruct percpu_ref q_usage_counter;\nstruct lock_class_key io_lock_cls_key;\nstruct lockdep_map io_lockdep_map;\nstruct lock_class_key q_lock_cls_key;\nstruct lockdep_map q_lockdep_map;\nstruct request *last_merge;\nspinlock_t queue_lock;\nint quiesce_depth;\nstruct gendisk *disk;\n/*\n* mq queue kobject\n*/\nstruct kobject *mq_kobj;\nstruct queue_limits limits;\n#ifdef CONFIG_PM\nstruct device *dev;\nenum rpm_status rpm_status;\n#endif\n/*\n* Number of contexts that have called blk_set_pm_only(). If this\n* counter is above zero then only RQF_PM requests are processed.\n*/\natomic_t pm_only;\nstruct blk_queue_stats *stats;\nstruct rq_qos *rq_qos;\nstruct mutex rq_qos_mutex;\n/*\n* ida allocated id for this queue. Used to index queues from\n* ioctx.\n*/\nint id;\n/*\n* queue settings\n*/\nunsigned long nr_requests; /* Max # of requests */\n#ifdef CONFIG_BLK_INLINE_ENCRYPTION\nstruct blk_crypto_profile *crypto_profile;\nstruct kobject *crypto_kobject;\n#endif\nstruct timer_list timeout;\nstruct work_struct timeout_work;\natomic_t nr_active_requests_shared_tags;\nstruct blk_mq_tags *sched_shared_tags;\nstruct list_head icq_list;\n#ifdef CONFIG_BLK_CGROUP\nDECLARE_BITMAP (blkcg_pols, BLKCG_MAX_POLS);\nstruct blkcg_gq *root_blkg;\nstruct list_head blkg_list;\nstruct mutex blkcg_mutex;\n#endif\nint node;\nspinlock_t requeue_lock;\nstruct list_head requeue_list;\nstruct delayed_work requeue_work;\n#ifdef CONFIG_BLK_DEV_IO_TRACE\nstruct blk_trace __rcu *blk_trace;\n#endif\n/*\n* for flush operations\n*/\nstruct blk_flush_queue *fq;\nstruct list_head flush_list;\nstruct mutex sysfs_lock;\nstruct mutex sysfs_dir_lock;\nstruct mutex limits_lock;\n/*\n* for reusing dead hctx instance in case of updating\n* nr_hw_queues\n*/\nstruct list_head unused_hctx_list;\nspinlock_t unused_hctx_lock;\nint mq_freeze_depth;\n#ifdef CONFIG_BLK_DEV_THROTTLING\n/* Throttle data */\nstruct throtl_data *td;\n#endif\nstruct rcu_head rcu_head;\n#ifdef CONFIG_LOCKDEP\nstruct task_struct *mq_freeze_owner;\nint mq_freeze_owner_depth;\n#endif\nwait_queue_head_t mq_freeze_wq;\n/*\n* Protect concurrent access to q_usage_counter by\n* percpu_ref_kill() and percpu_ref_reinit().\n*/\nstruct mutex mq_freeze_lock;\nstruct blk_mq_tag_set *tag_set;\nstruct list_head tag_set_list;\nstruct dentry *debugfs_dir;\nstruct dentry *sched_debugfs_dir;\nstruct dentry *rqos_debugfs_dir;\n/*\n* Serializes all debugfs metadata operations using the above dentries.\n*/\nstruct mutex debugfs_mutex;\nbool mq_sysfs_init_done;\n};\n```\n```c\nvoid __sched mutex_lock(struct mutex *lock)\n{\n__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);\n}\nEXPORT_SYMBOL(mutex_lock);\n```\n```c\nvoid blk_mq_freeze_queue(struct request_queue *q)\n{\nblk_freeze_queue_start(q);\nblk_mq_freeze_queue_wait(q);\n}\nEXPORT_SYMBOL_GPL(blk_mq_freeze_queue);\n```\n```c\nstruct queue_limits {\nblk_features_t features;\nblk_flags_t flags;\nunsigned long seg_boundary_mask;\nunsigned long virt_boundary_mask;\nunsigned int max_hw_sectors;\nunsigned int max_dev_sectors;\nunsigned int chunk_sectors;\nunsigned int max_sectors;\nunsigned int max_user_sectors;\nunsigned int max_segment_size;\nunsigned int physical_block_size;\nunsigned int logical_block_size;\nunsigned int alignment_offset;\nunsigned int io_min;\nunsigned int io_opt;\nunsigned int max_discard_sectors;\nunsigned int max_hw_discard_sectors;\nunsigned int max_user_discard_sectors;\nunsigned int max_secure_erase_sectors;\nunsigned int max_write_zeroes_sectors;\nunsigned int max_hw_zone_append_sectors;\nunsigned int max_zone_append_sectors;\nunsigned int discard_granularity;\nunsigned int discard_alignment;\nunsigned int zone_write_granularity;\n/* atomic write limits */\nunsigned int atomic_write_hw_max;\nunsigned int atomic_write_max_sectors;\nunsigned int atomic_write_hw_boundary;\nunsigned int atomic_write_boundary_sectors;\nunsigned int atomic_write_hw_unit_min;\nunsigned int atomic_write_unit_min;\nunsigned int atomic_write_hw_unit_max;\nunsigned int atomic_write_unit_max;\nunsigned short max_segments;\nunsigned short max_integrity_segments;\nunsigned short max_discard_segments;\nunsigned int max_open_zones;\nunsigned int max_active_zones;\n/*\n* Drivers that set dma_alignment to less than 511 must be prepared to\n* handle individual bvec's that are not a multiple of a SECTOR_SIZE\n* due to possible offsets.\n*/\nunsigned int dma_alignment;\nunsigned int dma_pad_mask;\nstruct blk_integrity integrity;\n};\n```\n```c\nstatic inline struct queue_limits queue_limits_start_update(struct request_queue *q)\n{\nmutex_lock(&q->limits_lock);\nreturn q->limits;\n}\n```\n```c\nssize_t (*store)(struct gendisk *disk, const char *page, size_t count);\n```\n```c\nstatic inline void queue_limits_cancel_update(struct request_queue *q)\n{\nmutex_unlock(&q->limits_lock);\n}\n```\n```c\nint queue_limits_commit_update(struct request_queue *q,\nstruct queue_limits *lim)\n{\nint error;\nerror = blk_validate_limits(lim);\nif (error)\ngoto out_unlock;\n#ifdef CONFIG_BLK_INLINE_ENCRYPTION\nif (q->crypto_profile && lim->integrity.tag_size) {\npr_warn(\"blk-integrity: Integrity and hardware inline encryption are not supported together.\\n\");\nerror = -EINVAL;\ngoto out_unlock;\n}\n#endif\nq->limits = *lim;\nif (q->disk)\nblk_apply_bdi_limits(q->disk->bdi, lim);\nout_unlock:\nmutex_unlock(&q->limits_lock);\nreturn error;\n}\nEXPORT_SYMBOL_GPL(queue_limits_commit_update);\n```\n```c\nvoid blk_mq_unfreeze_queue(struct request_queue *q)\n{\nif (__blk_mq_unfreeze_queue(q, false))\nblk_unfreeze_release_lock(q, false, false);\n}\nEXPORT_SYMBOL_GPL(blk_mq_unfreeze_queue);\n```\n```c\nvoid __sched mutex_unlock(struct mutex *lock)\n{\nmutex_release(&lock->dep_map, _RET_IP_);\n__rt_mutex_unlock(&lock->rtmutex);\n}\nEXPORT_SYMBOL(mutex_unlock);\n```\n",
 "function_prototype": "static ssize_t queue_attr_store(struct kobject *kobj, struct attribute *attr, const char *page, size_t length)",
 "re_implemented_code": "\n```c\nstatic ssize_t queue_attr_store(struct kobject *kobj, struct attribute *attr, const char *page, size_t length) {\n// Convert the attribute to a queue_sysfs_entry structure\nstruct queue_sysfs_entry *entry = to_queue(attr);\n// Retrieve the gendisk structure from the kobject\nstruct gendisk *disk = kobj_to_gd(kobj);\n// Get the request_queue from the gendisk\nstruct request_queue *q = disk->queue;\n// Declare a variable to store the result of the operation\nssize_t res = length;\n// Check if there are no store_limit and store functions available\nif (!entry->store_limit && !entry->store)\n// Return an error if neither function is available\nreturn -EOPNOTSUPP;\n// If the attribute requires loading a module, do it before freezing the queue\nif (entry->load_module)\n// Load the module using the provided function\nentry->load_module(disk, page, length);\n// Lock the sysfs mutex to ensure exclusive access\nmutex_lock(&q->sysfs_lock);\n// Freeze the request queue to prevent new requests\nblk_mq_freeze_queue(q);\n// Check if there is a store_limit function available\nif (entry->store_limit) {\n// Start updating the queue limits\nstruct queue_limits lim = queue_limits_start_update(q);\n// Call the store_limit function and store the result\nres = entry->store_limit(disk, &lim, page, length);\n// If the result is negative, cancel the update\nif (res < 0) {\n// Cancel the queue limits update\nqueue_limits_cancel_update(q);\n} else {\n// Commit the queue limits update\nqueue_limits_commit_update(q, &lim);\n// If the update was successful, set the result to the length\nif (!res)\nres = length;\n}\n} else {\n// Call the store function and store the result\nres = entry->store(disk, page, length);\n}\n// Unfreeze the request queue to allow new requests\nblk_mq_unfreeze_queue(q);\n// Unlock the sysfs mutex\nmutex_unlock(&q->sysfs_lock);\n// Return the result of the operation\nreturn res;\n}\n",
 "is_vulnerable": true
}