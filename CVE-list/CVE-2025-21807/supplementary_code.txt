```c
struct kobject {
    const char      *name;
    struct list_head    entry;
    struct kobject      *parent;
    struct kset     *kset;
    const struct kobj_type  *ktype;
    struct kernfs_node  *sd; /* sysfs directory entry */
    struct kref     kref;

    unsigned int state_initialized:1;
    unsigned int state_in_sysfs:1;
    unsigned int state_add_uevent_sent:1;
    unsigned int state_remove_uevent_sent:1;
    unsigned int uevent_suppress:1;

#ifdef CONFIG_DEBUG_KOBJECT_RELEASE
    struct delayed_work release;
#endif
};
```

```c
struct attribute {
    const char      *name;
    umode_t         mode;
#ifdef CONFIG_DEBUG_LOCK_ALLOC
    bool            ignore_lockdep:1;
    struct lock_class_key   *key;
    struct lock_class_key   skey;
#endif
};
```

```c
struct queue_sysfs_entry {
    struct attribute attr;
    ssize_t (*show)(struct gendisk *disk, char *page);
    ssize_t (*store)(struct gendisk *disk, const char *page, size_t count);
    void (*load_module)(struct gendisk *disk, const char *page, size_t count);
};
```

```c
struct gendisk {
    /*
     * major/first_minor/minors should not be set by any new driver, the
     * block core will take care of allocating them automatically.
     */
    int major;
    int first_minor;
    int minors;

    char disk_name[DISK_NAME_LEN];  /* name of major driver */

    unsigned short events;      /* supported events */
    unsigned short event_flags; /* flags related to event processing */

    struct xarray part_tbl;
    struct block_device *part0;

    const struct block_device_operations *fops;
    struct request_queue *queue;
    void *private_data;

    struct bio_set bio_split;

    int flags;
    unsigned long state;
#define GD_NEED_PART_SCAN       0
#define GD_READ_ONLY            1
#define GD_DEAD             2
#define GD_NATIVE_CAPACITY      3
#define GD_ADDED            4
#define GD_SUPPRESS_PART_SCAN       5
#define GD_OWNS_QUEUE           6

    struct mutex open_mutex;    /* open/close mutex */
    unsigned open_partitions;   /* number of open partitions */

    struct backing_dev_info *bdi;
    struct kobject queue_kobj;  /* the queue/ directory */
    struct kobject *slave_dir;
#ifdef CONFIG_BLOCK_HOLDER_DEPRECATED
    struct list_head slave_bdevs;
#endif
    struct timer_rand_state *random;
    atomic_t sync_io;       /* RAID */
    struct disk_events *ev;

#ifdef CONFIG_BLK_DEV_ZONED
    /*
     * Zoned block device information. Reads of this information must be
     * protected with blk_queue_enter() / blk_queue_exit(). Modifying this
     * information is only allowed while no requests are being processed.
     * See also blk_mq_freeze_queue() and blk_mq_unfreeze_queue().
     */
    unsigned int        nr_zones;
    unsigned int        zone_capacity;
    unsigned int        last_zone_capacity;
    unsigned long __rcu *conv_zones_bitmap;
    unsigned int            zone_wplugs_hash_bits;
    spinlock_t              zone_wplugs_lock;
    struct mempool_s    *zone_wplugs_pool;
    struct hlist_head       *zone_wplugs_hash;
    struct workqueue_struct *zone_wplugs_wq;
#endif /* CONFIG_BLK_DEV_ZONED */

#if IS_ENABLED(CONFIG_CDROM)
    struct cdrom_device_info *cdi;
#endif
    int node_id;
    struct badblocks *bb;
    struct lockdep_map lockdep_map;
    u64 diskseq;
    blk_mode_t open_mode;

    /*
     * Independent sector access ranges. This is always NULL for
     * devices that do not have multiple independent access ranges.
     */
    struct blk_independent_access_ranges *ia_ranges;
};
```

```c
#define to_queue(atr) container_of((atr), struct queue_sysfs_entry, attr)
```

```c
#define container_of(ptr, type, member) ({              \
    void *__mptr = (void *)(ptr);                   \
    static_assert(__same_type(*(ptr), ((type *)0)->member) ||   \
              __same_type(*(ptr), void),            \
              "pointer type mismatch in container_of()");   \
    ((type *)(__mptr - offsetof(type, member))); })
```

```c
struct request_queue {
    /*
     * The queue owner gets to use this for whatever they like.
     * ll_rw_blk doesn't touch it.
     */
    void            *queuedata;

    struct elevator_queue   *elevator;

    const struct blk_mq_ops *mq_ops;

    /* sw queues */
    struct blk_mq_ctx __percpu  *queue_ctx;

    /*
     * various queue flags, see QUEUE_* below
     */
    unsigned long       queue_flags;

    unsigned int        rq_timeout;

    unsigned int        queue_depth;

    refcount_t      refs;

    /* hw dispatch queues */
    unsigned int        nr_hw_queues;
    struct xarray       hctx_table;

    struct percpu_ref   q_usage_counter;
    struct lock_class_key   io_lock_cls_key;
    struct lockdep_map  io_lockdep_map;

    struct lock_class_key   q_lock_cls_key;
    struct lockdep_map  q_lockdep_map;

    struct request      *last_merge;

    spinlock_t      queue_lock;

    int         quiesce_depth;

    struct gendisk      *disk;

    /*
     * mq queue kobject
     */
    struct kobject *mq_kobj;

    struct queue_limits limits;

#ifdef CONFIG_PM
    struct device       *dev;
    enum rpm_status     rpm_status;
#endif

    /*
     * Number of contexts that have called blk_set_pm_only(). If this
     * counter is above zero then only RQF_PM requests are processed.
     */
    atomic_t        pm_only;

    struct blk_queue_stats  *stats;
    struct rq_qos       *rq_qos;
    struct mutex        rq_qos_mutex;

    /*
     * ida allocated id for this queue.  Used to index queues from
     * ioctx.
     */
    int         id;

    /*
     * queue settings
     */
    unsigned long       nr_requests;    /* Max # of requests */

#ifdef CONFIG_BLK_INLINE_ENCRYPTION
    struct blk_crypto_profile *crypto_profile;
    struct kobject *crypto_kobject;
#endif

    struct timer_list   timeout;
    struct work_struct  timeout_work;

    atomic_t        nr_active_requests_shared_tags;

    struct blk_mq_tags  *sched_shared_tags;

    struct list_head    icq_list;
#ifdef CONFIG_BLK_CGROUP
    DECLARE_BITMAP      (blkcg_pols, BLKCG_MAX_POLS);
    struct blkcg_gq     *root_blkg;
    struct list_head    blkg_list;
    struct mutex        blkcg_mutex;
#endif

    int         node;

    spinlock_t      requeue_lock;
    struct list_head    requeue_list;
    struct delayed_work requeue_work;

#ifdef CONFIG_BLK_DEV_IO_TRACE
    struct blk_trace __rcu  *blk_trace;
#endif
    /*
     * for flush operations
     */
    struct blk_flush_queue  *fq;
    struct list_head    flush_list;

    struct mutex        sysfs_lock;
    struct mutex        sysfs_dir_lock;
    struct mutex        limits_lock;

    /*
     * for reusing dead hctx instance in case of updating
     * nr_hw_queues
     */
    struct list_head    unused_hctx_list;
    spinlock_t      unused_hctx_lock;

    int         mq_freeze_depth;

#ifdef CONFIG_BLK_DEV_THROTTLING
    /* Throttle data */
    struct throtl_data *td;
#endif
    struct rcu_head     rcu_head;
#ifdef CONFIG_LOCKDEP
    struct task_struct  *mq_freeze_owner;
    int         mq_freeze_owner_depth;
#endif
    wait_queue_head_t   mq_freeze_wq;
    /*
     * Protect concurrent access to q_usage_counter by
     * percpu_ref_kill() and percpu_ref_reinit().
     */
    struct mutex        mq_freeze_lock;

    struct blk_mq_tag_set   *tag_set;
    struct list_head    tag_set_list;

    struct dentry       *debugfs_dir;
    struct dentry       *sched_debugfs_dir;
    struct dentry       *rqos_debugfs_dir;
    /*
     * Serializes all debugfs metadata operations using the above dentries.
     */
    struct mutex        debugfs_mutex;

    bool            mq_sysfs_init_done;
};
```

```c
void __sched mutex_lock(struct mutex *lock)
{
    __mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);
}
EXPORT_SYMBOL(mutex_lock);
```

```c
void blk_mq_freeze_queue(struct request_queue *q)
{
    blk_freeze_queue_start(q);
    blk_mq_freeze_queue_wait(q);
}
EXPORT_SYMBOL_GPL(blk_mq_freeze_queue);
```

```c
struct queue_limits {
    blk_features_t      features;
    blk_flags_t     flags;
    unsigned long       seg_boundary_mask;
    unsigned long       virt_boundary_mask;

    unsigned int        max_hw_sectors;
    unsigned int        max_dev_sectors;
    unsigned int        chunk_sectors;
    unsigned int        max_sectors;
    unsigned int        max_user_sectors;
    unsigned int        max_segment_size;
    unsigned int        physical_block_size;
    unsigned int        logical_block_size;
    unsigned int        alignment_offset;
    unsigned int        io_min;
    unsigned int        io_opt;
    unsigned int        max_discard_sectors;
    unsigned int        max_hw_discard_sectors;
    unsigned int        max_user_discard_sectors;
    unsigned int        max_secure_erase_sectors;
    unsigned int        max_write_zeroes_sectors;
    unsigned int        max_hw_zone_append_sectors;
    unsigned int        max_zone_append_sectors;
    unsigned int        discard_granularity;
    unsigned int        discard_alignment;
    unsigned int        zone_write_granularity;

    /* atomic write limits */
    unsigned int        atomic_write_hw_max;
    unsigned int        atomic_write_max_sectors;
    unsigned int        atomic_write_hw_boundary;
    unsigned int        atomic_write_boundary_sectors;
    unsigned int        atomic_write_hw_unit_min;
    unsigned int        atomic_write_unit_min;
    unsigned int        atomic_write_hw_unit_max;
    unsigned int        atomic_write_unit_max;

    unsigned short      max_segments;
    unsigned short      max_integrity_segments;
    unsigned short      max_discard_segments;

    unsigned int        max_open_zones;
    unsigned int        max_active_zones;

    /*
     * Drivers that set dma_alignment to less than 511 must be prepared to
     * handle individual bvec's that are not a multiple of a SECTOR_SIZE
     * due to possible offsets.
     */
    unsigned int        dma_alignment;
    unsigned int        dma_pad_mask;

    struct blk_integrity    integrity;
};
```

```c
static inline struct queue_limits queue_limits_start_update(struct request_queue *q)
{
    mutex_lock(&q->limits_lock);
    return q->limits;
}
```

```c
ssize_t (*store)(struct gendisk *disk, const char *page, size_t count);
```

```c
static inline void queue_limits_cancel_update(struct request_queue *q)
{
    mutex_unlock(&q->limits_lock);
}
```

```c
int queue_limits_commit_update(struct request_queue *q,
        struct queue_limits *lim)
{
    int error;

    error = blk_validate_limits(lim);
    if (error)
        goto out_unlock;

#ifdef CONFIG_BLK_INLINE_ENCRYPTION
    if (q->crypto_profile && lim->integrity.tag_size) {
        pr_warn("blk-integrity: Integrity and hardware inline encryption are not supported together.\n");
        error = -EINVAL;
        goto out_unlock;
    }
#endif

    q->limits = *lim;
    if (q->disk)
        blk_apply_bdi_limits(q->disk->bdi, lim);
out_unlock:
    mutex_unlock(&q->limits_lock);
    return error;
}
EXPORT_SYMBOL_GPL(queue_limits_commit_update);
```

```c
void blk_mq_unfreeze_queue(struct request_queue *q)
{
    if (__blk_mq_unfreeze_queue(q, false))
        blk_unfreeze_release_lock(q, false, false);
}
EXPORT_SYMBOL_GPL(blk_mq_unfreeze_queue);
```

```c
void __sched mutex_unlock(struct mutex *lock)
{
    mutex_release(&lock->dep_map, _RET_IP_);
    __rt_mutex_unlock(&lock->rtmutex);
}
EXPORT_SYMBOL(mutex_unlock);
```
