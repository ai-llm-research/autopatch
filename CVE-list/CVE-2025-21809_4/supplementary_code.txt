```c
struct rxrpc_local {
    struct rcu_head     rcu;
    atomic_t        active_users;   /* Number of users of the local endpoint */
    refcount_t      ref;        /* Number of references to the structure */
    struct net      *net;       /* The network namespace */
    struct rxrpc_net    *rxnet;     /* Our bits in the network namespace */
    struct hlist_node   link;
    struct socket       *socket;    /* my UDP socket */
    struct task_struct  *io_thread;
    struct completion   io_thread_ready; /* Indication that the I/O thread started */
    struct page_frag_cache  tx_alloc;   /* Tx control packet allocation (I/O thread only) */
    struct rxrpc_sock   *service;   /* Service(s) listening on this endpoint */
#ifdef CONFIG_AF_RXRPC_INJECT_RX_DELAY
    struct sk_buff_head rx_delay_queue; /* Delay injection queue */
#endif
    struct sk_buff_head rx_queue;   /* Received packets */
    struct list_head    conn_attend_q;  /* Conns requiring immediate attention */
    struct list_head    call_attend_q;  /* Calls requiring immediate attention */

    struct rb_root      client_bundles; /* Client connection bundles by socket params */
    spinlock_t      client_bundles_lock; /* Lock for client_bundles */
    bool            kill_all_client_conns;
    struct list_head    idle_client_conns;
    struct timer_list   client_conn_reap_timer;
    unsigned long       client_conn_flags;
#define RXRPC_CLIENT_CONN_REAP_TIMER    0   /* The client conn reap timer expired */

    spinlock_t      lock;       /* access lock */
    rwlock_t        services_lock;  /* lock for services list */
    int         debug_id;   /* debug ID for printks */
    bool            dead;
    bool            service_closed; /* Service socket closed */
    struct idr      conn_ids;   /* List of connection IDs */
    struct list_head    new_client_calls; /* Newly created client calls need connection */
    spinlock_t      client_call_lock; /* Lock for ->new_client_calls */
    struct sockaddr_rxrpc   srx;        /* local address */
};
```

```c
struct sockaddr_rxrpc {
    __kernel_sa_family_t    srx_family; /* address family */
    __u16           srx_service;    /* service desired */
    __u16           transport_type; /* type of transport socket (SOCK_DGRAM) */
    __u16           transport_len;  /* length of transport address */
    union {
        __kernel_sa_family_t family;    /* transport address family */
        struct sockaddr_in sin;     /* IPv4 transport address */
        struct sockaddr_in6 sin6;   /* IPv6 transport address */
    } transport;
};
```

```c
struct rxrpc_peer {
    struct rcu_head     rcu;        /* This must be first */
    refcount_t      ref;
    unsigned long       hash_key;
    struct hlist_node   hash_link;
    struct rxrpc_local  *local;
    struct hlist_head   error_targets;  /* targets for net error distribution */
    struct rb_root      service_conns;  /* Service connections */
    struct list_head    keepalive_link; /* Link in net->peer_keepalive[] */
    time64_t        last_tx_at; /* Last time packet sent here */
    seqlock_t       service_conn_lock;
    spinlock_t      lock;       /* access lock */
    unsigned int        if_mtu;     /* interface MTU for this peer */
    unsigned int        mtu;        /* network MTU for this peer */
    unsigned int        maxdata;    /* data size (MTU - hdrsize) */
    unsigned short      hdrsize;    /* header size (IP + UDP + RxRPC) */
    int         debug_id;   /* debug ID for printks */
    struct sockaddr_rxrpc   srx;        /* remote address */

    /* calculated RTT cache */
#define RXRPC_RTT_CACHE_SIZE 32
    spinlock_t      rtt_input_lock; /* RTT lock for input routine */
    ktime_t         rtt_last_req;   /* Time of last RTT request */
    unsigned int        rtt_count;  /* Number of samples we've got */

    u32         srtt_us;    /* smoothed round trip time << 3 in usecs */
    u32         mdev_us;    /* medium deviation         */
    u32         mdev_max_us;    /* maximal mdev for the last rtt period */
    u32         rttvar_us;  /* smoothed mdev_max            */
    u32         rto_us;     /* Retransmission timeout in usec */
    u8          backoff;    /* Backoff timeout (as shift) */

    u8          cong_ssthresh;  /* Congestion slow-start threshold */
};
```

```c
struct rxrpc_net {
    struct proc_dir_entry   *proc_net;  /* Subdir in /proc/net */
    u32         epoch;      /* Local epoch for detecting local-end reset */
    struct list_head    calls;      /* List of calls active in this namespace */
    spinlock_t      call_lock;  /* Lock for ->calls */
    atomic_t        nr_calls;   /* Count of allocated calls */

    atomic_t        nr_conns;
    struct list_head    bundle_proc_list; /* List of bundles for proc */
    struct list_head    conn_proc_list; /* List of conns in this namespace for proc */
    struct list_head    service_conns;  /* Service conns in this namespace */
    rwlock_t        conn_lock;  /* Lock for ->conn_proc_list, ->service_conns */
    struct work_struct  service_conn_reaper;
    struct timer_list   service_conn_reap_timer;

    bool            live;

    atomic_t        nr_client_conns;

    struct hlist_head   local_endpoints;
    struct mutex        local_mutex;    /* Lock for ->local_endpoints */

    DECLARE_HASHTABLE   (peer_hash, 10);
    spinlock_t      peer_hash_lock; /* Lock for ->peer_hash */

#define RXRPC_KEEPALIVE_TIME 20 /* NAT keepalive time in seconds */
    u8          peer_keepalive_cursor;
    time64_t        peer_keepalive_base;
    struct list_head    peer_keepalive[32];
    struct list_head    peer_keepalive_new;
    struct timer_list   peer_keepalive_timer;
    struct work_struct  peer_keepalive_work;

    atomic_t        stat_tx_data;
    atomic_t        stat_tx_data_retrans;
    atomic_t        stat_tx_data_send;
    atomic_t        stat_tx_data_send_frag;
    atomic_t        stat_tx_data_send_fail;
    atomic_t        stat_tx_data_underflow;
    atomic_t        stat_tx_data_cwnd_reset;
    atomic_t        stat_rx_data;
    atomic_t        stat_rx_data_reqack;
    atomic_t        stat_rx_data_jumbo;

    atomic_t        stat_tx_ack_fill;
    atomic_t        stat_tx_ack_send;
    atomic_t        stat_tx_ack_skip;
    atomic_t        stat_tx_acks[256];
    atomic_t        stat_rx_acks[256];

    atomic_t        stat_why_req_ack[8];

    atomic_t        stat_io_loop;
};
```

```c
static unsigned long rxrpc_peer_hash_key(struct rxrpc_local *local, const struct sockaddr_rxrpc *srx)
{
    const u16 *p;
    unsigned int i, size;
    unsigned long hash_key;

    _enter("");

    hash_key = (unsigned long)local / __alignof__(*local);
    hash_key += srx->transport_type;
    hash_key += srx->transport_len;
    hash_key += srx->transport.family;

    switch (srx->transport.family) {
    case AF_INET:
        hash_key += (u16 __force)srx->transport.sin.sin_port;
        size = sizeof(srx->transport.sin.sin_addr);
        p = (u16 *)&srx->transport.sin.sin_addr;
        break;
#ifdef CONFIG_AF_RXRPC_IPV6
    case AF_INET6:
        hash_key += (u16 __force)srx->transport.sin.sin_port;
        size = sizeof(srx->transport.sin6.sin6_addr);
        p = (u16 *)&srx->transport.sin6.sin6_addr;
        break;
#endif
    default:
        WARN(1, "AF_RXRPC: Unsupported transport address family\n");
        return 0;
    }

    /* Step through the peer address in 16-bit portions for speed */
    for (i = 0; i < size; i += sizeof(*p), p++)
        hash_key += *p;

    _leave(" 0x%lx", hash_key);
    return hash_key;
}
```

```c
static __always_inline void rcu_read_lock(void)
{
    __rcu_read_lock();
    __acquire(RCU);
    rcu_lock_acquire(&rcu_lock_map);
    RCU_LOCKDEP_WARN(!rcu_is_watching(),
             "rcu_read_lock() used illegally while idle");
}
```

```c
static struct rxrpc_peer *__rxrpc_lookup_peer_rcu(struct rxrpc_local *local, const struct sockaddr_rxrpc *srx, unsigned long hash_key)
{
    struct rxrpc_peer *peer;
    struct rxrpc_net *rxnet = local->rxnet;

    hash_for_each_possible_rcu(rxnet->peer_hash, peer, hash_link, hash_key) {
        if (rxrpc_peer_cmp_key(peer, local, srx, hash_key) == 0 &&
            refcount_read(&peer->ref) > 0)
            return peer;
    }

    return NULL;
}
```

```c
struct rxrpc_peer *rxrpc_get_peer_maybe(struct rxrpc_peer *peer, enum rxrpc_peer_trace why)
{
    int r;

    if (peer) {
        if (__refcount_inc_not_zero(&peer->ref, &r))
            trace_rxrpc_peer(peer->debug_id, r + 1, why);
        else
            peer = NULL;
    }
    return peer;
}
```

```c
static inline void rcu_read_unlock(void)
{
    RCU_LOCKDEP_WARN(!rcu_is_watching(),
             "rcu_read_unlock() used illegally while idle");
    rcu_lock_release(&rcu_lock_map); /* Keep acq info for rls diags. */
    __release(RCU);
    __rcu_read_unlock();
}
```

```c
static struct rxrpc_peer *rxrpc_create_peer(struct rxrpc_local *local, struct sockaddr_rxrpc *srx, unsigned long hash_key, gfp_t gfp)
{
    struct rxrpc_peer *peer;

    _enter("");

    peer = rxrpc_alloc_peer(local, gfp, rxrpc_peer_new_client);
    if (peer) {
        memcpy(&peer->srx, srx, sizeof(*srx));
        rxrpc_init_peer(local, peer, hash_key);
    }

    _leave(" = %p", peer);
    return peer;
}
```

```c
static inline void spin_lock(spinlock_t *lock)
{
    int ret = pthread_spin_lock(lock);
    assert(!ret);
}
```

```c
#define hash_add_rcu(hashtable, node, key)                  \
    hlist_add_head_rcu(node, &hashtable[hash_min(key, HASH_BITS(hashtable))])
```

```c
static inline void list_add_tail(struct list_head *new, struct list_head *head)
{
    __list_add(new, head->prev, head);
}
```

```c
static inline void spin_unlock(spinlock_t *lock)
{
    int ret = pthread_spin_unlock(lock);
    assert(!ret);
}
```

```c
static void rxrpc_free_peer(struct rxrpc_peer *peer)
{
    trace_rxrpc_peer(peer->debug_id, 0, rxrpc_peer_free);
    rxrpc_put_local(peer->local, rxrpc_local_put_peer);
    kfree_rcu(peer, rcu);
}
```

```c
static inline unsigned int refcount_read(const refcount_t *r)
{
    return atomic_read(&r->refs);
}
```
