{
  "cwe_type": "Improper Locking",
  "cve_id": "CVE-2025-21809",
  "supplementary_code": "```c\nstruct rxrpc_local {\nstruct rcu_head rcu;\natomic_t active_users; /* Number of users of the local endpoint */\nrefcount_t ref; /* Number of references to the structure */\nstruct net *net; /* The network namespace */\nstruct rxrpc_net *rxnet; /* Our bits in the network namespace */\nstruct hlist_node link;\nstruct socket *socket; /* my UDP socket */\nstruct task_struct *io_thread;\nstruct completion io_thread_ready; /* Indication that the I/O thread started */\nstruct page_frag_cache tx_alloc; /* Tx control packet allocation (I/O thread only) */\nstruct rxrpc_sock *service; /* Service(s) listening on this endpoint */\n#ifdef CONFIG_AF_RXRPC_INJECT_RX_DELAY\nstruct sk_buff_head rx_delay_queue; /* Delay injection queue */\n#endif\nstruct sk_buff_head rx_queue; /* Received packets */\nstruct list_head conn_attend_q; /* Conns requiring immediate attention */\nstruct list_head call_attend_q; /* Calls requiring immediate attention */\nstruct rb_root client_bundles; /* Client connection bundles by socket params */\nspinlock_t client_bundles_lock; /* Lock for client_bundles */\nbool kill_all_client_conns;\nstruct list_head idle_client_conns;\nstruct timer_list client_conn_reap_timer;\nunsigned long client_conn_flags;\n#define RXRPC_CLIENT_CONN_REAP_TIMER 0 /* The client conn reap timer expired */\nspinlock_t lock; /* access lock */\nrwlock_t services_lock; /* lock for services list */\nint debug_id; /* debug ID for printks */\nbool dead;\nbool service_closed; /* Service socket closed */\nstruct idr conn_ids; /* List of connection IDs */\nstruct list_head new_client_calls; /* Newly created client calls need connection */\nspinlock_t client_call_lock; /* Lock for ->new_client_calls */\nstruct sockaddr_rxrpc srx; /* local address */\n};\n```\n```c\nstruct sockaddr_rxrpc {\n__kernel_sa_family_t srx_family; /* address family */\n__u16 srx_service; /* service desired */\n__u16 transport_type; /* type of transport socket (SOCK_DGRAM) */\n__u16 transport_len; /* length of transport address */\nunion {\n__kernel_sa_family_t family; /* transport address family */\nstruct sockaddr_in sin; /* IPv4 transport address */\nstruct sockaddr_in6 sin6; /* IPv6 transport address */\n} transport;\n};\n```\n```c\nstruct rxrpc_peer {\nstruct rcu_head rcu; /* This must be first */\nrefcount_t ref;\nunsigned long hash_key;\nstruct hlist_node hash_link;\nstruct rxrpc_local *local;\nstruct hlist_head error_targets; /* targets for net error distribution */\nstruct rb_root service_conns; /* Service connections */\nstruct list_head keepalive_link; /* Link in net->peer_keepalive[] */\ntime64_t last_tx_at; /* Last time packet sent here */\nseqlock_t service_conn_lock;\nspinlock_t lock; /* access lock */\nunsigned int if_mtu; /* interface MTU for this peer */\nunsigned int mtu; /* network MTU for this peer */\nunsigned int maxdata; /* data size (MTU - hdrsize) */\nunsigned short hdrsize; /* header size (IP + UDP + RxRPC) */\nint debug_id; /* debug ID for printks */\nstruct sockaddr_rxrpc srx; /* remote address */\n/* calculated RTT cache */\n#define RXRPC_RTT_CACHE_SIZE 32\nspinlock_t rtt_input_lock; /* RTT lock for input routine */\nktime_t rtt_last_req; /* Time of last RTT request */\nunsigned int rtt_count; /* Number of samples we've got */\nu32 srtt_us; /* smoothed round trip time << 3 in usecs */\nu32 mdev_us; /* medium deviation */\nu32 mdev_max_us; /* maximal mdev for the last rtt period */\nu32 rttvar_us; /* smoothed mdev_max */\nu32 rto_us; /* Retransmission timeout in usec */\nu8 backoff; /* Backoff timeout (as shift) */\nu8 cong_ssthresh; /* Congestion slow-start threshold */\n};\n```\n```c\nstruct rxrpc_net {\nstruct proc_dir_entry *proc_net; /* Subdir in /proc/net */\nu32 epoch; /* Local epoch for detecting local-end reset */\nstruct list_head calls; /* List of calls active in this namespace */\nspinlock_t call_lock; /* Lock for ->calls */\natomic_t nr_calls; /* Count of allocated calls */\natomic_t nr_conns;\nstruct list_head bundle_proc_list; /* List of bundles for proc */\nstruct list_head conn_proc_list; /* List of conns in this namespace for proc */\nstruct list_head service_conns; /* Service conns in this namespace */\nrwlock_t conn_lock; /* Lock for ->conn_proc_list, ->service_conns */\nstruct work_struct service_conn_reaper;\nstruct timer_list service_conn_reap_timer;\nbool live;\natomic_t nr_client_conns;\nstruct hlist_head local_endpoints;\nstruct mutex local_mutex; /* Lock for ->local_endpoints */\nDECLARE_HASHTABLE (peer_hash, 10);\nspinlock_t peer_hash_lock; /* Lock for ->peer_hash */\n#define RXRPC_KEEPALIVE_TIME 20 /* NAT keepalive time in seconds */\nu8 peer_keepalive_cursor;\ntime64_t peer_keepalive_base;\nstruct list_head peer_keepalive[32];\nstruct list_head peer_keepalive_new;\nstruct timer_list peer_keepalive_timer;\nstruct work_struct peer_keepalive_work;\natomic_t stat_tx_data;\natomic_t stat_tx_data_retrans;\natomic_t stat_tx_data_send;\natomic_t stat_tx_data_send_frag;\natomic_t stat_tx_data_send_fail;\natomic_t stat_tx_data_underflow;\natomic_t stat_tx_data_cwnd_reset;\natomic_t stat_rx_data;\natomic_t stat_rx_data_reqack;\natomic_t stat_rx_data_jumbo;\natomic_t stat_tx_ack_fill;\natomic_t stat_tx_ack_send;\natomic_t stat_tx_ack_skip;\natomic_t stat_tx_acks[256];\natomic_t stat_rx_acks[256];\natomic_t stat_why_req_ack[8];\natomic_t stat_io_loop;\n};\n```\n```c\nstatic unsigned long rxrpc_peer_hash_key(struct rxrpc_local *local, const struct sockaddr_rxrpc *srx)\n{\nconst u16 *p;\nunsigned int i, size;\nunsigned long hash_key;\n_enter(\"\");\nhash_key = (unsigned long)local / __alignof__(*local);\nhash_key += srx->transport_type;\nhash_key += srx->transport_len;\nhash_key += srx->transport.family;\nswitch (srx->transport.family) {\ncase AF_INET:\nhash_key += (u16 __force)srx->transport.sin.sin_port;\nsize = sizeof(srx->transport.sin.sin_addr);\np = (u16 *)&srx->transport.sin.sin_addr;\nbreak;\n#ifdef CONFIG_AF_RXRPC_IPV6\ncase AF_INET6:\nhash_key += (u16 __force)srx->transport.sin.sin_port;\nsize = sizeof(srx->transport.sin6.sin6_addr);\np = (u16 *)&srx->transport.sin6.sin6_addr;\nbreak;\n#endif\ndefault:\nWARN(1, \"AF_RXRPC: Unsupported transport address family\\n\");\nreturn 0;\n}\n/* Step through the peer address in 16-bit portions for speed */\nfor (i = 0; i < size; i += sizeof(*p), p++)\nhash_key += *p;\n_leave(\" 0x%lx\", hash_key);\nreturn hash_key;\n}\n```\n```c\nstatic __always_inline void rcu_read_lock(void)\n{\n__rcu_read_lock();\n__acquire(RCU);\nrcu_lock_acquire(&rcu_lock_map);\nRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\"rcu_read_lock() used illegally while idle\");\n}\n```\n```c\nstatic struct rxrpc_peer *__rxrpc_lookup_peer_rcu(struct rxrpc_local *local, const struct sockaddr_rxrpc *srx, unsigned long hash_key)\n{\nstruct rxrpc_peer *peer;\nstruct rxrpc_net *rxnet = local->rxnet;\nhash_for_each_possible_rcu(rxnet->peer_hash, peer, hash_link, hash_key) {\nif (rxrpc_peer_cmp_key(peer, local, srx, hash_key) == 0 &&\nrefcount_read(&peer->ref) > 0)\nreturn peer;\n}\nreturn NULL;\n}\n```\n```c\nstruct rxrpc_peer *rxrpc_get_peer_maybe(struct rxrpc_peer *peer, enum rxrpc_peer_trace why)\n{\nint r;\nif (peer) {\nif (__refcount_inc_not_zero(&peer->ref, &r))\ntrace_rxrpc_peer(peer->debug_id, r + 1, why);\nelse\npeer = NULL;\n}\nreturn peer;\n}\n```\n```c\nstatic inline void rcu_read_unlock(void)\n{\nRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\"rcu_read_unlock() used illegally while idle\");\nrcu_lock_release(&rcu_lock_map); /* Keep acq info for rls diags. */\n__release(RCU);\n__rcu_read_unlock();\n}\n```\n```c\nstatic struct rxrpc_peer *rxrpc_create_peer(struct rxrpc_local *local, struct sockaddr_rxrpc *srx, unsigned long hash_key, gfp_t gfp)\n{\nstruct rxrpc_peer *peer;\n_enter(\"\");\npeer = rxrpc_alloc_peer(local, gfp, rxrpc_peer_new_client);\nif (peer) {\nmemcpy(&peer->srx, srx, sizeof(*srx));\nrxrpc_init_peer(local, peer, hash_key);\n}\n_leave(\" = %p\", peer);\nreturn peer;\n}\n```\n```c\nstatic inline void spin_lock(spinlock_t *lock)\n{\nint ret = pthread_spin_lock(lock);\nassert(!ret);\n}\n```\n```c\n#define hash_add_rcu(hashtable, node, key) \\\nhlist_add_head_rcu(node, &hashtable[hash_min(key, HASH_BITS(hashtable))])\n```\n```c\nstatic inline void list_add_tail(struct list_head *new, struct list_head *head)\n{\n__list_add(new, head->prev, head);\n}\n```\n```c\nstatic inline void spin_unlock(spinlock_t *lock)\n{\nint ret = pthread_spin_unlock(lock);\nassert(!ret);\n}\n```\n```c\nstatic void rxrpc_free_peer(struct rxrpc_peer *peer)\n{\ntrace_rxrpc_peer(peer->debug_id, 0, rxrpc_peer_free);\nrxrpc_put_local(peer->local, rxrpc_local_put_peer);\nkfree_rcu(peer, rcu);\n}\n```\n```c\nstatic inline unsigned int refcount_read(const refcount_t *r)\n{\nreturn atomic_read(&r->refs);\n}\n```",
  "original_code": "```c\nstruct rxrpc_peer *rxrpc_lookup_peer(struct rxrpc_local *local, struct sockaddr_rxrpc *srx, gfp_t gfp)\n{\nstruct rxrpc_peer *peer, *candidate;\nstruct rxrpc_net *rxnet = local->rxnet;\nunsigned long hash_key = rxrpc_peer_hash_key(local, srx);\n_enter(\"{%pISp}\", &srx->transport);\n/* search the peer list first */\nrcu_read_lock();\npeer = __rxrpc_lookup_peer_rcu(local, srx, hash_key);\nif (peer && !rxrpc_get_peer_maybe(peer, rxrpc_peer_get_lookup_client))\npeer = NULL;\nrcu_read_unlock();\nif (!peer) {\n/* The peer is not yet present in hash - create a candidate\n* for a new record and then redo the search.\n*/\ncandidate = rxrpc_create_peer(local, srx, hash_key, gfp);\nif (!candidate) {\n_leave(\" = NULL [nomem]\");\nreturn NULL;\n}\nspin_lock(&rxnet->peer_hash_lock);\n/* Need to check that we aren't racing with someone else */\npeer = __rxrpc_lookup_peer_rcu(local, srx, hash_key);\nif (peer && !rxrpc_get_peer_maybe(peer, rxrpc_peer_get_lookup_client))\npeer = NULL;\nif (!peer) {\nhash_add_rcu(rxnet->peer_hash,\n&candidate->hash_link, hash_key);\nlist_add_tail(&candidate->keepalive_link,\n&rxnet->peer_keepalive_new);\n}\nspin_unlock(&rxnet->peer_hash_lock);\nif (peer)\nrxrpc_free_peer(candidate);\nelse\npeer = candidate;\n}\n_leave(\" = %p {u=%d}\", peer, refcount_read(&peer->ref));\nreturn peer;\n}\n```",
  "vuln_patch": "```c\nstruct rxrpc_peer *rxrpc_lookup_peer(struct rxrpc_local *local,\nstruct sockaddr_rxrpc *srx, gfp_t gfp)\n{\nstruct rxrpc_peer *peer, *candidate;\nstruct rxrpc_net *rxnet = local->rxnet;\nunsigned long hash_key = rxrpc_peer_hash_key(local, srx);\n_enter(\"{%pISp}\", &srx->transport);\n/* search the peer list first */\nrcu_read_lock();\npeer = __rxrpc_lookup_peer_rcu(local, srx, hash_key);\nif (peer && !rxrpc_get_peer_maybe(peer, rxrpc_peer_get_lookup_client))\npeer = NULL;\nrcu_read_unlock();\nif (!peer) {\n/* The peer is not yet present in hash - create a candidate\n* for a new record and then redo the search.\n*/\ncandidate = rxrpc_create_peer(local, srx, hash_key, gfp);\nif (!candidate) {\n_leave(\" = NULL [nomem]\");\nreturn NULL;\n}\nspin_lock_bh(&rxnet->peer_hash_lock);\n/* Need to check that we aren't racing with someone else */\npeer = __rxrpc_lookup_peer_rcu(local, srx, hash_key);\nif (peer && !rxrpc_get_peer_maybe(peer, rxrpc_peer_get_lookup_client))\npeer = NULL;\nif (!peer) {\nhash_add_rcu(rxnet->peer_hash,\n&candidate->hash_link, hash_key);\nlist_add_tail(&candidate->keepalive_link,\n&rxnet->peer_keepalive_new);\n}\nspin_unlock_bh(&rxnet->peer_hash_lock);\nif (peer)\nrxrpc_free_peer(candidate);\nelse\npeer = candidate;\n}\n_leave(\" = %p {u=%d}\", peer, refcount_read(&peer->ref));\nreturn peer;\n}\n```",
  "function_name": "rxrpc_lookup_peer",
  "function_prototype": "struct rxrpc_peer *rxrpc_lookup_peer(struct rxrpc_local *local, struct sockaddr_rxrpc *srx, gfp_t gfp)",
  "code_semantics": "The function is responsible for looking up a network peer in a hash table using a unique key derived from the local network endpoint and the peer's address. It first attempts to find an existing peer and checks if it can be safely used by incrementing its reference count. If no peer is found, it creates a new peer object and adds it to the hash table, ensuring thread safety with locks. The function manages the reference count of the peer to prevent premature deletion.",
  "safe_verification_cot": "1. The Target Code uses spin_lock_bh to lock rxnet->peer_hash_lock, which disables bottom halves, preventing race conditions with interrupt handlers or bottom halves. 2. The spin_unlock_bh is used to unlock rxnet->peer_hash_lock, which re-enables bottom halves safely. 3. The function __rxrpc_lookup_peer_rcu safely handles concurrent access due to the proper locking mechanism. 4. The creation of peers using rxrpc_create_peer is protected from race conditions by disabling bottom halves. 5. The operations hash_add_rcu and list_add_tail are performed within a critical section where bottom halves are disabled, ensuring data integrity.",
  "verification_cot": "1. The Vulnerable Code uses spin_lock to lock rxnet->peer_hash_lock, which does not disable bottom halves. This can lead to race conditions if an interrupt handler or bottom half accesses rxnet->peer_hash concurrently. 2. The spin_unlock is used to unlock rxnet->peer_hash_lock, which does not re-enable bottom halves, leaving the system vulnerable to race conditions. 3. The function __rxrpc_lookup_peer_rcu may not safely handle concurrent access due to the improper locking mechanism. 4. The creation of peers using rxrpc_create_peer can race with other operations due to the lack of bottom half disabling. 5. The operations hash_add_rcu and list_add_tail are performed without ensuring that bottom halves are disabled, leading to potential data corruption.",
  "vulnerability_related_variables": {
    "rxnet->peer_hash_lock": "A synchronization mechanism used to ensure exclusive access to shared resources, preventing concurrent modifications and ensuring data consistency.",
    "rxnet->peer_hash": "A data structure designed to store and manage connections efficiently, allowing for quick retrieval and insertion operations based on a hash function.",
    "candidate->hash_link": "A linkage element that connects an item to a hash-based data structure, enabling its inclusion and management within the structure.",
    "rxnet->peer_keepalive_new": "A list structure used to track and manage newly created connections that require periodic keepalive operations to maintain their active status.",
    "candidate->keepalive_link": "A linkage element that connects an item to a list-based data structure, enabling its inclusion and management within the structure for keepalive purposes."
  },
  "vulnerability_related_functions": {
    "spin_lock": "Acquires a lock on a synchronization primitive to ensure exclusive access to a shared resource.",
    "spin_unlock": "Releases a previously acquired lock on a synchronization primitive, allowing other threads to access the shared resource.",
    "__rxrpc_lookup_peer_rcu": "Searches for an entry in a concurrent hash table using a key. It iterates over potential matches and checks for a specific condition to find the correct entry.",
    "rxrpc_create_peer": "Allocates and initializes a new data structure using provided parameters. It sets up the structure with initial values and returns it for further use.",
    "hash_add_rcu": "Adds an entry to a concurrent hash table, ensuring that the operation is safe in a multi-threaded environment.",
    "list_add_tail": "Appends a new entry to the end of a linked list, maintaining the order of elements."
  },
  "root_cause": "Improper locking mechanism using spin_lock instead of spin_lock_bh, leading to race conditions with bottom halves.",
  "patch_cot": "First, identify the sections of the code where spin_lock and spin_unlock are used to protect critical sections involving rxnet->peer_hash_lock. Replace spin_lock(&rxnet->peer_hash_lock) with spin_lock_bh(&rxnet->peer_hash_lock) to ensure that bottom halves are disabled during the lock. Replace spin_unlock(&rxnet->peer_hash_lock) with spin_unlock_bh(&rxnet->peer_hash_lock) to re-enable bottom halves after the lock is released. Ensure that all accesses to rxnet->peer_hash, candidate->hash_link, and rxnet->peer_keepalive_new are within the critical section protected by spin_lock_bh and spin_unlock_bh."
}