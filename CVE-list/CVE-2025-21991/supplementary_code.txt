```c
struct cpuinfo_x86 {
    union {
        /*
         * The particular ordering (low-to-high) of (vendor,
         * family, model) is done in case range of models, like
         * it is usually done on AMD, need to be compared.
         */
        struct {
            __u8    x86_model;
            /* CPU family */
            __u8    x86;
            /* CPU vendor */
            __u8    x86_vendor;
            __u8    x86_reserved;
        };
        /* combined vendor, family, model */
        __u32       x86_vfm;
    };
    __u8            x86_stepping;
#ifdef CONFIG_X86_64
    /* Number of 4K pages in DTLB/ITLB combined(in pages): */
    int         x86_tlbsize;
#endif
#ifdef CONFIG_X86_VMX_FEATURE_NAMES
    __u32           vmx_capability[NVMXINTS];
#endif
    __u8            x86_virt_bits;
    __u8            x86_phys_bits;
    /* Max extended CPUID function supported: */
    __u32           extended_cpuid_level;
    /* Maximum supported CPUID level, -1=no CPUID: */
    int         cpuid_level;
    /*
     * Align to size of unsigned long because the x86_capability array
     * is passed to bitops which require the alignment. Use unnamed
     * union to enforce the array is aligned to size of unsigned long.
     */
    union {
        __u32       x86_capability[NCAPINTS + NBUGINTS];
        unsigned long   x86_capability_alignment;
    };
    char            x86_vendor_id[16];
    char            x86_model_id[64];
    struct cpuinfo_topology topo;
    /* in KB - valid for CPUS which support this call: */
    unsigned int        x86_cache_size;
    int         x86_cache_alignment;    /* In bytes */
    /* Cache QoS architectural values, valid only on the BSP: */
    int         x86_cache_max_rmid; /* max index */
    int         x86_cache_occ_scale;    /* scale to bytes */
    int         x86_cache_mbm_width_offset;
    int         x86_power;
    unsigned long       loops_per_jiffy;
    /* protected processor identification number */
    u64         ppin;
    u16         x86_clflush_size;
    /* number of cores as seen by the OS: */
    u16         booted_cores;
    /* Index into per_cpu list: */
    u16         cpu_index;
    /*  Is SMT active on this core? */
    bool            smt_active;
    u32         microcode;
    /* Address space bits used by the cache internally */
    u8          x86_cache_bits;
    unsigned        initialized : 1;
} __randomize_layout;
```

```c
struct ucode_patch {
    struct list_head plist;
    void *data;
    unsigned int size;
    u32 patch_id;
    u16 equiv_cpu;
};
```

```c
enum ucode_state {
    UCODE_OK    = 0,
    UCODE_NEW,
    UCODE_NEW_SAFE,
    UCODE_UPDATED,
    UCODE_NFOUND,
    UCODE_ERROR,
    UCODE_TIMEOUT,
    UCODE_OFFLINE,
};
```

```c
static enum ucode_state _load_microcode_amd(u8 family, const u8 *data, size_t size)
{
    enum ucode_state ret;

    /* free old equiv table */
    free_equiv_cpu_table();

    ret = __load_microcode_amd(family, data, size);
    if (ret != UCODE_OK)
        cleanup();

    return ret;
}
```

```c
static __always_inline unsigned int cpumask_first(const struct cpumask *srcp)
{
    return find_first_bit(cpumask_bits(srcp), small_cpumask_bits);
}
```

```c
#define cpu_data(cpu)       per_cpu(cpu_info, cpu)
```

```c
static struct ucode_patch *find_patch(unsigned int cpu)
{
    struct ucode_cpu_info *uci = ucode_cpu_info + cpu;
    u32 rev, dummy __always_unused;
    u16 equiv_id = 0;

    /* fetch rev if not populated yet: */
    if (!uci->cpu_sig.rev) {
        rdmsr(MSR_AMD64_PATCH_LEVEL, rev, dummy);
        uci->cpu_sig.rev = rev;
    }

    if (x86_family(bsp_cpuid_1_eax) < 0x17) {
        equiv_id = find_equiv_id(&equiv_table, uci->cpu_sig.sig);
        if (!equiv_id)
            return NULL;
    }

    return cache_find_patch(uci, equiv_id);
}
```
