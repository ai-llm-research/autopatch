{
 "supplementary_code": "```c\nstruct cpuinfo_x86 {\nunion {\n/*\n* The particular ordering (low-to-high) of (vendor,\n* family, model) is done in case range of models, like\n* it is usually done on AMD, need to be compared.\n*/\nstruct {\n__u8 x86_model;\n/* CPU family */\n__u8 x86;\n/* CPU vendor */\n__u8 x86_vendor;\n__u8 x86_reserved;\n};\n/* combined vendor, family, model */\n__u32 x86_vfm;\n};\n__u8 x86_stepping;\n#ifdef CONFIG_X86_64\n/* Number of 4K pages in DTLB/ITLB combined(in pages): */\nint x86_tlbsize;\n#endif\n#ifdef CONFIG_X86_VMX_FEATURE_NAMES\n__u32 vmx_capability[NVMXINTS];\n#endif\n__u8 x86_virt_bits;\n__u8 x86_phys_bits;\n/* Max extended CPUID function supported: */\n__u32 extended_cpuid_level;\n/* Maximum supported CPUID level, -1=no CPUID: */\nint cpuid_level;\n/*\n* Align to size of unsigned long because the x86_capability array\n* is passed to bitops which require the alignment. Use unnamed\n* union to enforce the array is aligned to size of unsigned long.\n*/\nunion {\n__u32 x86_capability[NCAPINTS + NBUGINTS];\nunsigned long x86_capability_alignment;\n};\nchar x86_vendor_id[16];\nchar x86_model_id[64];\nstruct cpuinfo_topology topo;\n/* in KB - valid for CPUS which support this call: */\nunsigned int x86_cache_size;\nint x86_cache_alignment; /* In bytes */\n/* Cache QoS architectural values, valid only on the BSP: */\nint x86_cache_max_rmid; /* max index */\nint x86_cache_occ_scale; /* scale to bytes */\nint x86_cache_mbm_width_offset;\nint x86_power;\nunsigned long loops_per_jiffy;\n/* protected processor identification number */\nu64 ppin;\nu16 x86_clflush_size;\n/* number of cores as seen by the OS: */\nu16 booted_cores;\n/* Index into per_cpu list: */\nu16 cpu_index;\n/* Is SMT active on this core? */\nbool smt_active;\nu32 microcode;\n/* Address space bits used by the cache internally */\nu8 x86_cache_bits;\nunsigned initialized : 1;\n} __randomize_layout;\n```\n```c\nstruct ucode_patch {\nstruct list_head plist;\nvoid *data;\nunsigned int size;\nu32 patch_id;\nu16 equiv_cpu;\n};\n```\n```c\nenum ucode_state {\nUCODE_OK = 0,\nUCODE_NEW,\nUCODE_NEW_SAFE,\nUCODE_UPDATED,\nUCODE_NFOUND,\nUCODE_ERROR,\nUCODE_TIMEOUT,\nUCODE_OFFLINE,\n};\n```\n```c\nstatic enum ucode_state _load_microcode_amd(u8 family, const u8 *data, size_t size)\n{\nenum ucode_state ret;\n/* free old equiv table */\nfree_equiv_cpu_table();\nret = __load_microcode_amd(family, data, size);\nif (ret != UCODE_OK)\ncleanup();\nreturn ret;\n}\n```\n```c\nstatic __always_inline unsigned int cpumask_first(const struct cpumask *srcp)\n{\nreturn find_first_bit(cpumask_bits(srcp), small_cpumask_bits);\n}\n```\n```c\n#define cpu_data(cpu) per_cpu(cpu_info, cpu)\n```\n```c\nstatic struct ucode_patch *find_patch(unsigned int cpu)\n{\nstruct ucode_cpu_info *uci = ucode_cpu_info + cpu;\nu32 rev, dummy __always_unused;\nu16 equiv_id = 0;\n/* fetch rev if not populated yet: */\nif (!uci->cpu_sig.rev) {\nrdmsr(MSR_AMD64_PATCH_LEVEL, rev, dummy);\nuci->cpu_sig.rev = rev;\n}\nif (x86_family(bsp_cpuid_1_eax) < 0x17) {\nequiv_id = find_equiv_id(&equiv_table, uci->cpu_sig.sig);\nif (!equiv_id)\nreturn NULL;\n}\nreturn cache_find_patch(uci, equiv_id);\n}\n```\n",
 "function_prototype": "static enum ucode_state load_microcode_amd(u8 family, const u8 *data, size_t size)",
 "re_implemented_code": "\n```c\nstatic enum ucode_state load_microcode_amd(u8 family, const u8 *data, size_t size)\n{\nstatic struct cpuinfo_x86 boot_cpu_data;\nstruct cpuinfo_x86 *boot_cpu;\nunsigned int nid, cpu;\nstruct ucode_patch *patch;\nenum ucode_state ret;\nboot_cpu = &boot_cpu_data;\nget_cpu_details(boot_cpu);\nret = _load_microcode_amd(family, data, size);\nif (ret != UCODE_OK)\ngoto out;\nfor_each_online_node(nid) {\nfor_each_cpu(cpu, cpumask_of_node(nid)) {\nif ((!is_uv_system() && !cpu_online(cpu)))\ncontinue;\npatch = find_patch(cpu);\nif (!patch || patch->version >= boot_cpu->microcode)\ncontinue;\nret = UCODE_NEW;\nbreak;\n}\nif (ret > UCODE_NEW)\nbreak;\n}\nout:\nreturn ret;\n}\n```\n",
 "is_vulnerable": false
}