```c
struct device {
    struct kobject kobj;
    struct device       *parent;

    struct device_private   *p;

    const char      *init_name; /* initial name of the device */
    const struct device_type *type;

    const struct bus_type   *bus;   /* type of bus device is on */
    struct device_driver *driver;   /* which driver has allocated this
                       device */
    void        *platform_data; /* Platform specific data, device
                       core doesn't touch it */
    void        *driver_data;   /* Driver data, set and get with
                       dev_set_drvdata/dev_get_drvdata */
    struct mutex        mutex;  /* mutex to synchronize calls to
                     * its driver.
                     */

    struct dev_links_info   links;
    struct dev_pm_info  power;
    struct dev_pm_domain    *pm_domain;

#ifdef CONFIG_ENERGY_MODEL
    struct em_perf_domain   *em_pd;
#endif

#ifdef CONFIG_PINCTRL
    struct dev_pin_info *pins;
#endif
    struct dev_msi_info msi;
#ifdef CONFIG_ARCH_HAS_DMA_OPS
    const struct dma_map_ops *dma_ops;
#endif
    u64     *dma_mask;  /* dma mask (if dma'able device) */
    u64     coherent_dma_mask;/* Like dma_mask, but for
                         alloc_coherent mappings as
                         not all hardware supports
                         64 bit addresses for consistent
                         allocations such descriptors. */
    u64     bus_dma_limit;  /* upstream dma constraint */
    const struct bus_dma_region *dma_range_map;

    struct device_dma_parameters *dma_parms;

    struct list_head    dma_pools;  /* dma pools (if dma'ble) */

#ifdef CONFIG_DMA_DECLARE_COHERENT
    struct dma_coherent_mem *dma_mem; /* internal for coherent mem
                         override */
#endif
#ifdef CONFIG_DMA_CMA
    struct cma *cma_area;       /* contiguous memory area for dma
                       allocations */
#endif
#ifdef CONFIG_SWIOTLB
    struct io_tlb_mem *dma_io_tlb_mem;
#endif
#ifdef CONFIG_SWIOTLB_DYNAMIC
    struct list_head dma_io_tlb_pools;
    spinlock_t dma_io_tlb_lock;
    bool dma_uses_io_tlb;
#endif
    /* arch specific additions */
    struct dev_archdata archdata;

    struct device_node  *of_node; /* associated device tree node */
    struct fwnode_handle    *fwnode; /* firmware device node */

#ifdef CONFIG_NUMA
    int     numa_node;  /* NUMA node this device is close to */
#endif
    dev_t           devt;   /* dev_t, creates the sysfs "dev" */
    u32         id; /* device instance */

    spinlock_t      devres_lock;
    struct list_head    devres_head;

    const struct class  *class;
    const struct attribute_group **groups;  /* optional groups */

    void    (*release)(struct device *dev);
    struct iommu_group  *iommu_group;
    struct dev_iommu    *iommu;

    struct device_physical_location *physical_location;

    enum device_removable   removable;

    bool            offline_disabled:1;
    bool            offline:1;
    bool            of_node_reused:1;
    bool            state_synced:1;
    bool            can_match:1;
#if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \
    defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \
    defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL)
    bool            dma_coherent:1;
#endif
#ifdef CONFIG_DMA_OPS_BYPASS
    bool            dma_ops_bypass : 1;
#endif
#ifdef CONFIG_DMA_NEED_SYNC
    bool            dma_skip_sync:1;
#endif
#ifdef CONFIG_IOMMU_DMA
    bool            dma_iommu:1;
#endif
};
```

```c
struct net_device {
    /* Cacheline organization can be found documented in
     * Documentation/networking/net_cachelines/net_device.rst.
     * Please update the document when adding new fields.
     */

    /* TX read-mostly hotpath */
    __cacheline_group_begin(net_device_read_tx);
    struct_group(priv_flags_fast,
        unsigned long       priv_flags:32;
        unsigned long       lltx:1;
    );
    const struct net_device_ops *netdev_ops;
    const struct header_ops *header_ops;
    struct netdev_queue *_tx;
    netdev_features_t   gso_partial_features;
    unsigned int        real_num_tx_queues;
    unsigned int        gso_max_size;
    unsigned int        gso_ipv4_max_size;
    u16         gso_max_segs;
    s16         num_tc;
    /* Note : dev->mtu is often read without holding a lock.
     * Writers usually hold RTNL.
     * It is recommended to use READ_ONCE() to annotate the reads,
     * and to use WRITE_ONCE() to annotate the writes.
     */
    unsigned int        mtu;
    unsigned short      needed_headroom;
    struct netdev_tc_txq    tc_to_txq[TC_MAX_QUEUE];
#ifdef CONFIG_XPS
    struct xps_dev_maps __rcu *xps_maps[XPS_MAPS_MAX];
#endif
#ifdef CONFIG_NETFILTER_EGRESS
    struct nf_hook_entries __rcu *nf_hooks_egress;
#endif
#ifdef CONFIG_NET_XGRESS
    struct bpf_mprog_entry __rcu *tcx_egress;
#endif
    __cacheline_group_end(net_device_read_tx);

    /* TXRX read-mostly hotpath */
    __cacheline_group_begin(net_device_read_txrx);
    union {
        struct pcpu_lstats __percpu     *lstats;
        struct pcpu_sw_netstats __percpu    *tstats;
        struct pcpu_dstats __percpu     *dstats;
    };
    unsigned long       state;
    unsigned int        flags;
    unsigned short      hard_header_len;
    netdev_features_t   features;
    struct inet6_dev __rcu  *ip6_ptr;
    __cacheline_group_end(net_device_read_txrx);

    /* RX read-mostly hotpath */
    __cacheline_group_begin(net_device_read_rx);
    struct bpf_prog __rcu   *xdp_prog;
    struct list_head    ptype_specific;
    int         ifindex;
    unsigned int        real_num_rx_queues;
    struct netdev_rx_queue  *_rx;
    unsigned int        gro_max_size;
    unsigned int        gro_ipv4_max_size;
    rx_handler_func_t __rcu *rx_handler;
    void __rcu      *rx_handler_data;
    possible_net_t          nd_net;
#ifdef CONFIG_NETPOLL
    struct netpoll_info __rcu   *npinfo;
#endif
#ifdef CONFIG_NET_XGRESS
    struct bpf_mprog_entry __rcu *tcx_ingress;
#endif
    __cacheline_group_end(net_device_read_rx);

    char            name[IFNAMSIZ];
    struct netdev_name_node *name_node;
    struct dev_ifalias  __rcu *ifalias;
    /*
     *  I/O specific fields
     *  FIXME: Merge these and struct ifmap into one
     */
    unsigned long       mem_end;
    unsigned long       mem_start;
    unsigned long       base_addr;

    /*
     *  Some hardware also needs these fields (state,dev_list,
     *  napi_list,unreg_list,close_list) but they are not
     *  part of the usual set specified in Space.c.
     */


    struct list_head    dev_list;
    struct list_head    napi_list;
    struct list_head    unreg_list;
    struct list_head    close_list;
    struct list_head    ptype_all;

    struct {
        struct list_head upper;
        struct list_head lower;
    } adj_list;

    /* Read-mostly cache-line for fast-path access */
    xdp_features_t      xdp_features;
    const struct xdp_metadata_ops *xdp_metadata_ops;
    const struct xsk_tx_metadata_ops *xsk_tx_metadata_ops;
    unsigned short      gflags;

    unsigned short      needed_tailroom;

    netdev_features_t   hw_features;
    netdev_features_t   wanted_features;
    netdev_features_t   vlan_features;
    netdev_features_t   hw_enc_features;
    netdev_features_t   mpls_features;

    unsigned int        min_mtu;
    unsigned int        max_mtu;
    unsigned short      type;
    unsigned char       min_header_len;
    unsigned char       name_assign_type;

    int         group;

    struct net_device_stats stats; /* not used by modern drivers */

    struct net_device_core_stats __percpu *core_stats;

    /* Stats to monitor link on/off, flapping */
    atomic_t        carrier_up_count;
    atomic_t        carrier_down_count;

#ifdef CONFIG_WIRELESS_EXT
    const struct iw_handler_def *wireless_handlers;
#endif
    const struct ethtool_ops *ethtool_ops;
#ifdef CONFIG_NET_L3_MASTER_DEV
    const struct l3mdev_ops *l3mdev_ops;
#endif
#if IS_ENABLED(CONFIG_IPV6)
    const struct ndisc_ops *ndisc_ops;
#endif

#ifdef CONFIG_XFRM_OFFLOAD
    const struct xfrmdev_ops *xfrmdev_ops;
#endif

#if IS_ENABLED(CONFIG_TLS_DEVICE)
    const struct tlsdev_ops *tlsdev_ops;
#endif

    unsigned int        operstate;
    unsigned char       link_mode;

    unsigned char       if_port;
    unsigned char       dma;

    /* Interface address info. */
    unsigned char       perm_addr[MAX_ADDR_LEN];
    unsigned char       addr_assign_type;
    unsigned char       addr_len;
    unsigned char       upper_level;
    unsigned char       lower_level;

    unsigned short      neigh_priv_len;
    unsigned short          dev_id;
    unsigned short          dev_port;
    int         irq;
    u32         priv_len;

    spinlock_t      addr_list_lock;

    struct netdev_hw_addr_list  uc;
    struct netdev_hw_addr_list  mc;
    struct netdev_hw_addr_list  dev_addrs;

#ifdef CONFIG_SYSFS
    struct kset     *queues_kset;
#endif
#ifdef CONFIG_LOCKDEP
    struct list_head    unlink_list;
#endif
    unsigned int        promiscuity;
    unsigned int        allmulti;
    bool            uc_promisc;
#ifdef CONFIG_LOCKDEP
    unsigned char       nested_level;
#endif


    /* Protocol-specific pointers */
    struct in_device __rcu  *ip_ptr;
    /** @fib_nh_head: nexthops associated with this netdev */
    struct hlist_head   fib_nh_head;

#if IS_ENABLED(CONFIG_VLAN_8021Q)
    struct vlan_info __rcu  *vlan_info;
#endif
#if IS_ENABLED(CONFIG_NET_DSA)
    struct dsa_port     *dsa_ptr;
#endif
#if IS_ENABLED(CONFIG_TIPC)
    struct tipc_bearer __rcu *tipc_ptr;
#endif
#if IS_ENABLED(CONFIG_ATALK)
    void            *atalk_ptr;
#endif
#if IS_ENABLED(CONFIG_AX25)
    void            *ax25_ptr;
#endif
#if IS_ENABLED(CONFIG_CFG80211)
    struct wireless_dev *ieee80211_ptr;
#endif
#if IS_ENABLED(CONFIG_IEEE802154) || IS_ENABLED(CONFIG_6LOWPAN)
    struct wpan_dev     *ieee802154_ptr;
#endif
#if IS_ENABLED(CONFIG_MPLS_ROUTING)
    struct mpls_dev __rcu   *mpls_ptr;
#endif
#if IS_ENABLED(CONFIG_MCTP)
    struct mctp_dev __rcu   *mctp_ptr;
#endif

/*
 * Cache lines mostly used on receive path (including eth_type_trans())
 */
    /* Interface address info used in eth_type_trans() */
    const unsigned char *dev_addr;

    unsigned int        num_rx_queues;
#define GRO_LEGACY_MAX_SIZE 65536u
/* TCP minimal MSS is 8 (TCP_MIN_GSO_SIZE),
 * and shinfo->gso_segs is a 16bit field.
 */
#define GRO_MAX_SIZE        (8 * 65535u)
    unsigned int        xdp_zc_max_segs;
    struct netdev_queue __rcu *ingress_queue;
#ifdef CONFIG_NETFILTER_INGRESS
    struct nf_hook_entries __rcu *nf_hooks_ingress;
#endif

    unsigned char       broadcast[MAX_ADDR_LEN];
#ifdef CONFIG_RFS_ACCEL
    struct cpu_rmap     *rx_cpu_rmap;
#endif
    struct hlist_node   index_hlist;

/*
 * Cache lines mostly used on transmit path
 */
    unsigned int        num_tx_queues;
    struct Qdisc __rcu  *qdisc;
    unsigned int        tx_queue_len;
    spinlock_t      tx_global_lock;

    struct xdp_dev_bulk_queue __percpu *xdp_bulkq;

#ifdef CONFIG_NET_SCHED
    DECLARE_HASHTABLE   (qdisc_hash, 4);
#endif
    /* These may be needed for future network-power-down code. */
    struct timer_list   watchdog_timer;
    int         watchdog_timeo;

    u32                     proto_down_reason;

    struct list_head    todo_list;

#ifdef CONFIG_PCPU_DEV_REFCNT
    int __percpu        *pcpu_refcnt;
#else
    refcount_t      dev_refcnt;
#endif
    struct ref_tracker_dir  refcnt_tracker;

    struct list_head    link_watch_list;

    u8 reg_state;

    bool dismantle;

    enum {
        RTNL_LINK_INITIALIZED,
        RTNL_LINK_INITIALIZING,
    } rtnl_link_state:16;

    bool needs_free_netdev;
    void (*priv_destructor)(struct net_device *dev);

    /* mid-layer private */
    void                *ml_priv;
    enum netdev_ml_priv_type    ml_priv_type;

    enum netdev_stat_type       pcpu_stat_type:8;

#if IS_ENABLED(CONFIG_GARP)
    struct garp_port __rcu  *garp_port;
#endif
#if IS_ENABLED(CONFIG_MRP)
    struct mrp_port __rcu   *mrp_port;
#endif
#if IS_ENABLED(CONFIG_NET_DROP_MONITOR)
    struct dm_hw_stat_delta __rcu *dm_private;
#endif
    struct device       dev;
    const struct attribute_group *sysfs_groups[4];
    const struct attribute_group *sysfs_rx_queue_group;

    const struct rtnl_link_ops *rtnl_link_ops;

    const struct netdev_stat_ops *stat_ops;

    const struct netdev_queue_mgmt_ops *queue_mgmt_ops;

    /* for setting kernel sock attribute on TCP connection setup */
#define GSO_MAX_SEGS        65535u
#define GSO_LEGACY_MAX_SIZE 65536u
/* TCP minimal MSS is 8 (TCP_MIN_GSO_SIZE),
 * and shinfo->gso_segs is a 16bit field.
 */
#define GSO_MAX_SIZE        (8 * GSO_MAX_SEGS)

#define TSO_LEGACY_MAX_SIZE 65536
#define TSO_MAX_SIZE        UINT_MAX
    unsigned int        tso_max_size;
#define TSO_MAX_SEGS        U16_MAX
    u16         tso_max_segs;

#ifdef CONFIG_DCB
    const struct dcbnl_rtnl_ops *dcbnl_ops;
#endif
    u8          prio_tc_map[TC_BITMASK + 1];

#if IS_ENABLED(CONFIG_FCOE)
    unsigned int        fcoe_ddp_xid;
#endif
#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)
    struct netprio_map __rcu *priomap;
#endif
    struct phy_link_topology    *link_topo;
    struct phy_device   *phydev;
    struct sfp_bus      *sfp_bus;
    struct lock_class_key   *qdisc_tx_busylock;
    bool            proto_down;
    bool            threaded;

    /* priv_flags_slow, ungrouped to save space */
    unsigned long       see_all_hwtstamp_requests:1;
    unsigned long       change_proto_down:1;
    unsigned long       netns_local:1;
    unsigned long       fcoe_mtu:1;

    struct list_head    net_notifier_list;

#if IS_ENABLED(CONFIG_MACSEC)
    /* MACsec management functions */
    const struct macsec_ops *macsec_ops;
#endif
    const struct udp_tunnel_nic_info    *udp_tunnel_nic_info;
    struct udp_tunnel_nic   *udp_tunnel_nic;

    struct ethtool_netdev_state *ethtool;

    /* protected by rtnl_lock */
    struct bpf_xdp_entity   xdp_state[__MAX_XDP_MODE];

    u8 dev_addr_shadow[MAX_ADDR_LEN];
    netdevice_tracker   linkwatch_dev_tracker;
    netdevice_tracker   watchdog_dev_tracker;
    netdevice_tracker   dev_registered_tracker;
    struct rtnl_hw_stats64  *offload_xstats_l3;

    struct devlink_port *devlink_port;

#if IS_ENABLED(CONFIG_DPLL)
    struct dpll_pin __rcu   *dpll_pin;
#endif
#if IS_ENABLED(CONFIG_PAGE_POOL)
    /** @page_pools: page pools created for this netdevice */
    struct hlist_head   page_pools;
#endif

    /** @irq_moder: dim parameters used if IS_ENABLED(CONFIG_DIMLIB). */
    struct dim_irq_moder    *irq_moder;

    u64         max_pacing_offload_horizon;
    struct napi_config  *napi_config;
    unsigned long       gro_flush_timeout;
    u32         napi_defer_hard_irqs;

    /**
     * @lock: protects @net_shaper_hierarchy, feel free to use for other
     * netdev-scope protection. Ordering: take after rtnl_lock.
     */
    struct mutex        lock;

#if IS_ENABLED(CONFIG_NET_SHAPER)
    /**
     * @net_shaper_hierarchy: data tracking the current shaper status
     *  see include/net/net_shapers.h
     */
    struct net_shaper_hierarchy *net_shaper_hierarchy;
#endif

    struct hlist_head neighbours[NEIGH_NR_TABLES];

    u8          priv[] ____cacheline_aligned
                       __counted_by(priv_len);
} ____cacheline_aligned;
#define to_net_dev(d) container_of(d, struct net_device, dev)
```

```c
static inline void *dev_get_drvdata(const struct device *dev)
{
    return dev->driver_data;
}
```

```c
struct ravb_private {
    struct net_device *ndev;
    struct platform_device *pdev;
    void __iomem *addr;
    struct clk *clk;
    struct clk *refclk;
    struct clk *gptp_clk;
    struct mdiobb_ctrl mdiobb;
    u32 num_rx_ring[NUM_RX_QUEUE];
    u32 num_tx_ring[NUM_TX_QUEUE];
    u32 desc_bat_size;
    dma_addr_t desc_bat_dma;
    struct ravb_desc *desc_bat;
    dma_addr_t rx_desc_dma[NUM_RX_QUEUE];
    dma_addr_t tx_desc_dma[NUM_TX_QUEUE];
    union {
        struct ravb_rx_desc *desc;
        struct ravb_ex_rx_desc *ex_desc;
        void *raw;
    } rx_ring[NUM_RX_QUEUE];
    struct ravb_tx_desc *tx_ring[NUM_TX_QUEUE];
    void *tx_align[NUM_TX_QUEUE];
    struct sk_buff *rx_1st_skb;
    struct page_pool *rx_pool[NUM_RX_QUEUE];
    struct ravb_rx_buffer *rx_buffers[NUM_RX_QUEUE];
    struct sk_buff **tx_skb[NUM_TX_QUEUE];
    u32 rx_over_errors;
    u32 rx_fifo_errors;
    struct net_device_stats stats[NUM_RX_QUEUE];
    u32 tstamp_tx_ctrl;
    u32 tstamp_rx_ctrl;
    struct list_head ts_skb_list;
    u32 ts_skb_tag;
    struct ravb_ptp ptp;
    spinlock_t lock;        /* Register access lock */
    u32 cur_rx[NUM_RX_QUEUE];   /* Consumer ring indices */
    u32 dirty_rx[NUM_RX_QUEUE]; /* Producer ring indices */
    u32 cur_tx[NUM_TX_QUEUE];
    u32 dirty_tx[NUM_TX_QUEUE];
    struct napi_struct napi[NUM_RX_QUEUE];
    struct work_struct work;
    /* MII transceiver section. */
    struct mii_bus *mii_bus;    /* MDIO bus control */
    int link;
    phy_interface_t phy_interface;
    int msg_enable;
    int speed;
    int emac_irq;

    unsigned no_avb_link:1;
    unsigned avb_link_active_low:1;
    unsigned wol_enabled:1;
    unsigned rxcidm:1;      /* RX Clock Internal Delay Mode */
    unsigned txcidm:1;      /* TX Clock Internal Delay Mode */
    unsigned rgmii_override:1;  /* Deprecated rgmii-*id behavior */
    unsigned int num_tx_desc;   /* TX descriptors per packet */

    int duplex;

    const struct ravb_hw_info *info;
    struct reset_control *rstc;

    u32 gti_tiv;
};
```

```c
static inline void *netdev_priv(const struct net_device *dev)
{
    return (void *)dev->priv;
}
```

```c
static inline int reset_control_deassert(struct reset_control *rstc)
{
    return 0;
}
```

```c
static inline bool netif_running(const struct net_device *dev)
{
    return test_bit(__LINK_STATE_START, &dev->state);
}
```

```c
static int ravb_wol_restore(struct net_device *ndev)
{
    struct ravb_private *priv = netdev_priv(ndev);
    const struct ravb_hw_info *info = priv->info;
    int error;

    /* Set reset mode to rearm the WoL logic. */
    error = ravb_set_opmode(ndev, CCC_OPC_RESET);
    if (error)
        return error;

    /* Set AVB config mode. */
    error = ravb_set_config_mode(ndev);
    if (error)
        return error;

    if (priv->info->ccc_gac)
        ravb_ptp_init(ndev, priv->pdev);

    if (info->nc_queues)
        napi_enable(&priv->napi[RAVB_NC]);
    napi_enable(&priv->napi[RAVB_BE]);

    /* Disable MagicPacket */
    ravb_modify(ndev, ECMR, ECMR_MPDE, 0);

    ravb_close(ndev);

    return disable_irq_wake(priv->emac_irq);
}
```

```c
static inline int pm_runtime_force_resume(struct device *dev) { return 0; }
```

```c
static int ravb_open(struct net_device *ndev)
{
    struct ravb_private *priv = netdev_priv(ndev);
    const struct ravb_hw_info *info = priv->info;
    struct device *dev = &priv->pdev->dev;
    int error;

    napi_enable(&priv->napi[RAVB_BE]);
    if (info->nc_queues)
        napi_enable(&priv->napi[RAVB_NC]);

    error = pm_runtime_resume_and_get(dev);
    if (error < 0)
        goto out_napi_off;

    /* Set AVB config mode */
    error = ravb_set_config_mode(ndev);
    if (error)
        goto out_rpm_put;

    ravb_set_delay_mode(ndev);
    ravb_write(ndev, priv->desc_bat_dma, DBAT);

    /* Device init */
    error = ravb_dmac_init(ndev);
    if (error)
        goto out_set_reset;

    ravb_emac_init(ndev);

    ravb_set_gti(ndev);

    /* Initialise PTP Clock driver */
    if (info->gptp || info->ccc_gac)
        ravb_ptp_init(ndev, priv->pdev);

    /* PHY control start */
    error = ravb_phy_start(ndev);
    if (error)
        goto out_ptp_stop;

    netif_tx_start_all_queues(ndev);

    return 0;

out_ptp_stop:
    /* Stop PTP Clock driver */
    if (info->gptp || info->ccc_gac)
        ravb_ptp_stop(ndev);
    ravb_stop_dma(ndev);
out_set_reset:
    ravb_set_opmode(ndev, CCC_OPC_RESET);
out_rpm_put:
    pm_runtime_mark_last_busy(dev);
    pm_runtime_put_autosuspend(dev);
out_napi_off:
    if (info->nc_queues)
        napi_disable(&priv->napi[RAVB_NC]);
    napi_disable(&priv->napi[RAVB_BE]);
    return error;
}
```

```c
static void ravb_set_rx_mode(struct net_device *ndev)
{
    struct ravb_private *priv = netdev_priv(ndev);
    unsigned long flags;

    spin_lock_irqsave(&priv->lock, flags);
    ravb_modify(ndev, ECMR, ECMR_PRM,
            ndev->flags & IFF_PROMISC ? ECMR_PRM : 0);
    spin_unlock_irqrestore(&priv->lock, flags);
}
```

```c
static void ravb_set_rx_mode(struct net_device *ndev)
{
    struct ravb_private *priv = netdev_priv(ndev);
    unsigned long flags;

    spin_lock_irqsave(&priv->lock, flags);
    ravb_modify(ndev, ECMR, ECMR_PRM,
            ndev->flags & IFF_PROMISC ? ECMR_PRM : 0);
    spin_unlock_irqrestore(&priv->lock, flags);
}
```

```c
void netif_device_attach(struct net_device *dev)
{
    if (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&
        netif_running(dev)) {
        netif_tx_wake_all_queues(dev);
        __netdev_watchdog_up(dev);
    }
}
EXPORT_SYMBOL(netif_device_attach);
```

```c
static inline void pm_runtime_mark_last_busy(struct device *dev) {}
```

```c
static inline int pm_runtime_put_autosuspend(struct device *dev)
{
    return __pm_runtime_suspend(dev,
        RPM_GET_PUT | RPM_ASYNC | RPM_AUTO);
}
```
