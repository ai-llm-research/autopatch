{
  "cwe_type": "Improper Locking",
  "cve_id": "CVE-2025-21801",
  "supplementary_code": "```c\nstruct device {\nstruct kobject kobj;\nstruct device *parent;\nstruct device_private *p;\nconst char *init_name; /* initial name of the device */\nconst struct device_type *type;\nconst struct bus_type *bus; /* type of bus device is on */\nstruct device_driver *driver; /* which driver has allocated this\ndevice */\nvoid *platform_data; /* Platform specific data, device\ncore doesn't touch it */\nvoid *driver_data; /* Driver data, set and get with\ndev_set_drvdata/dev_get_drvdata */\nstruct mutex mutex; /* mutex to synchronize calls to\n* its driver.\n*/\nstruct dev_links_info links;\nstruct dev_pm_info power;\nstruct dev_pm_domain *pm_domain;\n#ifdef CONFIG_ENERGY_MODEL\nstruct em_perf_domain *em_pd;\n#endif\n#ifdef CONFIG_PINCTRL\nstruct dev_pin_info *pins;\n#endif\nstruct dev_msi_info msi;\n#ifdef CONFIG_ARCH_HAS_DMA_OPS\nconst struct dma_map_ops *dma_ops;\n#endif\nu64 *dma_mask; /* dma mask (if dma'able device) */\nu64 coherent_dma_mask;/* Like dma_mask, but for\nalloc_coherent mappings as\nnot all hardware supports\n64 bit addresses for consistent\nallocations such descriptors. */\nu64 bus_dma_limit; /* upstream dma constraint */\nconst struct bus_dma_region *dma_range_map;\nstruct device_dma_parameters *dma_parms;\nstruct list_head dma_pools; /* dma pools (if dma'ble) */\n#ifdef CONFIG_DMA_DECLARE_COHERENT\nstruct dma_coherent_mem *dma_mem; /* internal for coherent mem\noverride */\n#endif\n#ifdef CONFIG_DMA_CMA\nstruct cma *cma_area; /* contiguous memory area for dma\nallocations */\n#endif\n#ifdef CONFIG_SWIOTLB\nstruct io_tlb_mem *dma_io_tlb_mem;\n#endif\n#ifdef CONFIG_SWIOTLB_DYNAMIC\nstruct list_head dma_io_tlb_pools;\nspinlock_t dma_io_tlb_lock;\nbool dma_uses_io_tlb;\n#endif\n/* arch specific additions */\nstruct dev_archdata archdata;\nstruct device_node *of_node; /* associated device tree node */\nstruct fwnode_handle *fwnode; /* firmware device node */\n#ifdef CONFIG_NUMA\nint numa_node; /* NUMA node this device is close to */\n#endif\ndev_t devt; /* dev_t, creates the sysfs \"dev\" */\nu32 id; /* device instance */\nspinlock_t devres_lock;\nstruct list_head devres_head;\nconst struct class *class;\nconst struct attribute_group **groups; /* optional groups */\nvoid (*release)(struct device *dev);\nstruct iommu_group *iommu_group;\nstruct dev_iommu *iommu;\nstruct device_physical_location *physical_location;\nenum device_removable removable;\nbool offline_disabled:1;\nbool offline:1;\nbool of_node_reused:1;\nbool state_synced:1;\nbool can_match:1;\n#if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \\\ndefined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \\\ndefined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL)\nbool dma_coherent:1;\n#endif\n#ifdef CONFIG_DMA_OPS_BYPASS\nbool dma_ops_bypass : 1;\n#endif\n#ifdef CONFIG_DMA_NEED_SYNC\nbool dma_skip_sync:1;\n#endif\n#ifdef CONFIG_IOMMU_DMA\nbool dma_iommu:1;\n#endif\n};\n```\n```c\nstruct net_device {\n/* Cacheline organization can be found documented in\n* Documentation/networking/net_cachelines/net_device.rst.\n* Please update the document when adding new fields.\n*/\n/* TX read-mostly hotpath */\n__cacheline_group_begin(net_device_read_tx);\nstruct_group(priv_flags_fast,\nunsigned long priv_flags:32;\nunsigned long lltx:1;\n);\nconst struct net_device_ops *netdev_ops;\nconst struct header_ops *header_ops;\nstruct netdev_queue *_tx;\nnetdev_features_t gso_partial_features;\nunsigned int real_num_tx_queues;\nunsigned int gso_max_size;\nunsigned int gso_ipv4_max_size;\nu16 gso_max_segs;\ns16 num_tc;\n/* Note : dev->mtu is often read without holding a lock.\n* Writers usually hold RTNL.\n* It is recommended to use READ_ONCE() to annotate the reads,\n* and to use WRITE_ONCE() to annotate the writes.\n*/\nunsigned int mtu;\nunsigned short needed_headroom;\nstruct netdev_tc_txq tc_to_txq[TC_MAX_QUEUE];\n#ifdef CONFIG_XPS\nstruct xps_dev_maps __rcu *xps_maps[XPS_MAPS_MAX];\n#endif\n#ifdef CONFIG_NETFILTER_EGRESS\nstruct nf_hook_entries __rcu *nf_hooks_egress;\n#endif\n#ifdef CONFIG_NET_XGRESS\nstruct bpf_mprog_entry __rcu *tcx_egress;\n#endif\n__cacheline_group_end(net_device_read_tx);\n/* TXRX read-mostly hotpath */\n__cacheline_group_begin(net_device_read_txrx);\nunion {\nstruct pcpu_lstats __percpu *lstats;\nstruct pcpu_sw_netstats __percpu *tstats;\nstruct pcpu_dstats __percpu *dstats;\n};\nunsigned long state;\nunsigned int flags;\nunsigned short hard_header_len;\nnetdev_features_t features;\nstruct inet6_dev __rcu *ip6_ptr;\n__cacheline_group_end(net_device_read_txrx);\n/* RX read-mostly hotpath */\n__cacheline_group_begin(net_device_read_rx);\nstruct bpf_prog __rcu *xdp_prog;\nstruct list_head ptype_specific;\nint ifindex;\nunsigned int real_num_rx_queues;\nstruct netdev_rx_queue *_rx;\nunsigned int gro_max_size;\nunsigned int gro_ipv4_max_size;\nrx_handler_func_t __rcu *rx_handler;\nvoid __rcu *rx_handler_data;\npossible_net_t nd_net;\n#ifdef CONFIG_NETPOLL\nstruct netpoll_info __rcu *npinfo;\n#endif\n#ifdef CONFIG_NET_XGRESS\nstruct bpf_mprog_entry __rcu *tcx_ingress;\n#endif\n__cacheline_group_end(net_device_read_rx);\nchar name[IFNAMSIZ];\nstruct netdev_name_node *name_node;\nstruct dev_ifalias __rcu *ifalias;\n/*\n* I/O specific fields\n* FIXME: Merge these and struct ifmap into one\n*/\nunsigned long mem_end;\nunsigned long mem_start;\nunsigned long base_addr;\n/*\n* Some hardware also needs these fields (state,dev_list,\n* napi_list,unreg_list,close_list) but they are not\n* part of the usual set specified in Space.c.\n*/\nstruct list_head dev_list;\nstruct list_head napi_list;\nstruct list_head unreg_list;\nstruct list_head close_list;\nstruct list_head ptype_all;\nstruct {\nstruct list_head upper;\nstruct list_head lower;\n} adj_list;\n/* Read-mostly cache-line for fast-path access */\nxdp_features_t xdp_features;\nconst struct xdp_metadata_ops *xdp_metadata_ops;\nconst struct xsk_tx_metadata_ops *xsk_tx_metadata_ops;\nunsigned short gflags;\nunsigned short needed_tailroom;\nnetdev_features_t hw_features;\nnetdev_features_t wanted_features;\nnetdev_features_t vlan_features;\nnetdev_features_t hw_enc_features;\nnetdev_features_t mpls_features;\nunsigned int min_mtu;\nunsigned int max_mtu;\nunsigned short type;\nunsigned char min_header_len;\nunsigned char name_assign_type;\nint group;\nstruct net_device_stats stats; /* not used by modern drivers */\nstruct net_device_core_stats __percpu *core_stats;\n/* Stats to monitor link on/off, flapping */\natomic_t carrier_up_count;\natomic_t carrier_down_count;\n#ifdef CONFIG_WIRELESS_EXT\nconst struct iw_handler_def *wireless_handlers;\n#endif\nconst struct ethtool_ops *ethtool_ops;\n#ifdef CONFIG_NET_L3_MASTER_DEV\nconst struct l3mdev_ops *l3mdev_ops;\n#endif\n#if IS_ENABLED(CONFIG_IPV6)\nconst struct ndisc_ops *ndisc_ops;\n#endif\n#ifdef CONFIG_XFRM_OFFLOAD\nconst struct xfrmdev_ops *xfrmdev_ops;\n#endif\n#if IS_ENABLED(CONFIG_TLS_DEVICE)\nconst struct tlsdev_ops *tlsdev_ops;\n#endif\nunsigned int operstate;\nunsigned char link_mode;\nunsigned char if_port;\nunsigned char dma;\n/* Interface address info. */\nunsigned char perm_addr[MAX_ADDR_LEN];\nunsigned char addr_assign_type;\nunsigned char addr_len;\nunsigned char upper_level;\nunsigned char lower_level;\nunsigned short neigh_priv_len;\nunsigned short dev_id;\nunsigned short dev_port;\nint irq;\nu32 priv_len;\nspinlock_t addr_list_lock;\nstruct netdev_hw_addr_list uc;\nstruct netdev_hw_addr_list mc;\nstruct netdev_hw_addr_list dev_addrs;\n#ifdef CONFIG_SYSFS\nstruct kset *queues_kset;\n#endif\n#ifdef CONFIG_LOCKDEP\nstruct list_head unlink_list;\n#endif\nunsigned int promiscuity;\nunsigned int allmulti;\nbool uc_promisc;\n#ifdef CONFIG_LOCKDEP\nunsigned char nested_level;\n#endif\n/* Protocol-specific pointers */\nstruct in_device __rcu *ip_ptr;\n/** @fib_nh_head: nexthops associated with this netdev */\nstruct hlist_head fib_nh_head;\n#if IS_ENABLED(CONFIG_VLAN_8021Q)\nstruct vlan_info __rcu *vlan_info;\n#endif\n#if IS_ENABLED(CONFIG_NET_DSA)\nstruct dsa_port *dsa_ptr;\n#endif\n#if IS_ENABLED(CONFIG_TIPC)\nstruct tipc_bearer __rcu *tipc_ptr;\n#endif\n#if IS_ENABLED(CONFIG_ATALK)\nvoid *atalk_ptr;\n#endif\n#if IS_ENABLED(CONFIG_AX25)\nvoid *ax25_ptr;\n#endif\n#if IS_ENABLED(CONFIG_CFG80211)\nstruct wireless_dev *ieee80211_ptr;\n#endif\n#if IS_ENABLED(CONFIG_IEEE802154) || IS_ENABLED(CONFIG_6LOWPAN)\nstruct wpan_dev *ieee802154_ptr;\n#endif\n#if IS_ENABLED(CONFIG_MPLS_ROUTING)\nstruct mpls_dev __rcu *mpls_ptr;\n#endif\n#if IS_ENABLED(CONFIG_MCTP)\nstruct mctp_dev __rcu *mctp_ptr;\n#endif\n/*\n* Cache lines mostly used on receive path (including eth_type_trans())\n*/\n/* Interface address info used in eth_type_trans() */\nconst unsigned char *dev_addr;\nunsigned int num_rx_queues;\n#define GRO_LEGACY_MAX_SIZE 65536u\n/* TCP minimal MSS is 8 (TCP_MIN_GSO_SIZE),\n* and shinfo->gso_segs is a 16bit field.\n*/\n#define GRO_MAX_SIZE (8 * 65535u)\nunsigned int xdp_zc_max_segs;\nstruct netdev_queue __rcu *ingress_queue;\n#ifdef CONFIG_NETFILTER_INGRESS\nstruct nf_hook_entries __rcu *nf_hooks_ingress;\n#endif\nunsigned char broadcast[MAX_ADDR_LEN];\n#ifdef CONFIG_RFS_ACCEL\nstruct cpu_rmap *rx_cpu_rmap;\n#endif\nstruct hlist_node index_hlist;\n/*\n* Cache lines mostly used on transmit path\n*/\nunsigned int num_tx_queues;\nstruct Qdisc __rcu *qdisc;\nunsigned int tx_queue_len;\nspinlock_t tx_global_lock;\nstruct xdp_dev_bulk_queue __percpu *xdp_bulkq;\n#ifdef CONFIG_NET_SCHED\nDECLARE_HASHTABLE (qdisc_hash, 4);\n#endif\n/* These may be needed for future network-power-down code. */\nstruct timer_list watchdog_timer;\nint watchdog_timeo;\nu32 proto_down_reason;\nstruct list_head todo_list;\n#ifdef CONFIG_PCPU_DEV_REFCNT\nint __percpu *pcpu_refcnt;\n#else\nrefcount_t dev_refcnt;\n#endif\nstruct ref_tracker_dir refcnt_tracker;\nstruct list_head link_watch_list;\nu8 reg_state;\nbool dismantle;\nenum {\nRTNL_LINK_INITIALIZED,\nRTNL_LINK_INITIALIZING,\n} rtnl_link_state:16;\nbool needs_free_netdev;\nvoid (*priv_destructor)(struct net_device *dev);\n/* mid-layer private */\nvoid *ml_priv;\nenum netdev_ml_priv_type ml_priv_type;\nenum netdev_stat_type pcpu_stat_type:8;\n#if IS_ENABLED(CONFIG_GARP)\nstruct garp_port __rcu *garp_port;\n#endif\n#if IS_ENABLED(CONFIG_MRP)\nstruct mrp_port __rcu *mrp_port;\n#endif\n#if IS_ENABLED(CONFIG_NET_DROP_MONITOR)\nstruct dm_hw_stat_delta __rcu *dm_private;\n#endif\nstruct device dev;\nconst struct attribute_group *sysfs_groups[4];\nconst struct attribute_group *sysfs_rx_queue_group;\nconst struct rtnl_link_ops *rtnl_link_ops;\nconst struct netdev_stat_ops *stat_ops;\nconst struct netdev_queue_mgmt_ops *queue_mgmt_ops;\n/* for setting kernel sock attribute on TCP connection setup */\n#define GSO_MAX_SEGS 65535u\n#define GSO_LEGACY_MAX_SIZE 65536u\n/* TCP minimal MSS is 8 (TCP_MIN_GSO_SIZE),\n* and shinfo->gso_segs is a 16bit field.\n*/\n#define GSO_MAX_SIZE (8 * GSO_MAX_SEGS)\n#define TSO_LEGACY_MAX_SIZE 65536\n#define TSO_MAX_SIZE UINT_MAX\nunsigned int tso_max_size;\n#define TSO_MAX_SEGS U16_MAX\nu16 tso_max_segs;\n#ifdef CONFIG_DCB\nconst struct dcbnl_rtnl_ops *dcbnl_ops;\n#endif\nu8 prio_tc_map[TC_BITMASK + 1];\n#if IS_ENABLED(CONFIG_FCOE)\nunsigned int fcoe_ddp_xid;\n#endif\n#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)\nstruct netprio_map __rcu *priomap;\n#endif\nstruct phy_link_topology *link_topo;\nstruct phy_device *phydev;\nstruct sfp_bus *sfp_bus;\nstruct lock_class_key *qdisc_tx_busylock;\nbool proto_down;\nbool threaded;\n/* priv_flags_slow, ungrouped to save space */\nunsigned long see_all_hwtstamp_requests:1;\nunsigned long change_proto_down:1;\nunsigned long netns_local:1;\nunsigned long fcoe_mtu:1;\nstruct list_head net_notifier_list;\n#if IS_ENABLED(CONFIG_MACSEC)\n/* MACsec management functions */\nconst struct macsec_ops *macsec_ops;\n#endif\nconst struct udp_tunnel_nic_info *udp_tunnel_nic_info;\nstruct udp_tunnel_nic *udp_tunnel_nic;\nstruct ethtool_netdev_state *ethtool;\n/* protected by rtnl_lock */\nstruct bpf_xdp_entity xdp_state[__MAX_XDP_MODE];\nu8 dev_addr_shadow[MAX_ADDR_LEN];\nnetdevice_tracker linkwatch_dev_tracker;\nnetdevice_tracker watchdog_dev_tracker;\nnetdevice_tracker dev_registered_tracker;\nstruct rtnl_hw_stats64 *offload_xstats_l3;\nstruct devlink_port *devlink_port;\n#if IS_ENABLED(CONFIG_DPLL)\nstruct dpll_pin __rcu *dpll_pin;\n#endif\n#if IS_ENABLED(CONFIG_PAGE_POOL)\n/** @page_pools: page pools created for this netdevice */\nstruct hlist_head page_pools;\n#endif\n/** @irq_moder: dim parameters used if IS_ENABLED(CONFIG_DIMLIB). */\nstruct dim_irq_moder *irq_moder;\nu64 max_pacing_offload_horizon;\nstruct napi_config *napi_config;\nunsigned long gro_flush_timeout;\nu32 napi_defer_hard_irqs;\n/**\n* @lock: protects @net_shaper_hierarchy, feel free to use for other\n* netdev-scope protection. Ordering: take after rtnl_lock.\n*/\nstruct mutex lock;\n#if IS_ENABLED(CONFIG_NET_SHAPER)\n/**\n* @net_shaper_hierarchy: data tracking the current shaper status\n* see include/net/net_shapers.h\n*/\nstruct net_shaper_hierarchy *net_shaper_hierarchy;\n#endif\nstruct hlist_head neighbours[NEIGH_NR_TABLES];\nu8 priv[] ____cacheline_aligned\n__counted_by(priv_len);\n} ____cacheline_aligned;\n#define to_net_dev(d) container_of(d, struct net_device, dev)\n```\n```c\nstatic inline void *dev_get_drvdata(const struct device *dev)\n{\nreturn dev->driver_data;\n}\n```\n```c\nstruct ravb_private {\nstruct net_device *ndev;\nstruct platform_device *pdev;\nvoid __iomem *addr;\nstruct clk *clk;\nstruct clk *refclk;\nstruct clk *gptp_clk;\nstruct mdiobb_ctrl mdiobb;\nu32 num_rx_ring[NUM_RX_QUEUE];\nu32 num_tx_ring[NUM_TX_QUEUE];\nu32 desc_bat_size;\ndma_addr_t desc_bat_dma;\nstruct ravb_desc *desc_bat;\ndma_addr_t rx_desc_dma[NUM_RX_QUEUE];\ndma_addr_t tx_desc_dma[NUM_TX_QUEUE];\nunion {\nstruct ravb_rx_desc *desc;\nstruct ravb_ex_rx_desc *ex_desc;\nvoid *raw;\n} rx_ring[NUM_RX_QUEUE];\nstruct ravb_tx_desc *tx_ring[NUM_TX_QUEUE];\nvoid *tx_align[NUM_TX_QUEUE];\nstruct sk_buff *rx_1st_skb;\nstruct page_pool *rx_pool[NUM_RX_QUEUE];\nstruct ravb_rx_buffer *rx_buffers[NUM_RX_QUEUE];\nstruct sk_buff **tx_skb[NUM_TX_QUEUE];\nu32 rx_over_errors;\nu32 rx_fifo_errors;\nstruct net_device_stats stats[NUM_RX_QUEUE];\nu32 tstamp_tx_ctrl;\nu32 tstamp_rx_ctrl;\nstruct list_head ts_skb_list;\nu32 ts_skb_tag;\nstruct ravb_ptp ptp;\nspinlock_t lock; /* Register access lock */\nu32 cur_rx[NUM_RX_QUEUE]; /* Consumer ring indices */\nu32 dirty_rx[NUM_RX_QUEUE]; /* Producer ring indices */\nu32 cur_tx[NUM_TX_QUEUE];\nu32 dirty_tx[NUM_TX_QUEUE];\nstruct napi_struct napi[NUM_RX_QUEUE];\nstruct work_struct work;\n/* MII transceiver section. */\nstruct mii_bus *mii_bus; /* MDIO bus control */\nint link;\nphy_interface_t phy_interface;\nint msg_enable;\nint speed;\nint emac_irq;\nunsigned no_avb_link:1;\nunsigned avb_link_active_low:1;\nunsigned wol_enabled:1;\nunsigned rxcidm:1; /* RX Clock Internal Delay Mode */\nunsigned txcidm:1; /* TX Clock Internal Delay Mode */\nunsigned rgmii_override:1; /* Deprecated rgmii-*id behavior */\nunsigned int num_tx_desc; /* TX descriptors per packet */\nint duplex;\nconst struct ravb_hw_info *info;\nstruct reset_control *rstc;\nu32 gti_tiv;\n};\n```\n```c\nstatic inline void *netdev_priv(const struct net_device *dev)\n{\nreturn (void *)dev->priv;\n}\n```\n```c\nstatic inline int reset_control_deassert(struct reset_control *rstc)\n{\nreturn 0;\n}\n```\n```c\nstatic inline bool netif_running(const struct net_device *dev)\n{\nreturn test_bit(__LINK_STATE_START, &dev->state);\n}\n```\n```c\nstatic int ravb_wol_restore(struct net_device *ndev)\n{\nstruct ravb_private *priv = netdev_priv(ndev);\nconst struct ravb_hw_info *info = priv->info;\nint error;\n/* Set reset mode to rearm the WoL logic. */\nerror = ravb_set_opmode(ndev, CCC_OPC_RESET);\nif (error)\nreturn error;\n/* Set AVB config mode. */\nerror = ravb_set_config_mode(ndev);\nif (error)\nreturn error;\nif (priv->info->ccc_gac)\nravb_ptp_init(ndev, priv->pdev);\nif (info->nc_queues)\nnapi_enable(&priv->napi[RAVB_NC]);\nnapi_enable(&priv->napi[RAVB_BE]);\n/* Disable MagicPacket */\nravb_modify(ndev, ECMR, ECMR_MPDE, 0);\nravb_close(ndev);\nreturn disable_irq_wake(priv->emac_irq);\n}\n```\n```c\nstatic inline int pm_runtime_force_resume(struct device *dev) { return 0; }\n```\n```c\nstatic int ravb_open(struct net_device *ndev)\n{\nstruct ravb_private *priv = netdev_priv(ndev);\nconst struct ravb_hw_info *info = priv->info;\nstruct device *dev = &priv->pdev->dev;\nint error;\nnapi_enable(&priv->napi[RAVB_BE]);\nif (info->nc_queues)\nnapi_enable(&priv->napi[RAVB_NC]);\nerror = pm_runtime_resume_and_get(dev);\nif (error < 0)\ngoto out_napi_off;\n/* Set AVB config mode */\nerror = ravb_set_config_mode(ndev);\nif (error)\ngoto out_rpm_put;\nravb_set_delay_mode(ndev);\nravb_write(ndev, priv->desc_bat_dma, DBAT);\n/* Device init */\nerror = ravb_dmac_init(ndev);\nif (error)\ngoto out_set_reset;\nravb_emac_init(ndev);\nravb_set_gti(ndev);\n/* Initialise PTP Clock driver */\nif (info->gptp || info->ccc_gac)\nravb_ptp_init(ndev, priv->pdev);\n/* PHY control start */\nerror = ravb_phy_start(ndev);\nif (error)\ngoto out_ptp_stop;\nnetif_tx_start_all_queues(ndev);\nreturn 0;\nout_ptp_stop:\n/* Stop PTP Clock driver */\nif (info->gptp || info->ccc_gac)\nravb_ptp_stop(ndev);\nravb_stop_dma(ndev);\nout_set_reset:\nravb_set_opmode(ndev, CCC_OPC_RESET);\nout_rpm_put:\npm_runtime_mark_last_busy(dev);\npm_runtime_put_autosuspend(dev);\nout_napi_off:\nif (info->nc_queues)\nnapi_disable(&priv->napi[RAVB_NC]);\nnapi_disable(&priv->napi[RAVB_BE]);\nreturn error;\n}\n```\n```c\nstatic void ravb_set_rx_mode(struct net_device *ndev)\n{\nstruct ravb_private *priv = netdev_priv(ndev);\nunsigned long flags;\nspin_lock_irqsave(&priv->lock, flags);\nravb_modify(ndev, ECMR, ECMR_PRM,\nndev->flags & IFF_PROMISC ? ECMR_PRM : 0);\nspin_unlock_irqrestore(&priv->lock, flags);\n}\n```\n```c\nstatic void ravb_set_rx_mode(struct net_device *ndev)\n{\nstruct ravb_private *priv = netdev_priv(ndev);\nunsigned long flags;\nspin_lock_irqsave(&priv->lock, flags);\nravb_modify(ndev, ECMR, ECMR_PRM,\nndev->flags & IFF_PROMISC ? ECMR_PRM : 0);\nspin_unlock_irqrestore(&priv->lock, flags);\n}\n```\n```c\nvoid netif_device_attach(struct net_device *dev)\n{\nif (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&\nnetif_running(dev)) {\nnetif_tx_wake_all_queues(dev);\n__netdev_watchdog_up(dev);\n}\n}\nEXPORT_SYMBOL(netif_device_attach);\n```\n```c\nstatic inline void pm_runtime_mark_last_busy(struct device *dev) {}\n```\n```c\nstatic inline int pm_runtime_put_autosuspend(struct device *dev)\n{\nreturn __pm_runtime_suspend(dev,\nRPM_GET_PUT | RPM_ASYNC | RPM_AUTO);\n}\n```",
  "original_code": "```c\nstatic int ravb_resume(struct device *dev)\n{\nstruct net_device *ndev = dev_get_drvdata(dev);\nstruct ravb_private *priv = netdev_priv(ndev);\nint ret;\nret = reset_control_deassert(priv->rstc);\nif (ret)\nreturn ret;\nif (!netif_running(ndev))\nreturn 0;\n/* If WoL is enabled restore the interface. */\nif (priv->wol_enabled) {\nret = ravb_wol_restore(ndev);\nif (ret)\nreturn ret;\n} else {\nret = pm_runtime_force_resume(dev);\nif (ret)\nreturn ret;\n}\n/* Reopening the interface will restore the device to the working state. */\nret = ravb_open(ndev);\nif (ret < 0)\ngoto out_rpm_put;\nravb_set_rx_mode(ndev);\nnetif_device_attach(ndev);\nreturn 0;\nout_rpm_put:\nif (!priv->wol_enabled) {\npm_runtime_mark_last_busy(dev);\npm_runtime_put_autosuspend(dev);\n}\nreturn ret;\n}\n```",
  "vuln_patch": "```c\nstatic int ravb_resume(struct device *dev)\n{\nstruct net_device *ndev = dev_get_drvdata(dev);\nstruct ravb_private *priv = netdev_priv(ndev);\nint ret;\nret = reset_control_deassert(priv->rstc);\nif (ret)\nreturn ret;\nif (!netif_running(ndev))\nreturn 0;\nrtnl_lock();\n/* If WoL is enabled restore the interface. */\nif (priv->wol_enabled)\nret = ravb_wol_restore(ndev);\nelse\nret = pm_runtime_force_resume(dev);\nif (ret) {\nrtnl_unlock();\nreturn ret;\n}\n/* Reopening the interface will restore the device to the working state. */\nret = ravb_open(ndev);\nrtnl_unlock();\nif (ret < 0)\ngoto out_rpm_put;\nravb_set_rx_mode(ndev);\nnetif_device_attach(ndev);\nreturn 0;\nout_rpm_put:\nif (!priv->wol_enabled) {\npm_runtime_mark_last_busy(dev);\npm_runtime_put_autosuspend(dev);\n}\nreturn ret;\n}\n```",
  "function_name": "ravb_resume",
  "function_prototype": "static int ravb_resume(struct device *dev)",
  "code_semantics": "The function handles the resumption of a network device from a suspended state. It retrieves the network device and its private data, deasserts a reset control, and checks if the network interface is running. If the interface is running and Wake-on-LAN is enabled, it restores the interface; otherwise, it forces the device to resume from a runtime power management state. It then attempts to reopen the network interface, setting it to a working state. If reopening fails and Wake-on-LAN is not enabled, it marks the device as busy and puts it into an autosuspend state. Finally, it sets the receive mode and attaches the device to the network stack.",
  "vulnerability_checklist": "Check if function ravb_wol_restore(ndev) is called within a locked section using rtnl_lock(). Verify that function pm_runtime_force_resume(dev) is called within a locked section using rtnl_lock(). Ensure that function ravb_open(ndev) is called within a locked section using rtnl_lock(). Confirm that rtnl_lock() is used before any network device operations that can change the device state. Confirm that rtnl_unlock() is used after the network device operations are completed.",
  "safe_verification_cot": "The function ravb_wol_restore(ndev) is called within a locked section using rtnl_lock(), ensuring that the network device state is protected from concurrent modifications. The function pm_runtime_force_resume(dev) is also called within a locked section using rtnl_lock(), preventing race conditions. The function ravb_open(ndev) is executed within a locked section using rtnl_lock(), ensuring consistent device state. The use of rtnl_lock() before and rtnl_unlock() after the critical section ensures that the code is not vulnerable to race conditions, providing proper locking.",
  "verification_cot": "The function ravb_wol_restore(ndev) is called without acquiring the rtnl_lock(), which means the network device state can be altered by other threads concurrently. The function pm_runtime_force_resume(dev) is also called without the rtnl_lock(), leading to potential race conditions. The function ravb_open(ndev) is executed without the rtnl_lock(), allowing concurrent modifications to the network device state. The absence of rtnl_lock() and rtnl_unlock() around these operations means that the code is vulnerable to race conditions, leading to improper locking.",
  "vulnerability_related_variables": {
    "ndev": "This variable represents an entity that is used to manage and control a network interface. It is involved in various operations such as checking its operational state, restoring its configuration, and attaching it to the system.",
    "priv": "This variable acts as a container for private data associated with a network interface. It holds configuration and state information that is used to manage the interface's behavior and interactions with other components.",
    "ret": "This variable is used to store the result of operations or function calls. It is used to determine the success or failure of these operations and to control the flow of execution based on these outcomes."
  },
  "vulnerability_related_functions": {
    "ravb_wol_restore": "This function reinitializes the network interface to restore Wake-on-LAN functionality. It sets specific operational modes, enables certain features, and disables specific interrupts to ensure the device is ready for WoL.",
    "pm_runtime_force_resume": "This function resumes the power management of a device, ensuring it is in an active state, and returns a status indicating success or failure.",
    "ravb_open": "This function prepares and activates a network interface by configuring necessary settings, enabling features, and starting data transmission. It handles errors by reverting changes and disabling features if initialization fails.",
    "rtnl_lock": "This function acquires a lock to prevent concurrent modifications to network configurations, ensuring thread-safe operations.",
    "rtnl_unlock": "This function releases a previously acquired lock, allowing other operations to modify network configurations."
  },
  "root_cause": "Improper locking of network device operations leading to race conditions.",
  "patch_cot": "First, identify the critical sections in the code where race conditions could occur. In this case, these are the operations involving ravb_wol_restore, pm_runtime_force_resume, and ravb_open. Introduce rtnl_lock() before these operations to ensure that they are executed atomically. After these operations, introduce rtnl_unlock() to release the lock and allow other threads to proceed. Ensure that the variable ret is checked after each critical operation to handle any errors appropriately. Verify that all accesses to shared resources, such as ndev and priv, are properly synchronized using the introduced locks.",
  "fix_list": "Check if function ravb_wol_restore is safely handling variable ndev. Verify that variable priv is properly synchronized before it is used in function pm_runtime_force_resume. Ensure that function ravb_open is protected by rtnl_lock and rtnl_unlock to prevent race conditions. Verify that variable ret is checked and handled correctly after each function call to ensure proper error handling. Ensure that rtnl_lock is acquired before any critical operations and released with rtnl_unlock after these operations."
}