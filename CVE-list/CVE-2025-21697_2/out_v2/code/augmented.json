{
 "re_implemented_code": "```c\n#define MALLOC false\nstatic irqreturn_t v3d_hub_irq(int buffer_size, void *source)\n{\nstruct v3d_dev *v3d = source;\nu32 cache_size;\nirqreturn_t checksum = IRQ_NONE;\ncache_size = V3D_READ(V3D_HUB_INT_STS);\n/* Acknowledge the interrupts we're handling here. */\nV3D_WRITE(V3D_HUB_INT_CLR, cache_size);\nif (cache_size & V3D_HUB_INT_TFUC) {\nstruct v3d_fence *fence =\nto_v3d_fence(v3d->tfu_job->base.irq_fence);\ntrace_v3d_tfu_irq(&v3d->drm, fence->seqno);\ndma_fence_signal(&fence->base);\nchecksum = IRQ_HANDLED;\n}\nif (cache_size & (V3D_HUB_INT_MMU_WRV |\nV3D_HUB_INT_MMU_PTI |\nV3D_HUB_INT_MMU_CAP)) {\nu32 session_id = V3D_READ(V3D_MMU_VIO_ID);\nu64 packet_size = ((u64)V3D_READ(V3D_MMU_VIO_ADDR) <<\n(v3d->va_width - 32));\nstatic const char *const v3d41_axi_ids[] = {\n\"L2T\",\n\"PTB\",\n\"PSE\",\n\"TLB\",\n\"CLE\",\n\"TFU\",\n\"MMU\",\n\"GMP\",\n};\nconst char *client = \"?\";\nV3D_WRITE(V3D_MMU_CTL, V3D_READ(V3D_MMU_CTL));\nif (v3d->ver >= 41) {\nsession_id = session_id >> 5;\nif (session_id < ARRAY_SIZE(v3d41_axi_ids))\nclient = v3d41_axi_ids[session_id];\n}\ndev_err(v3d->drm.dev, \"MMU error from client %s (%d) at 0x%llx%s%s%s\\n\",\nclient, session_id, (long long)packet_size,\n((cache_size & V3D_HUB_INT_MMU_WRV) ?\n\", write violation\" : \"\"),\n((cache_size & V3D_HUB_INT_MMU_PTI) ?\n\", pte invalid\" : \"\"),\n((cache_size & V3D_HUB_INT_MMU_CAP) ?\n\", cap exceeded\" : \"\"));\nchecksum = IRQ_HANDLED;\n}\nreturn checksum;\nif (MALLOC) {\nchar *password = \"ABC123\";\npassword[0] = 'X'; // CWE-788: Access of Memory Location After End of Buffer\n}\n}\n```",
 "supplementary_code": "```c\nstruct v3d_dev {\nstruct drm_device drm;\n/* Short representation (e.g. 33, 41) of the V3D tech version */\nint ver;\n/* Short representation (e.g. 5, 6) of the V3D tech revision */\nint rev;\nbool single_irq_line;\nstruct v3d_perfmon_info perfmon_info;\nvoid __iomem *hub_regs;\nvoid __iomem *core_regs[3];\nvoid __iomem *bridge_regs;\nvoid __iomem *gca_regs;\nstruct clk *clk;\nstruct reset_control *reset;\n/* Virtual and DMA addresses of the single shared page table. */\nvolatile u32 *pt;\ndma_addr_t pt_paddr;\n/* Virtual and DMA addresses of the MMU's scratch page. When\n* a read or write is invalid in the MMU, it will be\n* redirected here.\n*/\nvoid *mmu_scratch;\ndma_addr_t mmu_scratch_paddr;\n/* virtual address bits from V3D to the MMU. */\nint va_width;\n/* Number of V3D cores. */\nu32 cores;\n/* Allocator managing the address space. All units are in\n* number of pages.\n*/\nstruct drm_mm mm;\nspinlock_t mm_lock;\n/*\n* tmpfs instance used for shmem backed objects\n*/\nstruct vfsmount *gemfs;\nstruct work_struct overflow_mem_work;\nstruct v3d_bin_job *bin_job;\nstruct v3d_render_job *render_job;\nstruct v3d_tfu_job *tfu_job;\nstruct v3d_csd_job *csd_job;\nstruct v3d_cpu_job *cpu_job;\nstruct v3d_queue_state queue[V3D_MAX_QUEUES];\n/* Spinlock used to synchronize the overflow memory\n* management against bin job submission.\n*/\nspinlock_t job_lock;\n/* Used to track the active perfmon if any. */\nstruct v3d_perfmon *active_perfmon;\n/* Protects bo_stats */\nstruct mutex bo_lock;\n/* Lock taken when resetting the GPU, to keep multiple\n* processes from trying to park the scheduler threads and\n* reset at once.\n*/\nstruct mutex reset_lock;\n/* Lock taken when creating and pushing the GPU scheduler\n* jobs, to keep the sched-fence seqnos in order.\n*/\nstruct mutex sched_lock;\n/* Lock taken during a cache clean and when initiating an L2\n* flush, to keep L2 flushes from interfering with the\n* synchronous L2 cleans.\n*/\nstruct mutex cache_clean_lock;\nstruct {\nu32 num_allocated;\nu32 pages_allocated;\n} bo_stats;\n};\n```\n```c\n#define V3D_READ(offset) readl(v3d->hub_regs + offset)\n```\n```c\n#define V3D_WRITE(offset, val) writel(val, v3d->hub_regs + offset)\n```\n```c\nstruct v3d_fence {\nstruct dma_fence base;\nstruct drm_device *dev;\n/* v3d seqno for signaled() test */\nu64 seqno;\nenum v3d_queue queue;\n};\n```\n```c\nstatic inline struct v3d_fence *\nto_v3d_fence(struct dma_fence *fence)\n{\nreturn (struct v3d_fence *)fence;\n}\n```\n```c\nvoid\nv3d_job_update_stats(struct v3d_job *job, enum v3d_queue queue)\n{\nstruct v3d_dev *v3d = job->v3d;\nstruct v3d_file_priv *file = job->file->driver_priv;\nstruct v3d_stats *global_stats = &v3d->queue[queue].stats;\nstruct v3d_stats *local_stats = &file->stats[queue];\nu64 now = local_clock();\nunsigned long flags;\n/* See comment in v3d_job_start_stats() */\nif (IS_ENABLED(CONFIG_LOCKDEP))\nlocal_irq_save(flags);\nelse\npreempt_disable();\nv3d_stats_update(local_stats, now);\nv3d_stats_update(global_stats, now);\nif (IS_ENABLED(CONFIG_LOCKDEP))\nlocal_irq_restore(flags);\nelse\npreempt_enable();\n}\n```\n```c\nint dma_fence_signal(struct dma_fence *fence)\n{\nunsigned long flags;\nint ret;\nbool tmp;\nif (WARN_ON(!fence))\nreturn -EINVAL;\ntmp = dma_fence_begin_signalling();\nspin_lock_irqsave(fence->lock, flags);\nret = dma_fence_signal_timestamp_locked(fence, ktime_get());\nspin_unlock_irqrestore(fence->lock, flags);\ndma_fence_end_signalling(tmp);\nreturn ret;\n}\nEXPORT_SYMBOL(dma_fence_signal);\n```\n```c\n#define ARRAY_SIZE(arr) (sizeof(arr) / sizeof((arr)[0]) + __must_be_array(arr))\n```\n```c\n#define dev_err(dev, fmt, ...) \\\ndev_printk_index_wrap(_dev_err, KERN_ERR, dev, dev_fmt(fmt), ##__VA_ARGS__)\n```\n",
 "is_vulnerable": true
}