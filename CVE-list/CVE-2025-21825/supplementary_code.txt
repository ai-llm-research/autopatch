```c
struct bpf_hrtimer {
    struct bpf_async_cb cb;
    struct hrtimer timer;
    atomic_t cancelling;
};
```

```c
static struct bpf_async_cb *__bpf_async_cancel_and_free(struct bpf_async_kern *async)
{
    struct bpf_async_cb *cb;

    /* Performance optimization: read async->cb without lock first. */
    if (!READ_ONCE(async->cb))
        return NULL;

    __bpf_spin_lock_irqsave(&async->lock);
    /* re-read it under lock */
    cb = async->cb;
    if (!cb)
        goto out;
    drop_prog_refcnt(cb);
    /* The subsequent bpf_timer_start/cancel() helpers won't be able to use
     * this timer, since it won't be initialized.
     */
    WRITE_ONCE(async->cb, NULL);
out:
    __bpf_spin_unlock_irqrestore(&async->lock);
    return cb;
}
```

```c
#define this_cpu_read(pcp)      __pcpu_size_call_return(this_cpu_read_, pcp)
```

```c
static inline bool queue_work(struct workqueue_struct *wq,
                  struct work_struct *work)
{
    return queue_work_on(WORK_CPU_UNBOUND, wq, work);
}
```

```c
static void bpf_timer_delete_work(struct work_struct *work)
{
    struct bpf_hrtimer *t = container_of(work, struct bpf_hrtimer, cb.delete_work);

    /* Cancel the timer and wait for callback to complete if it was running.
     * If hrtimer_cancel() can be safely called it's safe to call
     * kfree_rcu(t) right after for both preallocated and non-preallocated
     * maps.  The async->cb = NULL was already done and no code path can see
     * address 't' anymore. Timer if armed for existing bpf_hrtimer before
     * bpf_timer_cancel_and_free will have been cancelled.
     */
    hrtimer_cancel(&t->timer);
    kfree_rcu(t, cb.rcu);
}
```
