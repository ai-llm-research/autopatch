{
  "cwe_type": "Race Condition",
  "cve_id": "CVE-2025-21825",
  "supplementary_code": "```c\nstruct bpf_hrtimer {\nstruct bpf_async_cb cb;\nstruct hrtimer timer;\natomic_t cancelling;\n};\n```\n```c\nstatic struct bpf_async_cb *__bpf_async_cancel_and_free(struct bpf_async_kern *async)\n{\nstruct bpf_async_cb *cb;\n/* Performance optimization: read async->cb without lock first. */\nif (!READ_ONCE(async->cb))\nreturn NULL;\n__bpf_spin_lock_irqsave(&async->lock);\n/* re-read it under lock */\ncb = async->cb;\nif (!cb)\ngoto out;\ndrop_prog_refcnt(cb);\n/* The subsequent bpf_timer_start/cancel() helpers won't be able to use\n* this timer, since it won't be initialized.\n*/\nWRITE_ONCE(async->cb, NULL);\nout:\n__bpf_spin_unlock_irqrestore(&async->lock);\nreturn cb;\n}\n```\n```c\n#define this_cpu_read(pcp) __pcpu_size_call_return(this_cpu_read_, pcp)\n```\n```c\nstatic inline bool queue_work(struct workqueue_struct *wq,\nstruct work_struct *work)\n{\nreturn queue_work_on(WORK_CPU_UNBOUND, wq, work);\n}\n```\n```c\nstatic void bpf_timer_delete_work(struct work_struct *work)\n{\nstruct bpf_hrtimer *t = container_of(work, struct bpf_hrtimer, cb.delete_work);\n/* Cancel the timer and wait for callback to complete if it was running.\n* If hrtimer_cancel() can be safely called it's safe to call\n* kfree_rcu(t) right after for both preallocated and non-preallocated\n* maps. The async->cb = NULL was already done and no code path can see\n* address 't' anymore. Timer if armed for existing bpf_hrtimer before\n* bpf_timer_cancel_and_free will have been cancelled.\n*/\nhrtimer_cancel(&t->timer);\nkfree_rcu(t, cb.rcu);\n}\n```",
  "original_code": "```c\nvoid bpf_timer_cancel_and_free(void *val)\n{\nstruct bpf_hrtimer *t;\nt = (struct bpf_hrtimer *)__bpf_async_cancel_and_free(val);\nif (!t)\nreturn;\n/* We check that bpf_map_delete/update_elem() was called from timer\n* callback_fn. In such case we don't call hrtimer_cancel() (since it\n* will deadlock) and don't call hrtimer_try_to_cancel() (since it will\n* just return -1). Though callback_fn is still running on this cpu it's\n* safe to do kfree(t) because bpf_timer_cb() read everything it needed\n* from 't'. The bpf subprog callback_fn won't be able to access 't',\n* since async->cb = NULL was already done. The timer will be\n* effectively cancelled because bpf_timer_cb() will return\n* HRTIMER_NORESTART.\n*\n* However, it is possible the timer callback_fn calling us armed the\n* timer _before_ calling us, such that failing to cancel it here will\n* cause it to possibly use struct hrtimer after freeing bpf_hrtimer.\n* Therefore, we _need_ to cancel any outstanding timers before we do\n* kfree_rcu, even though no more timers can be armed.\n*\n* Moreover, we need to schedule work even if timer does not belong to\n* the calling callback_fn, as on two different CPUs, we can end up in a\n* situation where both sides run in parallel, try to cancel one\n* another, and we end up waiting on both sides in hrtimer_cancel\n* without making forward progress, since timer1 depends on time2\n* callback to finish, and vice versa.\n*\n* CPU 1 (timer1_cb)\tCPU 2 (timer2_cb)\n* bpf_timer_cancel_and_free(timer2)\tbpf_timer_cancel_and_free(timer1)\n*\n* To avoid these issues, punt to workqueue context when we are in a\n* timer callback.\n*/\nif (this_cpu_read(hrtimer_running))\nqueue_work(system_unbound_wq, &t->cb.delete_work);\nelse\nbpf_timer_delete_work(&t->cb.delete_work);\n}\n```",
  "vuln_patch": "```c\nvoid bpf_timer_cancel_and_free(void *val)\n{\nstruct bpf_hrtimer *t;\nt = (struct bpf_hrtimer *)__bpf_async_cancel_and_free(val);\nif (!t)\nreturn;\n/* We check that bpf_map_delete/update_elem() was called from timer\n* callback_fn. In such case we don't call hrtimer_cancel() (since it\n* will deadlock) and don't call hrtimer_try_to_cancel() (since it will\n* just return -1). Though callback_fn is still running on this cpu it's\n* safe to do kfree(t) because bpf_timer_cb() read everything it needed\n* from 't'. The bpf subprog callback_fn won't be able to access 't',\n* since async->cb = NULL was already done. The timer will be\n* effectively cancelled because bpf_timer_cb() will return\n* HRTIMER_NORESTART.\n*\n* However, it is possible the timer callback_fn calling us armed the\n* timer _before_ calling us, such that failing to cancel it here will\n* cause it to possibly use struct hrtimer after freeing bpf_hrtimer.\n* Therefore, we _need_ to cancel any outstanding timers before we do\n* kfree_rcu, even though no more timers can be armed.\n*\n* Moreover, we need to schedule work even if timer does not belong to\n* the calling callback_fn, as on two different CPUs, we can end up in a\n* situation where both sides run in parallel, try to cancel one\n* another, and we end up waiting on both sides in hrtimer_cancel\n* without making forward progress, since timer1 depends on time2\n* callback to finish, and vice versa.\n*\n* CPU 1 (timer1_cb)\tCPU 2 (timer2_cb)\n* bpf_timer_cancel_and_free(timer2)\tbpf_timer_cancel_and_free(timer1)\n*\n* To avoid these issues, punt to workqueue context when we are in a\n* timer callback.\n*/\nif (this_cpu_read(hrtimer_running)) {\nqueue_work(system_unbound_wq, &t->cb.delete_work);\nreturn;\n}\nif (IS_ENABLED(CONFIG_PREEMPT_RT)) {\n/* If the timer is running on other CPU, also use a kworker to\n* wait for the completion of the timer instead of trying to\n* acquire a sleepable lock in hrtimer_cancel() to wait for its\n* completion.\n*/\nif (hrtimer_try_to_cancel(&t->timer) >= 0)\nkfree_rcu(t, cb.rcu);\nelse\nqueue_work(system_unbound_wq, &t->cb.delete_work);\n} else {\nbpf_timer_delete_work(&t->cb.delete_work);\n}\n}\n```",
  "function_name": "bpf_timer_cancel_and_free",
  "function_prototype": "void bpf_timer_cancel_and_free(void *val)",
  "code_semantics": "The code manages the lifecycle of a timer by first attempting to cancel any ongoing asynchronous operations related to it. If the timer structure is valid, it checks if the function is being executed as part of a timer callback. If it is, it schedules a task to safely delete the timer later. If not, it directly deletes the timer. This ensures that the timer is properly canceled and resources are freed without causing conflicts or accessing invalid memory.",
  "safe_verification_cot": "1. The function bpf_timer_cancel_and_free includes a check for CONFIG_PREEMPT_RT and uses hrtimer_try_to_cancel to safely attempt cancellation of t->timer. 2. The variable t is not accessed after it has been freed because the code ensures that the timer is canceled or deferred using queue_work. 3. The use of hrtimer_try_to_cancel prevents deadlocks and ensures safe cancellation of the timer. 4. The function bpf_timer_delete_work is only called when it is safe to do so, preventing race conditions.",
  "verification_cot": "1. The function bpf_timer_cancel_and_free does not handle the case where t->timer is running on another CPU, leading to potential race conditions. 2. The variable t could be accessed after it has been freed if the timer is not properly canceled. 3. The code does not use hrtimer_try_to_cancel to safely attempt cancellation, which can lead to deadlocks or use-after-free scenarios. 4. The function bpf_timer_delete_work is called directly without ensuring that it is safe to do so, potentially leading to race conditions.",
  "vulnerability_related_variables": {
    "t": "This variable holds a reference to a data structure that contains a timer and associated callback information. It is used to manage the lifecycle of the timer, including cancellation and cleanup operations.",
    "t->timer": "This component represents a high-resolution timer within a data structure. It is used to schedule and manage precise timing events, and it can be canceled to prevent further execution of its associated callback.",
    "t->cb.delete_work": "This component is a work item within a data structure that is used to perform deferred execution of cleanup operations. It can be scheduled to run in a separate execution context to safely handle resource deallocation and other cleanup tasks."
  },
  "vulnerability_related_functions": {
    "queue_work": "The function takes a workqueue and a work item as inputs. It enqueues the work item to be processed by the workqueue, allowing deferred execution of the work item in a different context.",
    "bpf_timer_delete_work": "The function takes a work item as input. It cancels an associated high-resolution timer, ensuring it is not running, and then safely deallocates the memory associated with the timer object.",
    "hrtimer_cancel": "The function takes a timer as input. It stops the timer and ensures that any pending callbacks are completed before returning, effectively canceling the timer.",
    "hrtimer_try_to_cancel": "The function takes a timer as input. It attempts to stop the timer and returns a status indicating whether the timer was successfully stopped or if it was already running."
  },
  "root_cause": "The root cause of CVE-2025-21825 is a race condition that occurs when two CPUs attempt to cancel timers simultaneously, leading to potential deadlocks or use-after-free scenarios.",
  "patch_cot": "1. Check if the timer is currently running using this_cpu_read(hrtimer_running). If it is, schedule the work using queue_work to handle t->cb.delete_work in a non-blocking manner.\n2. If the system is configured with CONFIG_PREEMPT_RT, attempt to cancel the timer using hrtimer_try_to_cancel. This function will attempt to cancel the timer without blocking.\n3. If hrtimer_try_to_cancel returns a non-negative value, indicating the timer was successfully canceled, proceed to free the memory using kfree_rcu.\n4. If hrtimer_try_to_cancel fails, indicating the timer is still running, schedule the work using queue_work to handle the cancellation and memory freeing in a non-blocking manner.\n5. If the system is not configured with CONFIG_PREEMPT_RT, directly call bpf_timer_delete_work to handle t->cb.delete_work."
}