{
 "supplementary_code": "```c\nstruct bpf_hrtimer {\nstruct bpf_async_cb cb;\nstruct hrtimer timer;\natomic_t cancelling;\n};\n```\n```c\nstatic struct bpf_async_cb *__bpf_async_cancel_and_free(struct bpf_async_kern *async)\n{\nstruct bpf_async_cb *cb;\n/* Performance optimization: read async->cb without lock first. */\nif (!READ_ONCE(async->cb))\nreturn NULL;\n__bpf_spin_lock_irqsave(&async->lock);\n/* re-read it under lock */\ncb = async->cb;\nif (!cb)\ngoto out;\ndrop_prog_refcnt(cb);\n/* The subsequent bpf_timer_start/cancel() helpers won't be able to use\n* this timer, since it won't be initialized.\n*/\nWRITE_ONCE(async->cb, NULL);\nout:\n__bpf_spin_unlock_irqrestore(&async->lock);\nreturn cb;\n}\n```\n```c\n#define this_cpu_read(pcp) __pcpu_size_call_return(this_cpu_read_, pcp)\n```\n```c\nstatic inline bool queue_work(struct workqueue_struct *wq,\nstruct work_struct *work)\n{\nreturn queue_work_on(WORK_CPU_UNBOUND, wq, work);\n}\n```\n```c\nstatic void bpf_timer_delete_work(struct work_struct *work)\n{\nstruct bpf_hrtimer *t = container_of(work, struct bpf_hrtimer, cb.delete_work);\n/* Cancel the timer and wait for callback to complete if it was running.\n* If hrtimer_cancel() can be safely called it's safe to call\n* kfree_rcu(t) right after for both preallocated and non-preallocated\n* maps. The async->cb = NULL was already done and no code path can see\n* address 't' anymore. Timer if armed for existing bpf_hrtimer before\n* bpf_timer_cancel_and_free will have been cancelled.\n*/\nhrtimer_cancel(&t->timer);\nkfree_rcu(t, cb.rcu);\n}\n```\n",
 "function_prototype": "void bpf_timer_cancel_and_free(void *val)",
 "re_implemented_code": "\n```c\nvoid bpf_timer_cancel_and_free(void *val)\n{\nstruct bpf_hrtimer *t;\nt = __bpf_async_cancel_and_free((struct bpf_async_kern *) val);\nif (!t)\nreturn;\nif (this_cpu_read(hrtimer_running))\nqueue_work(system_highpri_wq, &t->cb.delete_work);\nelse\nbpf_timer_delete_work(&t->cb.delete_work);\n}\n```\n",
 "is_vulnerable": true
}