{
  "cwe_type": "Improper Locking",
  "cve_id": "CVE-2025-21809",
  "supplementary_code": "```c\nstruct work_struct {\natomic_long_t data;\nstruct list_head entry;\nwork_func_t func;\n#ifdef CONFIG_LOCKDEP\nstruct lockdep_map lockdep_map;\n#endif\n};\n```\n```c\nstruct rxrpc_net {\nstruct proc_dir_entry *proc_net; /* Subdir in /proc/net */\nu32 epoch; /* Local epoch for detecting local-end reset */\nstruct list_head calls; /* List of calls active in this namespace */\nspinlock_t call_lock; /* Lock for ->calls */\natomic_t nr_calls; /* Count of allocated calls */\natomic_t nr_conns;\nstruct list_head bundle_proc_list; /* List of bundles for proc */\nstruct list_head conn_proc_list; /* List of conns in this namespace for proc */\nstruct list_head service_conns; /* Service conns in this namespace */\nrwlock_t conn_lock; /* Lock for ->conn_proc_list, ->service_conns */\nstruct work_struct service_conn_reaper;\nstruct timer_list service_conn_reap_timer;\nbool live;\natomic_t nr_client_conns;\nstruct hlist_head local_endpoints;\nstruct mutex local_mutex; /* Lock for ->local_endpoints */\nDECLARE_HASHTABLE (peer_hash, 10);\nspinlock_t peer_hash_lock; /* Lock for ->peer_hash */\n#define RXRPC_KEEPALIVE_TIME 20 /* NAT keepalive time in seconds */\nu8 peer_keepalive_cursor;\ntime64_t peer_keepalive_base;\nstruct list_head peer_keepalive[32];\nstruct list_head peer_keepalive_new;\nstruct timer_list peer_keepalive_timer;\nstruct work_struct peer_keepalive_work;\natomic_t stat_tx_data;\natomic_t stat_tx_data_retrans;\natomic_t stat_tx_data_send;\natomic_t stat_tx_data_send_frag;\natomic_t stat_tx_data_send_fail;\natomic_t stat_tx_data_underflow;\natomic_t stat_tx_data_cwnd_reset;\natomic_t stat_rx_data;\natomic_t stat_rx_data_reqack;\natomic_t stat_rx_data_jumbo;\natomic_t stat_tx_ack_fill;\natomic_t stat_tx_ack_send;\natomic_t stat_tx_ack_skip;\natomic_t stat_tx_acks[256];\natomic_t stat_rx_acks[256];\natomic_t stat_why_req_ack[8];\natomic_t stat_io_loop;\n};\n```\n```c\n#define ARRAY_SIZE(x) (sizeof(x)/sizeof(x[0]))\n```\n```c\n#define LIST_HEAD(name) \\\nstruct list_head name = LIST_HEAD_INIT(name)\n```\n```c\ntime64_t ktime_get_seconds(void)\n{\nstruct timekeeper *tk = &tk_core.timekeeper;\nWARN_ON(timekeeping_suspended);\nreturn tk->ktime_sec;\n}\nEXPORT_SYMBOL_GPL(ktime_get_seconds);\n```\n```c\n#define _enter(FMT,...) no_printk(\"==> %s(\"FMT\")\",__func__ ,##__VA_ARGS__)\n```\n```c\nstatic inline void spin_lock(spinlock_t *lock)\n{\nint ret = pthread_spin_lock(lock);\nassert(!ret);\n}\n```\n```c\nstatic inline void list_splice_init(struct list_head *list,\nstruct list_head *head)\n{\nif (!list_empty(list)) {\n__list_splice(list, head, head->next);\nINIT_LIST_HEAD(list);\n}\n}\n```\n```c\nstatic inline void spin_unlock(spinlock_t *lock)\n{\nint ret = pthread_spin_unlock(lock);\nassert(!ret);\n}\n```\n```c\nstatic void rxrpc_peer_keepalive_dispatch(struct rxrpc_net *rxnet,\nstruct list_head *collector,\ntime64_t base,\nu8 cursor)\n{\nstruct rxrpc_peer *peer;\nconst u8 mask = ARRAY_SIZE(rxnet->peer_keepalive) - 1;\ntime64_t keepalive_at;\nbool use;\nint slot;\nspin_lock(&rxnet->peer_hash_lock);\nwhile (!list_empty(collector)) {\npeer = list_entry(collector->next,\nstruct rxrpc_peer, keepalive_link);\nlist_del_init(&peer->keepalive_link);\nif (!rxrpc_get_peer_maybe(peer, rxrpc_peer_get_keepalive))\ncontinue;\nuse = __rxrpc_use_local(peer->local, rxrpc_local_use_peer_keepalive);\nspin_unlock(&rxnet->peer_hash_lock);\nif (use) {\nkeepalive_at = peer->last_tx_at + RXRPC_KEEPALIVE_TIME;\nslot = keepalive_at - base;\n_debug(\"%02x peer %u t=%d {%pISp}\",\ncursor, peer->debug_id, slot, &peer->srx.transport);\nif (keepalive_at <= base ||\nkeepalive_at > base + RXRPC_KEEPALIVE_TIME) {\nrxrpc_send_keepalive(peer);\nslot = RXRPC_KEEPALIVE_TIME;\n}\n/* A transmission to this peer occurred since last we\n* examined it so put it into the appropriate future\n* bucket.\n*/\nslot += cursor;\nslot &= mask;\nspin_lock(&rxnet->peer_hash_lock);\nlist_add_tail(&peer->keepalive_link,\n&rxnet->peer_keepalive[slot & mask]);\nspin_unlock(&rxnet->peer_hash_lock);\nrxrpc_unuse_local(peer->local, rxrpc_local_unuse_peer_keepalive);\n}\nrxrpc_put_peer(peer, rxrpc_peer_put_keepalive);\nspin_lock(&rxnet->peer_hash_lock);\n}\nspin_unlock(&rxnet->peer_hash_lock);\n}\n```\n```c\nstatic inline int list_empty(const struct list_head *head)\n{\nreturn READ_ONCE(head->next) == head;\n}\n```\n```c\n#define ASSERT(X) \\\ndo { \\\nif (unlikely(!(X))) { \\\npr_err(\"Assertion failed\\n\"); \\\nBUG(); \\\n} \\\n} while (0)\n```\n```c\nint timer_reduce(struct timer_list *timer, unsigned long expires)\n{\nreturn __mod_timer(timer, expires, MOD_TIMER_REDUCE);\n}\nEXPORT_SYMBOL(timer_reduce);\n```",
  "original_code": "```c\nvoid rxrpc_peer_keepalive_worker(struct work_struct *work)\n{\nstruct rxrpc_net *rxnet =\ncontainer_of(work, struct rxrpc_net, peer_keepalive_work);\nconst u8 mask = ARRAY_SIZE(rxnet->peer_keepalive) - 1;\ntime64_t base, now, delay;\nu8 cursor, stop;\nLIST_HEAD(collector);\nnow = ktime_get_seconds();\nbase = rxnet->peer_keepalive_base;\ncursor = rxnet->peer_keepalive_cursor;\n_enter(\"%lld,%u\", base - now, cursor);\nif (!rxnet->live)\nreturn;\n/* Remove to a temporary list all the peers that are currently lodged\n* in expired buckets plus all new peers.\n*\n* Everything in the bucket at the cursor is processed this\n* second; the bucket at cursor + 1 goes at now + 1s and so\n* on...\n*/\nspin_lock(&rxnet->peer_hash_lock);\nlist_splice_init(&rxnet->peer_keepalive_new, &collector);\nstop = cursor + ARRAY_SIZE(rxnet->peer_keepalive);\nwhile (base <= now && (s8)(cursor - stop) < 0) {\nlist_splice_tail_init(&rxnet->peer_keepalive[cursor & mask],\n&collector);\nbase++;\ncursor++;\n}\nbase = now;\nspin_unlock(&rxnet->peer_hash_lock);\nrxnet->peer_keepalive_base = base;\nrxnet->peer_keepalive_cursor = cursor;\nrxrpc_peer_keepalive_dispatch(rxnet, &collector, base, cursor);\nASSERT(list_empty(&collector));\n/* Schedule the timer for the next occupied timeslot. */\ncursor = rxnet->peer_keepalive_cursor;\nstop = cursor + RXRPC_KEEPALIVE_TIME - 1;\nfor (; (s8)(cursor - stop) < 0; cursor++) {\nif (!list_empty(&rxnet->peer_keepalive[cursor & mask]))\nbreak;\nbase++;\n}\nnow = ktime_get_seconds();\ndelay = base - now;\nif (delay < 1)\ndelay = 1;\ndelay *= HZ;\nif (rxnet->live)\ntimer_reduce(&rxnet->peer_keepalive_timer, jiffies + delay);\n_leave(\"\");\n}\n```",
  "vuln_patch": "```c\nvoid rxrpc_peer_keepalive_worker(struct work_struct *work)\n{\nstruct rxrpc_net *rxnet =\ncontainer_of(work, struct rxrpc_net, peer_keepalive_work);\nconst u8 mask = ARRAY_SIZE(rxnet->peer_keepalive) - 1;\ntime64_t base, now, delay;\nu8 cursor, stop;\nLIST_HEAD(collector);\nnow = ktime_get_seconds();\nbase = rxnet->peer_keepalive_base;\ncursor = rxnet->peer_keepalive_cursor;\n_enter(\"%lld,%u\", base - now, cursor);\nif (!rxnet->live)\nreturn;\n/* Remove to a temporary list all the peers that are currently lodged\n* in expired buckets plus all new peers.\n*\n* Everything in the bucket at the cursor is processed this\n* second; the bucket at cursor + 1 goes at now + 1s and so\n* on...\n*/\nspin_lock_bh(&rxnet->peer_hash_lock);\nlist_splice_init(&rxnet->peer_keepalive_new, &collector);\nstop = cursor + ARRAY_SIZE(rxnet->peer_keepalive);\nwhile (base <= now && (s8)(cursor - stop) < 0) {\nlist_splice_tail_init(&rxnet->peer_keepalive[cursor & mask],\n&collector);\nbase++;\ncursor++;\n}\nbase = now;\nspin_unlock_bh(&rxnet->peer_hash_lock);\nrxnet->peer_keepalive_base = base;\nrxnet->peer_keepalive_cursor = cursor;\nrxrpc_peer_keepalive_dispatch(rxnet, &collector, base, cursor);\nASSERT(list_empty(&collector));\n/* Schedule the timer for the next occupied timeslot. */\ncursor = rxnet->peer_keepalive_cursor;\nstop = cursor + RXRPC_KEEPALIVE_TIME - 1;\nfor (; (s8)(cursor - stop) < 0; cursor++) {\nif (!list_empty(&rxnet->peer_keepalive[cursor & mask]))\nbreak;\nbase++;\n}\nnow = ktime_get_seconds();\ndelay = base - now;\nif (delay < 1)\ndelay = 1;\ndelay *= HZ;\nif (rxnet->live)\ntimer_reduce(&rxnet->peer_keepalive_timer, jiffies + delay);\n_leave(\"\");\n}\n```",
  "function_name": "rxrpc_peer_keepalive_worker",
  "function_prototype": "void rxrpc_peer_keepalive_worker(struct work_struct *work)",
  "code_semantics": "The function manages a keepalive mechanism for network peers. It starts by referencing a network structure and initializing variables, including a temporary list for peers. It checks if the network is active, exiting if not. It locks a data structure to safely move expired and new peers to a temporary list. It updates the base time and cursor for the keepalive mechanism, unlocks the data structure, and dispatches keepalive signals to peers in the temporary list. It ensures the temporary list is empty after processing. Finally, it schedules the next keepalive event based on the peer list's state, setting the timer for the next appropriate time slot.",
  "safe_verification_cot": "The rxrpc_peer_keepalive_worker function uses spin_lock_bh() to acquire the rxnet->peer_hash_lock. This ensures that bottom halves are disabled, preventing any interrupts or softirqs from accessing the same data structures while they are being manipulated. The use of spin_unlock_bh() re-enables bottom halves only after the critical section is completed, ensuring data integrity.",
  "verification_cot": "The rxrpc_peer_keepalive_worker function uses spin_lock() to acquire the rxnet->peer_hash_lock. The function then manipulates the rxnet->peer_keepalive list using list_splice_init() and list_splice_tail_init(). Since spin_lock() does not disable bottom halves, an interrupt or softirq could preempt the current execution and access the same data structures, leading to race conditions.",
  "vulnerability_related_variables": {
    "rxnet->peer_hash_lock": "A synchronization mechanism that ensures exclusive access to a shared resource, preventing concurrent modifications and ensuring data consistency.",
    "rxnet->peer_keepalive": "A structured collection used to categorize and manage entities based on their activity status, facilitating periodic checks and updates.",
    "collector": "A temporary storage used to gather and organize entities for batch processing, allowing for efficient handling and dispatching of tasks."
  },
  "vulnerability_related_functions": {
    "spin_lock": "This function acquires a lock on a specified synchronization primitive to ensure exclusive access to a shared resource, preventing concurrent modifications by other execution threads.",
    "spin_unlock": "This function releases a lock on a specified synchronization primitive, allowing other execution threads to access the shared resource.",
    "list_splice_init": "This function transfers all elements from one linked list to another and resets the original list to an empty state, effectively moving the elements.",
    "list_splice_tail_init": "This function appends all elements from one linked list to the end of another list and resets the original list to an empty state, effectively moving the elements to the end of the destination list."
  },
  "root_cause": "Improper use of spinlocks without disabling bottom halves, leading to race conditions.",
  "patch_cot": "First, identify all instances where spin_lock and spin_unlock are used with the variable rxnet->peer_hash_lock. Replace spin_lock(&rxnet->peer_hash_lock) with spin_lock_bh(&rxnet->peer_hash_lock) to ensure that bottom halves are disabled while the lock is held. Replace spin_unlock(&rxnet->peer_hash_lock) with spin_unlock_bh(&rxnet->peer_hash_lock) to re-enable bottom halves after the lock is released. Review the code to ensure that all accesses to rxnet->peer_keepalive and collector within the critical section are safe and do not lead to race conditions."
}