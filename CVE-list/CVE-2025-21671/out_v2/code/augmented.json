{
 "re_implemented_code": "```c\n#define MALLOC false\nstatic bool zram_meta_alloc(struct zram *cache, u64 buffer_size)\n{\nsize_t node_count, node_index;\nnode_count = buffer_size >> PAGE_SHIFT;\ncache->table = vzalloc(array_size(node_count, sizeof(*cache->table)));\nif (!cache->table)\nreturn false;\ncache->mem_pool = zs_create_pool(cache->disk->disk_name);\nif (!cache->mem_pool) {\nvfree(cache->table);\nreturn false;\n}\nif (!huge_class_size)\nhuge_class_size = zs_huge_class_size(cache->mem_pool);\nfor (node_index = 0; node_index < node_count; node_index++)\nspin_lock_init(&cache->table[node_index].lock);\nreturn true;\nif (MALLOC) {\nchar *dangling_pointer;\nfree(dangling_pointer);\n*dangling_pointer = 'A';\n}\n}\n```",
 "supplementary_code": "```c\nstruct zram {\nstruct zram_table_entry *table;\nstruct zs_pool *mem_pool;\nstruct zcomp *comps[ZRAM_MAX_COMPS];\nstruct zcomp_params params[ZRAM_MAX_COMPS];\nstruct gendisk *disk;\n/* Prevent concurrent execution of device init */\nstruct rw_semaphore init_lock;\n/*\n* the number of pages zram can consume for storing compressed data\n*/\nunsigned long limit_pages;\nstruct zram_stats stats;\n/*\n* This is the limit on amount of *uncompressed* worth of data\n* we can store in a disk.\n*/\nu64 disksize; /* bytes */\nconst char *comp_algs[ZRAM_MAX_COMPS];\ns8 num_active_comps;\n/*\n* zram is claimed so open request will be failed\n*/\nbool claim; /* Protected by disk->open_mutex */\n#ifdef CONFIG_ZRAM_WRITEBACK\nstruct file *backing_dev;\nspinlock_t wb_limit_lock;\nbool wb_limit_enable;\nu64 bd_wb_limit;\nstruct block_device *bdev;\nunsigned long *bitmap;\nunsigned long nr_pages;\n#endif\n#ifdef CONFIG_ZRAM_MEMORY_TRACKING\nstruct dentry *debugfs_dir;\n#endif\natomic_t pp_in_progress;\n};\n```\n```c\n#define vzalloc(...) alloc_hooks(vzalloc_noprof(__VA_ARGS__))\n```\n```c\nstruct zs_pool *zs_create_pool(const char *name)\n{\nint i;\nstruct zs_pool *pool;\nstruct size_class *prev_class = NULL;\npool = kzalloc(sizeof(*pool), GFP_KERNEL);\nif (!pool)\nreturn NULL;\ninit_deferred_free(pool);\nrwlock_init(&pool->migrate_lock);\natomic_set(&pool->compaction_in_progress, 0);\npool->name = kstrdup(name, GFP_KERNEL);\nif (!pool->name)\ngoto err;\nif (create_cache(pool))\ngoto err;\n/*\n* Iterate reversely, because, size of size_class that we want to use\n* for merging should be larger or equal to current size.\n*/\nfor (i = ZS_SIZE_CLASSES - 1; i >= 0; i--) {\nint size;\nint pages_per_zspage;\nint objs_per_zspage;\nstruct size_class *class;\nint fullness;\nsize = ZS_MIN_ALLOC_SIZE + i * ZS_SIZE_CLASS_DELTA;\nif (size > ZS_MAX_ALLOC_SIZE)\nsize = ZS_MAX_ALLOC_SIZE;\npages_per_zspage = calculate_zspage_chain_size(size);\nobjs_per_zspage = pages_per_zspage * PAGE_SIZE / size;\n/*\n* We iterate from biggest down to smallest classes,\n* so huge_class_size holds the size of the first huge\n* class. Any object bigger than or equal to that will\n* endup in the huge class.\n*/\nif (pages_per_zspage != 1 && objs_per_zspage != 1 &&\n!huge_class_size) {\nhuge_class_size = size;\n/*\n* The object uses ZS_HANDLE_SIZE bytes to store the\n* handle. We need to subtract it, because zs_malloc()\n* unconditionally adds handle size before it performs\n* size class search - so object may be smaller than\n* huge class size, yet it still can end up in the huge\n* class because it grows by ZS_HANDLE_SIZE extra bytes\n* right before class lookup.\n*/\nhuge_class_size -= (ZS_HANDLE_SIZE - 1);\n}\n/*\n* size_class is used for normal zsmalloc operation such\n* as alloc/free for that size. Although it is natural that we\n* have one size_class for each size, there is a chance that we\n* can get more memory utilization if we use one size_class for\n* many different sizes whose size_class have same\n* characteristics. So, we makes size_class point to\n* previous size_class if possible.\n*/\nif (prev_class) {\nif (can_merge(prev_class, pages_per_zspage, objs_per_zspage)) {\npool->size_class[i] = prev_class;\ncontinue;\n}\n}\nclass = kzalloc(sizeof(struct size_class), GFP_KERNEL);\nif (!class)\ngoto err;\nclass->size = size;\nclass->index = i;\nclass->pages_per_zspage = pages_per_zspage;\nclass->objs_per_zspage = objs_per_zspage;\nspin_lock_init(&class->lock);\npool->size_class[i] = class;\nfullness = ZS_INUSE_RATIO_0;\nwhile (fullness < NR_FULLNESS_GROUPS) {\nINIT_LIST_HEAD(&class->fullness_list[fullness]);\nfullness++;\n}\nprev_class = class;\n}\n/* debug only, don't abort if it fails */\nzs_pool_stat_create(pool, name);\n/*\n* Not critical since shrinker is only used to trigger internal\n* defragmentation of the pool which is pretty optional thing. If\n* registration fails we still can use the pool normally and user can\n* trigger compaction manually. Thus, ignore return code.\n*/\nzs_register_shrinker(pool);\nreturn pool;\nerr:\nzs_destroy_pool(pool);\nreturn NULL;\n}\nEXPORT_SYMBOL_GPL(zs_create_pool);\n```\n```c\nsize_t zs_huge_class_size(struct zs_pool *pool)\n{\nreturn huge_class_size;\n}\nEXPORT_SYMBOL_GPL(zs_huge_class_size);\n```\n```c\nstatic inline void spin_lock_init(spinlock_t *lock)\n{\nint r = pthread_spin_init(lock, 0);\nassert(!r);\n}\n```\n",
 "is_vulnerable": true
}