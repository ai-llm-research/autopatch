{
  "cwe_type": "Use After Free",
  "cve_id": "CVE-2025-21671",
  "supplementary_code": "```c\nstruct zram {\nstruct zram_table_entry *table;\nstruct zs_pool *mem_pool;\nstruct zcomp *comps[ZRAM_MAX_COMPS];\nstruct zcomp_params params[ZRAM_MAX_COMPS];\nstruct gendisk *disk;\n/* Prevent concurrent execution of device init */\nstruct rw_semaphore init_lock;\n/*\n* the number of pages zram can consume for storing compressed data\n*/\nunsigned long limit_pages;\nstruct zram_stats stats;\n/*\n* This is the limit on amount of *uncompressed* worth of data\n* we can store in a disk.\n*/\nu64 disksize; /* bytes */\nconst char *comp_algs[ZRAM_MAX_COMPS];\ns8 num_active_comps;\n/*\n* zram is claimed so open request will be failed\n*/\nbool claim; /* Protected by disk->open_mutex */\n#ifdef CONFIG_ZRAM_WRITEBACK\nstruct file *backing_dev;\nspinlock_t wb_limit_lock;\nbool wb_limit_enable;\nu64 bd_wb_limit;\nstruct block_device *bdev;\nunsigned long *bitmap;\nunsigned long nr_pages;\n#endif\n#ifdef CONFIG_ZRAM_MEMORY_TRACKING\nstruct dentry *debugfs_dir;\n#endif\natomic_t pp_in_progress;\n};\n```\n```c\n#define vzalloc(...) alloc_hooks(vzalloc_noprof(__VA_ARGS__))\n```\n```c\nstruct zs_pool *zs_create_pool(const char *name)\n{\nint i;\nstruct zs_pool *pool;\nstruct size_class *prev_class = NULL;\npool = kzalloc(sizeof(*pool), GFP_KERNEL);\nif (!pool)\nreturn NULL;\ninit_deferred_free(pool);\nrwlock_init(&pool->migrate_lock);\natomic_set(&pool->compaction_in_progress, 0);\npool->name = kstrdup(name, GFP_KERNEL);\nif (!pool->name)\ngoto err;\nif (create_cache(pool))\ngoto err;\n/*\n* Iterate reversely, because, size of size_class that we want to use\n* for merging should be larger or equal to current size.\n*/\nfor (i = ZS_SIZE_CLASSES - 1; i >= 0; i--) {\nint size;\nint pages_per_zspage;\nint objs_per_zspage;\nstruct size_class *class;\nint fullness;\nsize = ZS_MIN_ALLOC_SIZE + i * ZS_SIZE_CLASS_DELTA;\nif (size > ZS_MAX_ALLOC_SIZE)\nsize = ZS_MAX_ALLOC_SIZE;\npages_per_zspage = calculate_zspage_chain_size(size);\nobjs_per_zspage = pages_per_zspage * PAGE_SIZE / size;\n/*\n* We iterate from biggest down to smallest classes,\n* so huge_class_size holds the size of the first huge\n* class. Any object bigger than or equal to that will\n* endup in the huge class.\n*/\nif (pages_per_zspage != 1 && objs_per_zspage != 1 &&\n!huge_class_size) {\nhuge_class_size = size;\n/*\n* The object uses ZS_HANDLE_SIZE bytes to store the\n* handle. We need to subtract it, because zs_malloc()\n* unconditionally adds handle size before it performs\n* size class search - so object may be smaller than\n* huge class size, yet it still can end up in the huge\n* class because it grows by ZS_HANDLE_SIZE extra bytes\n* right before class lookup.\n*/\nhuge_class_size -= (ZS_HANDLE_SIZE - 1);\n}\n/*\n* size_class is used for normal zsmalloc operation such\n* as alloc/free for that size. Although it is natural that we\n* have one size_class for each size, there is a chance that we\n* can get more memory utilization if we use one size_class for\n* many different sizes whose size_class have same\n* characteristics. So, we makes size_class point to\n* previous size_class if possible.\n*/\nif (prev_class) {\nif (can_merge(prev_class, pages_per_zspage, objs_per_zspage)) {\npool->size_class[i] = prev_class;\ncontinue;\n}\n}\nclass = kzalloc(sizeof(struct size_class), GFP_KERNEL);\nif (!class)\ngoto err;\nclass->size = size;\nclass->index = i;\nclass->pages_per_zspage = pages_per_zspage;\nclass->objs_per_zspage = objs_per_zspage;\nspin_lock_init(&class->lock);\npool->size_class[i] = class;\nfullness = ZS_INUSE_RATIO_0;\nwhile (fullness < NR_FULLNESS_GROUPS) {\nINIT_LIST_HEAD(&class->fullness_list[fullness]);\nfullness++;\n}\nprev_class = class;\n}\n/* debug only, don't abort if it fails */\nzs_pool_stat_create(pool, name);\n/*\n* Not critical since shrinker is only used to trigger internal\n* defragmentation of the pool which is pretty optional thing. If\n* registration fails we still can use the pool normally and user can\n* trigger compaction manually. Thus, ignore return code.\n*/\nzs_register_shrinker(pool);\nreturn pool;\nerr:\nzs_destroy_pool(pool);\nreturn NULL;\n}\nEXPORT_SYMBOL_GPL(zs_create_pool);\n```\n```c\nsize_t zs_huge_class_size(struct zs_pool *pool)\n{\nreturn huge_class_size;\n}\nEXPORT_SYMBOL_GPL(zs_huge_class_size);\n```\n```c\nstatic inline void spin_lock_init(spinlock_t *lock)\n{\nint r = pthread_spin_init(lock, 0);\nassert(!r);\n}\n```",
  "original_code": "```c\nstatic bool zram_meta_alloc(struct zram *zram, u64 disksize)\n{\nsize_t num_pages, index;\nnum_pages = disksize >> PAGE_SHIFT;\nzram->table = vzalloc(array_size(num_pages, sizeof(*zram->table)));\nif (!zram->table)\nreturn false;\nzram->mem_pool = zs_create_pool(zram->disk->disk_name);\nif (!zram->mem_pool) {\nvfree(zram->table);\nreturn false;\n}\nif (!huge_class_size)\nhuge_class_size = zs_huge_class_size(zram->mem_pool);\nfor (index = 0; index < num_pages; index++)\nspin_lock_init(&zram->table[index].lock);\nreturn true;\n}\n```",
  "vuln_patch": "```c\nstatic bool zram_meta_alloc(struct zram *zram, u64 disksize)\n{\nsize_t num_pages, index;\nnum_pages = disksize >> PAGE_SHIFT;\nzram->table = vzalloc(array_size(num_pages, sizeof(*zram->table)));\nif (!zram->table)\nreturn false;\nzram->mem_pool = zs_create_pool(zram->disk->disk_name);\nif (!zram->mem_pool) {\nvfree(zram->table);\nzram->table = NULL;\nreturn false;\n}\nif (!huge_class_size)\nhuge_class_size = zs_huge_class_size(zram->mem_pool);\nfor (index = 0; index < num_pages; index++)\nspin_lock_init(&zram->table[index].lock);\nreturn true;\n}\n```",
  "function_name": "zram_meta_alloc",
  "function_prototype": "static bool zram_meta_alloc(struct zram *zram, u64 disksize)",
  "code_semantics": "The function calculates the number of pages required based on a given storage size and allocates memory for a metadata table to manage these pages. It then creates a memory pool associated with the storage medium's name. If the memory pool creation fails, it releases the allocated memory and returns a failure indicator. If a threshold for large memory allocations is not set, it retrieves and sets this threshold. The function initializes synchronization primitives for each entry in the metadata table to ensure safe concurrent access. It returns a success indicator if all operations are successful.",
  "safe_verification_cot": "1. In the Target Code, the handling of zs_create_pool is the same as in the Vulnerable Code.\n2. In the Target Code, after vfree(zram->table), zram->table is set to NULL. This prevents any use-after-free condition because any subsequent access to zram->table will be a NULL pointer dereference, which is safer than accessing freed memory.",
  "verification_cot": "1. In the Vulnerable Code, if zs_create_pool fails, zram->mem_pool is NULL, and the error handling block is executed.\n2. In the Vulnerable Code, zram->table is freed using vfree, but it is not set to NULL. This can lead to a use-after-free condition if zram->table is accessed after this point.",
  "vulnerability_related_variables": {
    "zram->table": "This variable is used to store a dynamically allocated array of structures. Each element in the array represents a unit of data, and the array is sized based on the number of pages calculated from a given size. The array is initialized with locks for concurrent access control.",
    "zram->mem_pool": "This variable is used to store a reference to a memory pool that is created based on a given name. The memory pool is used for managing memory allocations and deallocations efficiently, and it may influence the configuration of other system parameters."
  },
  "vulnerability_related_functions": {
    "zs_create_pool": "This function creates a memory pool structure by allocating memory and initializing its fields, including setting up size classes for memory allocation. It registers the pool for memory management operations and sets up debugging information. If any step fails, it cleans up and returns a null pointer.",
    "vfree": "This function deallocates memory that was previously allocated. It takes a pointer to the memory to be freed and releases the resources associated with it, ensuring that the system can reuse the memory."
  },
  "root_cause": "The root cause of CVE-2025-21671 is the failure to set zram->table to NULL after it has been freed, which can lead to a use-after-free condition if the pointer is accessed again.",
  "patch_cot": "First, identify where zram->table is being freed using vfree. Ensure that immediately after this operation, zram->table is set to NULL to prevent any further access to a freed memory location. Check the return value of zs_create_pool when assigning to zram->mem_pool. If zs_create_pool returns NULL, ensure that any resources allocated before this call are properly freed and set to NULL to prevent memory leaks and dangling pointers. Review the code to ensure that all pointers that are freed are subsequently set to NULL to prevent use-after-free vulnerabilities."
}