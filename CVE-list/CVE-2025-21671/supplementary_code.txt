```c
struct zram {
    struct zram_table_entry *table;
    struct zs_pool *mem_pool;
    struct zcomp *comps[ZRAM_MAX_COMPS];
    struct zcomp_params params[ZRAM_MAX_COMPS];
    struct gendisk *disk;
    /* Prevent concurrent execution of device init */
    struct rw_semaphore init_lock;
    /*
     * the number of pages zram can consume for storing compressed data
     */
    unsigned long limit_pages;

    struct zram_stats stats;
    /*
     * This is the limit on amount of *uncompressed* worth of data
     * we can store in a disk.
     */
    u64 disksize;   /* bytes */
    const char *comp_algs[ZRAM_MAX_COMPS];
    s8 num_active_comps;
    /*
     * zram is claimed so open request will be failed
     */
    bool claim; /* Protected by disk->open_mutex */
#ifdef CONFIG_ZRAM_WRITEBACK
    struct file *backing_dev;
    spinlock_t wb_limit_lock;
    bool wb_limit_enable;
    u64 bd_wb_limit;
    struct block_device *bdev;
    unsigned long *bitmap;
    unsigned long nr_pages;
#endif
#ifdef CONFIG_ZRAM_MEMORY_TRACKING
    struct dentry *debugfs_dir;
#endif
    atomic_t pp_in_progress;
};
```

```c
#define vzalloc(...)        alloc_hooks(vzalloc_noprof(__VA_ARGS__))
```

```c
struct zs_pool *zs_create_pool(const char *name)
{
    int i;
    struct zs_pool *pool;
    struct size_class *prev_class = NULL;

    pool = kzalloc(sizeof(*pool), GFP_KERNEL);
    if (!pool)
        return NULL;

    init_deferred_free(pool);
    rwlock_init(&pool->migrate_lock);
    atomic_set(&pool->compaction_in_progress, 0);

    pool->name = kstrdup(name, GFP_KERNEL);
    if (!pool->name)
        goto err;

    if (create_cache(pool))
        goto err;

    /*
     * Iterate reversely, because, size of size_class that we want to use
     * for merging should be larger or equal to current size.
     */
    for (i = ZS_SIZE_CLASSES - 1; i >= 0; i--) {
        int size;
        int pages_per_zspage;
        int objs_per_zspage;
        struct size_class *class;
        int fullness;

        size = ZS_MIN_ALLOC_SIZE + i * ZS_SIZE_CLASS_DELTA;
        if (size > ZS_MAX_ALLOC_SIZE)
            size = ZS_MAX_ALLOC_SIZE;
        pages_per_zspage = calculate_zspage_chain_size(size);
        objs_per_zspage = pages_per_zspage * PAGE_SIZE / size;

        /*
         * We iterate from biggest down to smallest classes,
         * so huge_class_size holds the size of the first huge
         * class. Any object bigger than or equal to that will
         * endup in the huge class.
         */
        if (pages_per_zspage != 1 && objs_per_zspage != 1 &&
                !huge_class_size) {
            huge_class_size = size;
            /*
             * The object uses ZS_HANDLE_SIZE bytes to store the
             * handle. We need to subtract it, because zs_malloc()
             * unconditionally adds handle size before it performs
             * size class search - so object may be smaller than
             * huge class size, yet it still can end up in the huge
             * class because it grows by ZS_HANDLE_SIZE extra bytes
             * right before class lookup.
             */
            huge_class_size -= (ZS_HANDLE_SIZE - 1);
        }

        /*
         * size_class is used for normal zsmalloc operation such
         * as alloc/free for that size. Although it is natural that we
         * have one size_class for each size, there is a chance that we
         * can get more memory utilization if we use one size_class for
         * many different sizes whose size_class have same
         * characteristics. So, we makes size_class point to
         * previous size_class if possible.
         */
        if (prev_class) {
            if (can_merge(prev_class, pages_per_zspage, objs_per_zspage)) {
                pool->size_class[i] = prev_class;
                continue;
            }
        }

        class = kzalloc(sizeof(struct size_class), GFP_KERNEL);
        if (!class)
            goto err;

        class->size = size;
        class->index = i;
        class->pages_per_zspage = pages_per_zspage;
        class->objs_per_zspage = objs_per_zspage;
        spin_lock_init(&class->lock);
        pool->size_class[i] = class;

        fullness = ZS_INUSE_RATIO_0;
        while (fullness < NR_FULLNESS_GROUPS) {
            INIT_LIST_HEAD(&class->fullness_list[fullness]);
            fullness++;
        }

        prev_class = class;
    }

    /* debug only, don't abort if it fails */
    zs_pool_stat_create(pool, name);

    /*
     * Not critical since shrinker is only used to trigger internal
     * defragmentation of the pool which is pretty optional thing.  If
     * registration fails we still can use the pool normally and user can
     * trigger compaction manually. Thus, ignore return code.
     */
    zs_register_shrinker(pool);

    return pool;

err:
    zs_destroy_pool(pool);
    return NULL;
}
EXPORT_SYMBOL_GPL(zs_create_pool);
```

```c
size_t zs_huge_class_size(struct zs_pool *pool)
{
    return huge_class_size;
}
EXPORT_SYMBOL_GPL(zs_huge_class_size);

```

```c
static inline void spin_lock_init(spinlock_t *lock)
{
    int r = pthread_spin_init(lock, 0);
    assert(!r);
}
```
