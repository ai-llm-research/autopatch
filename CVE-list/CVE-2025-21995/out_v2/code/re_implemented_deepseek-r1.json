{
 "supplementary_code": "```c\nstruct drm_sched_entity {\n/**\n* @list:\n*\n* Used to append this struct to the list of entities in the runqueue\n* @rq under &drm_sched_rq.entities.\n*\n* Protected by &drm_sched_rq.lock of @rq.\n*/\nstruct list_head list;\n/**\n* @lock:\n*\n* Lock protecting the run-queue (@rq) to which this entity belongs,\n* @priority and the list of schedulers (@sched_list, @num_sched_list).\n*/\nspinlock_t lock;\n/**\n* @rq:\n*\n* Runqueue on which this entity is currently scheduled.\n*\n* FIXME: Locking is very unclear for this. Writers are protected by\n* @lock, but readers are generally lockless and seem to just race with\n* not even a READ_ONCE.\n*/\nstruct drm_sched_rq *rq;\n/**\n* @sched_list:\n*\n* A list of schedulers (struct drm_gpu_scheduler). Jobs from this entity can\n* be scheduled on any scheduler on this list.\n*\n* This can be modified by calling drm_sched_entity_modify_sched().\n* Locking is entirely up to the driver, see the above function for more\n* details.\n*\n* This will be set to NULL if &num_sched_list equals 1 and @rq has been\n* set already.\n*\n* FIXME: This means priority changes through\n* drm_sched_entity_set_priority() will be lost henceforth in this case.\n*/\nstruct drm_gpu_scheduler **sched_list;\n/**\n* @num_sched_list:\n*\n* Number of drm_gpu_schedulers in the @sched_list.\n*/\nunsigned int num_sched_list;\n/**\n* @priority:\n*\n* Priority of the entity. This can be modified by calling\n* drm_sched_entity_set_priority(). Protected by @lock.\n*/\nenum drm_sched_priority priority;\n/**\n* @job_queue: the list of jobs of this entity.\n*/\nstruct spsc_queue job_queue;\n/**\n* @fence_seq:\n*\n* A linearly increasing seqno incremented with each new\n* &drm_sched_fence which is part of the entity.\n*\n* FIXME: Callers of drm_sched_job_arm() need to ensure correct locking,\n* this doesn't need to be atomic.\n*/\natomic_t fence_seq;\n/**\n* @fence_context:\n*\n* A unique context for all the fences which belong to this entity. The\n* &drm_sched_fence.scheduled uses the fence_context but\n* &drm_sched_fence.finished uses fence_context + 1.\n*/\nuint64_t fence_context;\n/**\n* @dependency:\n*\n* The dependency fence of the job which is on the top of the job queue.\n*/\nstruct dma_fence *dependency;\n/**\n* @cb:\n*\n* Callback for the dependency fence above.\n*/\nstruct dma_fence_cb cb;\n/**\n* @guilty:\n*\n* Points to entities' guilty.\n*/\natomic_t *guilty;\n/**\n* @last_scheduled:\n*\n* Points to the finished fence of the last scheduled job. Only written\n* by the scheduler thread, can be accessed locklessly from\n* drm_sched_job_arm() if the queue is empty.\n*/\nstruct dma_fence __rcu *last_scheduled;\n/**\n* @last_user: last group leader pushing a job into the entity.\n*/\nstruct task_struct *last_user;\n/**\n* @stopped:\n*\n* Marks the enity as removed from rq and destined for\n* termination. This is set by calling drm_sched_entity_flush() and by\n* drm_sched_fini().\n*/\nbool stopped;\n/**\n* @entity_idle:\n*\n* Signals when entity is not in use, used to sequence entity cleanup in\n* drm_sched_entity_fini().\n*/\nstruct completion entity_idle;\n/**\n* @oldest_job_waiting:\n*\n* Marks earliest job waiting in SW queue\n*/\nktime_t oldest_job_waiting;\n/**\n* @rb_tree_node:\n*\n* The node used to insert this entity into time based priority queue\n*/\nstruct rb_node rb_tree_node;\n};\n```\n```c\nstruct drm_sched_job {\nstruct spsc_node queue_node;\nstruct list_head list;\n/**\n* @sched:\n*\n* The scheduler this job is or will be scheduled on. Gets set by\n* drm_sched_job_arm(). Valid until drm_sched_backend_ops.free_job()\n* has finished.\n*/\nstruct drm_gpu_scheduler *sched;\nstruct drm_sched_fence *s_fence;\nu32 credits;\n/*\n* work is used only after finish_cb has been used and will not be\n* accessed anymore.\n*/\nunion {\nstruct dma_fence_cb finish_cb;\nstruct work_struct work;\n};\nuint64_t id;\natomic_t karma;\nenum drm_sched_priority s_priority;\nstruct drm_sched_entity *entity;\nstruct dma_fence_cb cb;\n/**\n* @dependencies:\n*\n* Contains the dependencies as struct dma_fence for this job, see\n* drm_sched_job_add_dependency() and\n* drm_sched_job_add_implicit_dependencies().\n*/\nstruct xarray dependencies;\n/** @last_dependency: tracks @dependencies as they signal */\nunsigned long last_dependency;\n/**\n* @submit_ts:\n*\n* When the job was pushed into the entity queue.\n*/\nktime_t submit_ts;\n};\n```\n```c\nstruct dma_fence {\nspinlock_t *lock;\nconst struct dma_fence_ops *ops;\n/*\n* We clear the callback list on kref_put so that by the time we\n* release the fence it is unused. No one should be adding to the\n* cb_list that they don't themselves hold a reference for.\n*\n* The lifetime of the timestamp is similarly tied to both the\n* rcu freelist and the cb_list. The timestamp is only set upon\n* signaling while simultaneously notifying the cb_list. Ergo, we\n* only use either the cb_list of timestamp. Upon destruction,\n* neither are accessible, and so we can use the rcu. This means\n* that the cb_list is *only* valid until the signal bit is set,\n* and to read either you *must* hold a reference to the fence,\n* and not just the rcu_read_lock.\n*\n* Listed in chronological order.\n*/\nunion {\nstruct list_head cb_list;\n/* @cb_list replaced by @timestamp on dma_fence_signal() */\nktime_t timestamp;\n/* @timestamp replaced by @rcu on dma_fence_release() */\nstruct rcu_head rcu;\n};\nu64 context;\nu64 seqno;\nunsigned long flags;\nstruct kref refcount;\nint error;\n};\n```\n```c\nstatic __always_inline void spin_lock(spinlock_t *lock)\n{\nraw_spin_lock(&lock->rlock);\n}\n```\n```c\nvoid drm_sched_rq_remove_entity(struct drm_sched_rq *rq,\nstruct drm_sched_entity *entity)\n{\nlockdep_assert_held(&entity->lock);\nif (list_empty(&entity->list))\nreturn;\nspin_lock(&rq->lock);\natomic_dec(rq->sched->score);\nlist_del_init(&entity->list);\nif (rq->current_entity == entity)\nrq->current_entity = NULL;\nif (drm_sched_policy == DRM_SCHED_POLICY_FIFO)\ndrm_sched_rq_remove_fifo_locked(entity, rq);\nspin_unlock(&rq->lock);\n}\n```\n```c\nstatic __always_inline void spin_unlock(spinlock_t *lock)\n{\nraw_spin_unlock(&lock->rlock);\n}\n```\n```c\nvoid __sched wait_for_completion(struct completion *x)\n{\nwait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE);\n}\nEXPORT_SYMBOL(wait_for_completion);\n```\n```c\n#define rcu_dereference_check(p, c) \\\n__rcu_dereference_check((p), __UNIQUE_ID(rcu), \\\n(c) || rcu_read_lock_held(), __rcu)\n```\n```c\nstatic inline struct dma_fence *dma_fence_get(struct dma_fence *fence)\n{\nif (fence)\nkref_get(&fence->refcount);\nreturn fence;\n}\n```\n```c\nstatic inline struct spsc_node *spsc_queue_pop(struct spsc_queue *queue)\n{\nstruct spsc_node *next, *node;\n/* Verify reading from memory and not the cache */\nsmp_rmb();\nnode = READ_ONCE(queue->head);\nif (!node)\nreturn NULL;\nnext = READ_ONCE(node->next);\nWRITE_ONCE(queue->head, next);\nif (unlikely(!next)) {\n/* slowpath for the last element in the queue */\nif (atomic_long_cmpxchg(&queue->tail,\n(long)&node->next, (long) &queue->head) != (long)&node->next) {\n/* Updating tail failed wait for new next to appear */\ndo {\nsmp_rmb();\n} while (unlikely(!(queue->head = READ_ONCE(node->next))));\n}\n}\natomic_dec(&queue->job_count);\nreturn node;\n}\n```\n```c\nstruct drm_sched_fence {\n/**\n* @scheduled: this fence is what will be signaled by the scheduler\n* when the job is scheduled.\n*/\nstruct dma_fence scheduled;\n/**\n* @finished: this fence is what will be signaled by the scheduler\n* when the job is completed.\n*\n* When setting up an out fence for the job, you should use\n* this, since it's available immediately upon\n* drm_sched_job_init(), and the fence returned by the driver\n* from run_job() won't be created until the dependencies have\n* resolved.\n*/\nstruct dma_fence finished;\n/**\n* @deadline: deadline set on &drm_sched_fence.finished which\n* potentially needs to be propagated to &drm_sched_fence.parent\n*/\nktime_t deadline;\n/**\n* @parent: the fence returned by &drm_sched_backend_ops.run_job\n* when scheduling the job on hardware. We signal the\n* &drm_sched_fence.finished fence once parent is signalled.\n*/\nstruct dma_fence *parent;\n/**\n* @sched: the scheduler instance to which the job having this struct\n* belongs to.\n*/\nstruct drm_gpu_scheduler *sched;\n/**\n* @lock: the lock used by the scheduled and the finished fences.\n*/\nspinlock_t lock;\n/**\n* @owner: job owner for debugging\n*/\nvoid *owner;\n};\n```\n```c\nint dma_fence_add_callback(struct dma_fence *fence, struct dma_fence_cb *cb,\ndma_fence_func_t func)\n{\nunsigned long flags;\nint ret = 0;\nif (WARN_ON(!fence || !func))\nreturn -EINVAL;\nif (test_bit(DMA_FENCE_FLAG_SIGNALED_BIT, &fence->flags)) {\nINIT_LIST_HEAD(&cb->node);\nreturn -ENOENT;\n}\nspin_lock_irqsave(fence->lock, flags);\nif (__dma_fence_enable_signaling(fence)) {\ncb->func = func;\nlist_add_tail(&cb->node, &fence->cb_list);\n} else {\nINIT_LIST_HEAD(&cb->node);\nret = -ENOENT;\n}\nspin_unlock_irqrestore(fence->lock, flags);\nreturn ret;\n}\nEXPORT_SYMBOL(dma_fence_add_callback);\n```\n```c\nstatic void drm_sched_entity_kill_jobs_cb(struct dma_fence *f,\nstruct dma_fence_cb *cb)\n{\nstruct drm_sched_job *job = container_of(cb, struct drm_sched_job,\nfinish_cb);\nunsigned long index;\ndma_fence_put(f);\n/* Wait for all dependencies to avoid data corruptions */\nxa_for_each(&job->dependencies, index, f) {\nstruct drm_sched_fence *s_fence = to_drm_sched_fence(f);\nif (s_fence && f == &s_fence->scheduled) {\n/* The dependencies array had a reference on the scheduled\n* fence, and the finished fence refcount might have\n* dropped to zero. Use dma_fence_get_rcu() so we get\n* a NULL fence in that case.\n*/\nf = dma_fence_get_rcu(&s_fence->finished);\n/* Now that we have a reference on the finished fence,\n* we can release the reference the dependencies array\n* had on the scheduled fence.\n*/\ndma_fence_put(&s_fence->scheduled);\n}\nxa_erase(&job->dependencies, index);\nif (f && !dma_fence_add_callback(f, &job->finish_cb,\ndrm_sched_entity_kill_jobs_cb))\nreturn;\ndma_fence_put(f);\n}\nINIT_WORK(&job->work, drm_sched_entity_kill_jobs_work);\nschedule_work(&job->work);\n}\n```\n```c\nstatic inline void dma_fence_put(struct dma_fence *fence)\n{\nif (fence)\nkref_put(&fence->refcount, dma_fence_release);\n}\n```\n",
 "function_prototype": "static void drm_sched_entity_kill(struct drm_sched_entity *entity)",
 "re_implemented_code": "\n```c\nstatic void drm_sched_entity_kill(struct drm_sched_entity *entity)\n{\nstruct drm_sched_job *job;\nstruct dma_fence *prev = NULL;\nif (!entity->rq)\nreturn;\nspin_lock(&entity->rq->lock);\nentity->stopped = true;\ndrm_sched_rq_remove_entity(entity->rq, entity);\nspin_unlock(&entity->rq->lock);\nwait_for_completion(&entity->entity_idle);\nprev = rcu_dereference(entity->last_scheduled);\nif (prev)\nprev = dma_fence_get(prev);\nwhile ((job = to_drm_sched_job(spsc_queue_pop(&entity->job_queue)))) {\nstruct dma_fence *fence = &job->s_fence->finished;\nfence = dma_fence_get(fence);\nif (!prev || dma_fence_add_callback(prev, &job->finish_cb,\ndrm_sched_entity_kill_jobs_cb)) {\ndrm_sched_entity_kill_jobs_cb(prev, &job->finish_cb);\n}\nprev = fence;\n}\nif (prev)\ndma_fence_put(prev);\n}\n```\n",
 "is_vulnerable": true
}