{
  "cwe_type": "Memory Leak",
  "cve_id": "CVE-2025-21995",
  "supplementary_code": "```c\nstruct drm_sched_entity {\n/**\n* @list:\n*\n* Used to append this struct to the list of entities in the runqueue\n* @rq under &drm_sched_rq.entities.\n*\n* Protected by &drm_sched_rq.lock of @rq.\n*/\nstruct list_head list;\n/**\n* @lock:\n*\n* Lock protecting the run-queue (@rq) to which this entity belongs,\n* @priority and the list of schedulers (@sched_list, @num_sched_list).\n*/\nspinlock_t lock;\n/**\n* @rq:\n*\n* Runqueue on which this entity is currently scheduled.\n*\n* FIXME: Locking is very unclear for this. Writers are protected by\n* @lock, but readers are generally lockless and seem to just race with\n* not even a READ_ONCE.\n*/\nstruct drm_sched_rq *rq;\n/**\n* @sched_list:\n*\n* A list of schedulers (struct drm_gpu_scheduler). Jobs from this entity can\n* be scheduled on any scheduler on this list.\n*\n* This can be modified by calling drm_sched_entity_modify_sched().\n* Locking is entirely up to the driver, see the above function for more\n* details.\n*\n* This will be set to NULL if &num_sched_list equals 1 and @rq has been\n* set already.\n*\n* FIXME: This means priority changes through\n* drm_sched_entity_set_priority() will be lost henceforth in this case.\n*/\nstruct drm_gpu_scheduler **sched_list;\n/**\n* @num_sched_list:\n*\n* Number of drm_gpu_schedulers in the @sched_list.\n*/\nunsigned int num_sched_list;\n/**\n* @priority:\n*\n* Priority of the entity. This can be modified by calling\n* drm_sched_entity_set_priority(). Protected by @lock.\n*/\nenum drm_sched_priority priority;\n/**\n* @job_queue: the list of jobs of this entity.\n*/\nstruct spsc_queue job_queue;\n/**\n* @fence_seq:\n*\n* A linearly increasing seqno incremented with each new\n* &drm_sched_fence which is part of the entity.\n*\n* FIXME: Callers of drm_sched_job_arm() need to ensure correct locking,\n* this doesn't need to be atomic.\n*/\natomic_t fence_seq;\n/**\n* @fence_context:\n*\n* A unique context for all the fences which belong to this entity. The\n* &drm_sched_fence.scheduled uses the fence_context but\n* &drm_sched_fence.finished uses fence_context + 1.\n*/\nuint64_t fence_context;\n/**\n* @dependency:\n*\n* The dependency fence of the job which is on the top of the job queue.\n*/\nstruct dma_fence *dependency;\n/**\n* @cb:\n*\n* Callback for the dependency fence above.\n*/\nstruct dma_fence_cb cb;\n/**\n* @guilty:\n*\n* Points to entities' guilty.\n*/\natomic_t *guilty;\n/**\n* @last_scheduled:\n*\n* Points to the finished fence of the last scheduled job. Only written\n* by the scheduler thread, can be accessed locklessly from\n* drm_sched_job_arm() if the queue is empty.\n*/\nstruct dma_fence __rcu *last_scheduled;\n/**\n* @last_user: last group leader pushing a job into the entity.\n*/\nstruct task_struct *last_user;\n/**\n* @stopped:\n*\n* Marks the enity as removed from rq and destined for\n* termination. This is set by calling drm_sched_entity_flush() and by\n* drm_sched_fini().\n*/\nbool stopped;\n/**\n* @entity_idle:\n*\n* Signals when entity is not in use, used to sequence entity cleanup in\n* drm_sched_entity_fini().\n*/\nstruct completion entity_idle;\n/**\n* @oldest_job_waiting:\n*\n* Marks earliest job waiting in SW queue\n*/\nktime_t oldest_job_waiting;\n/**\n* @rb_tree_node:\n*\n* The node used to insert this entity into time based priority queue\n*/\nstruct rb_node rb_tree_node;\n};\n```\n```c\nstruct drm_sched_job {\nstruct spsc_node queue_node;\nstruct list_head list;\n/**\n* @sched:\n*\n* The scheduler this job is or will be scheduled on. Gets set by\n* drm_sched_job_arm(). Valid until drm_sched_backend_ops.free_job()\n* has finished.\n*/\nstruct drm_gpu_scheduler *sched;\nstruct drm_sched_fence *s_fence;\nu32 credits;\n/*\n* work is used only after finish_cb has been used and will not be\n* accessed anymore.\n*/\nunion {\nstruct dma_fence_cb finish_cb;\nstruct work_struct work;\n};\nuint64_t id;\natomic_t karma;\nenum drm_sched_priority s_priority;\nstruct drm_sched_entity *entity;\nstruct dma_fence_cb cb;\n/**\n* @dependencies:\n*\n* Contains the dependencies as struct dma_fence for this job, see\n* drm_sched_job_add_dependency() and\n* drm_sched_job_add_implicit_dependencies().\n*/\nstruct xarray dependencies;\n/** @last_dependency: tracks @dependencies as they signal */\nunsigned long last_dependency;\n/**\n* @submit_ts:\n*\n* When the job was pushed into the entity queue.\n*/\nktime_t submit_ts;\n};\n```\n```c\nstruct dma_fence {\nspinlock_t *lock;\nconst struct dma_fence_ops *ops;\n/*\n* We clear the callback list on kref_put so that by the time we\n* release the fence it is unused. No one should be adding to the\n* cb_list that they don't themselves hold a reference for.\n*\n* The lifetime of the timestamp is similarly tied to both the\n* rcu freelist and the cb_list. The timestamp is only set upon\n* signaling while simultaneously notifying the cb_list. Ergo, we\n* only use either the cb_list of timestamp. Upon destruction,\n* neither are accessible, and so we can use the rcu. This means\n* that the cb_list is *only* valid until the signal bit is set,\n* and to read either you *must* hold a reference to the fence,\n* and not just the rcu_read_lock.\n*\n* Listed in chronological order.\n*/\nunion {\nstruct list_head cb_list;\n/* @cb_list replaced by @timestamp on dma_fence_signal() */\nktime_t timestamp;\n/* @timestamp replaced by @rcu on dma_fence_release() */\nstruct rcu_head rcu;\n};\nu64 context;\nu64 seqno;\nunsigned long flags;\nstruct kref refcount;\nint error;\n};\n```\n```c\nstatic __always_inline void spin_lock(spinlock_t *lock)\n{\nraw_spin_lock(&lock->rlock);\n}\n```\n```c\nvoid drm_sched_rq_remove_entity(struct drm_sched_rq *rq,\nstruct drm_sched_entity *entity)\n{\nlockdep_assert_held(&entity->lock);\nif (list_empty(&entity->list))\nreturn;\nspin_lock(&rq->lock);\natomic_dec(rq->sched->score);\nlist_del_init(&entity->list);\nif (rq->current_entity == entity)\nrq->current_entity = NULL;\nif (drm_sched_policy == DRM_SCHED_POLICY_FIFO)\ndrm_sched_rq_remove_fifo_locked(entity, rq);\nspin_unlock(&rq->lock);\n}\n```\n```c\nstatic __always_inline void spin_unlock(spinlock_t *lock)\n{\nraw_spin_unlock(&lock->rlock);\n}\n```\n```c\nvoid __sched wait_for_completion(struct completion *x)\n{\nwait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE);\n}\nEXPORT_SYMBOL(wait_for_completion);\n```\n```c\n#define rcu_dereference_check(p, c) \\\n__rcu_dereference_check((p), __UNIQUE_ID(rcu), \\\n(c) || rcu_read_lock_held(), __rcu)\n```\n```c\nstatic inline struct dma_fence *dma_fence_get(struct dma_fence *fence)\n{\nif (fence)\nkref_get(&fence->refcount);\nreturn fence;\n}\n```\n```c\nstatic inline struct spsc_node *spsc_queue_pop(struct spsc_queue *queue)\n{\nstruct spsc_node *next, *node;\n/* Verify reading from memory and not the cache */\nsmp_rmb();\nnode = READ_ONCE(queue->head);\nif (!node)\nreturn NULL;\nnext = READ_ONCE(node->next);\nWRITE_ONCE(queue->head, next);\nif (unlikely(!next)) {\n/* slowpath for the last element in the queue */\nif (atomic_long_cmpxchg(&queue->tail,\n(long)&node->next, (long) &queue->head) != (long)&node->next) {\n/* Updating tail failed wait for new next to appear */\ndo {\nsmp_rmb();\n} while (unlikely(!(queue->head = READ_ONCE(node->next))));\n}\n}\natomic_dec(&queue->job_count);\nreturn node;\n}\n```\n```c\nstruct drm_sched_fence {\n/**\n* @scheduled: this fence is what will be signaled by the scheduler\n* when the job is scheduled.\n*/\nstruct dma_fence scheduled;\n/**\n* @finished: this fence is what will be signaled by the scheduler\n* when the job is completed.\n*\n* When setting up an out fence for the job, you should use\n* this, since it's available immediately upon\n* drm_sched_job_init(), and the fence returned by the driver\n* from run_job() won't be created until the dependencies have\n* resolved.\n*/\nstruct dma_fence finished;\n/**\n* @deadline: deadline set on &drm_sched_fence.finished which\n* potentially needs to be propagated to &drm_sched_fence.parent\n*/\nktime_t deadline;\n/**\n* @parent: the fence returned by &drm_sched_backend_ops.run_job\n* when scheduling the job on hardware. We signal the\n* &drm_sched_fence.finished fence once parent is signalled.\n*/\nstruct dma_fence *parent;\n/**\n* @sched: the scheduler instance to which the job having this struct\n* belongs to.\n*/\nstruct drm_gpu_scheduler *sched;\n/**\n* @lock: the lock used by the scheduled and the finished fences.\n*/\nspinlock_t lock;\n/**\n* @owner: job owner for debugging\n*/\nvoid *owner;\n};\n```\n```c\nint dma_fence_add_callback(struct dma_fence *fence, struct dma_fence_cb *cb,\ndma_fence_func_t func)\n{\nunsigned long flags;\nint ret = 0;\nif (WARN_ON(!fence || !func))\nreturn -EINVAL;\nif (test_bit(DMA_FENCE_FLAG_SIGNALED_BIT, &fence->flags)) {\nINIT_LIST_HEAD(&cb->node);\nreturn -ENOENT;\n}\nspin_lock_irqsave(fence->lock, flags);\nif (__dma_fence_enable_signaling(fence)) {\ncb->func = func;\nlist_add_tail(&cb->node, &fence->cb_list);\n} else {\nINIT_LIST_HEAD(&cb->node);\nret = -ENOENT;\n}\nspin_unlock_irqrestore(fence->lock, flags);\nreturn ret;\n}\nEXPORT_SYMBOL(dma_fence_add_callback);\n```\n```c\nstatic void drm_sched_entity_kill_jobs_cb(struct dma_fence *f,\nstruct dma_fence_cb *cb)\n{\nstruct drm_sched_job *job = container_of(cb, struct drm_sched_job,\nfinish_cb);\nunsigned long index;\ndma_fence_put(f);\n/* Wait for all dependencies to avoid data corruptions */\nxa_for_each(&job->dependencies, index, f) {\nstruct drm_sched_fence *s_fence = to_drm_sched_fence(f);\nif (s_fence && f == &s_fence->scheduled) {\n/* The dependencies array had a reference on the scheduled\n* fence, and the finished fence refcount might have\n* dropped to zero. Use dma_fence_get_rcu() so we get\n* a NULL fence in that case.\n*/\nf = dma_fence_get_rcu(&s_fence->finished);\n/* Now that we have a reference on the finished fence,\n* we can release the reference the dependencies array\n* had on the scheduled fence.\n*/\ndma_fence_put(&s_fence->scheduled);\n}\nxa_erase(&job->dependencies, index);\nif (f && !dma_fence_add_callback(f, &job->finish_cb,\ndrm_sched_entity_kill_jobs_cb))\nreturn;\ndma_fence_put(f);\n}\nINIT_WORK(&job->work, drm_sched_entity_kill_jobs_work);\nschedule_work(&job->work);\n}\n```\n```c\nstatic inline void dma_fence_put(struct dma_fence *fence)\n{\nif (fence)\nkref_put(&fence->refcount, dma_fence_release);\n}\n```",
  "original_code": "```c\nstatic void drm_sched_entity_kill(struct drm_sched_entity *entity)\n{\nstruct drm_sched_job *job;\nstruct dma_fence *prev;\nif (!entity->rq)\nreturn;\nspin_lock(&entity->rq_lock);\nentity->stopped = true;\ndrm_sched_rq_remove_entity(entity->rq, entity);\nspin_unlock(&entity->rq_lock);\n/* Make sure this entity is not used by the scheduler at the moment */\nwait_for_completion(&entity->entity_idle);\n/* The entity is guaranteed to not be used by the scheduler */\nprev = rcu_dereference_check(entity->last_scheduled, true);\ndma_fence_get(prev);\nwhile ((job = to_drm_sched_job(spsc_queue_pop(&entity->job_queue)))) {\nstruct drm_sched_fence *s_fence = job->s_fence;\ndma_fence_get(&s_fence->finished);\nif (!prev || dma_fence_add_callback(prev, &job->finish_cb,\ndrm_sched_entity_kill_jobs_cb))\ndrm_sched_entity_kill_jobs_cb(NULL, &job->finish_cb);\nprev = &s_fence->finished;\n}\ndma_fence_put(prev);\n}\n```",
  "vuln_patch": "```c\nstatic void drm_sched_entity_kill(struct drm_sched_entity *entity)\n{\nstruct drm_sched_job *job;\nstruct dma_fence *prev;\nif (!entity->rq)\nreturn;\nspin_lock(&entity->rq_lock);\nentity->stopped = true;\ndrm_sched_rq_remove_entity(entity->rq, entity);\nspin_unlock(&entity->rq_lock);\n/* Make sure this entity is not used by the scheduler at the moment */\nwait_for_completion(&entity->entity_idle);\n/* The entity is guaranteed to not be used by the scheduler */\nprev = rcu_dereference_check(entity->last_scheduled, true);\ndma_fence_get(prev);\nwhile ((job = to_drm_sched_job(spsc_queue_pop(&entity->job_queue)))) {\nstruct drm_sched_fence *s_fence = job->s_fence;\ndma_fence_get(&s_fence->finished);\nif (!prev ||\ndma_fence_add_callback(prev, &job->finish_cb,\ndrm_sched_entity_kill_jobs_cb)) {\n/*\n* Adding callback above failed.\n* dma_fence_put() checks for NULL.\n*/\ndma_fence_put(prev);\ndrm_sched_entity_kill_jobs_cb(NULL, &job->finish_cb);\n}\nprev = &s_fence->finished;\n}\ndma_fence_put(prev);\n}\n```",
  "function_name": "drm_sched_entity_kill",
  "function_prototype": "static void drm_sched_entity_kill(struct drm_sched_entity *entity)",
  "code_semantics": "The function is responsible for safely terminating a scheduling entity. It ensures that the entity is no longer active in the scheduling system by removing it from the queue and waiting for it to become idle. It then processes any remaining jobs in the entity's queue, ensuring that they are properly completed or terminated. This involves managing references to job completion signals and invoking callbacks to handle job termination.",
  "safe_verification_cot": "1. The function dma_fence_add_callback is called with prev and job->finish_cb. 2. If dma_fence_add_callback fails, the code now calls dma_fence_put(prev), ensuring that the reference count of prev is decremented, preventing a memory leak. 3. The callback drm_sched_entity_kill_jobs_cb is called after dma_fence_put(prev), ensuring that the memory associated with prev is properly managed and released.",
  "verification_cot": "1. The function dma_fence_add_callback is called with prev and job->finish_cb. 2. If dma_fence_add_callback fails, the code does not call dma_fence_put(prev), leading to a memory leak because the reference count of prev is not decremented. 3. The callback drm_sched_entity_kill_jobs_cb is called, but without decrementing the reference count of prev, the memory associated with prev is not released.",
  "vulnerability_related_variables": {
    "prev": "This variable acts as a reference to a synchronization object that represents the last completed task in a sequence. It is used to manage dependencies between tasks by ensuring that operations on subsequent tasks are only performed after the previous task has been completed. This is achieved by manipulating reference counts and adding callbacks to be executed upon task completion.",
    "job->finish_cb": "This variable serves as a function pointer for a callback mechanism. It is used to specify a function that should be executed when a particular event occurs, such as the completion of a task. The callback is associated with a synchronization object and is either added to a list of callbacks to be executed upon event completion or directly invoked if the event has already occurred."
  },
  "vulnerability_related_functions": {
    "dma_fence_add_callback": "This function attempts to add a callback to a synchronization object. It first checks if the object is already signaled. If not, it locks the object, checks if signaling is enabled, and if so, adds the callback to a list. It then unlocks the object and returns a status code indicating success or failure.",
    "dma_fence_put": "This function decreases the reference count of a synchronization object. If the reference count reaches zero, it triggers a release operation for the object.",
    "drm_sched_entity_kill_jobs_cb": "This function processes a job's dependencies. It iterates over the dependencies, checking if they are scheduled. If so, it transitions the reference from the scheduled to the finished state. It then attempts to add a callback to the finished state. If unsuccessful, it schedules a work task to handle the job."
  },
  "root_cause": "Memory leak due to missing dma_fence_put(prev) call when dma_fence_add_callback fails.",
  "patch_cot": "First, identify where dma_fence_add_callback is used in the code and check if it handles failures correctly. Ensure that for every dma_fence_get call, there is a corresponding dma_fence_put to manage the reference count properly. Specifically, in the loop where dma_fence_add_callback is called, add a check to see if it fails. If it does, call dma_fence_put(prev) to decrement the reference count and prevent a memory leak. Ensure that drm_sched_entity_kill_jobs_cb is called after handling the failure of dma_fence_add_callback."
}