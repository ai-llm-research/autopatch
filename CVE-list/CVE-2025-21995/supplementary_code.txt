```c
struct drm_sched_entity {
    /**
     * @list:
     *
     * Used to append this struct to the list of entities in the runqueue
     * @rq under &drm_sched_rq.entities.
     *
     * Protected by &drm_sched_rq.lock of @rq.
     */
    struct list_head        list;

    /**
     * @lock:
     *
     * Lock protecting the run-queue (@rq) to which this entity belongs,
     * @priority and the list of schedulers (@sched_list, @num_sched_list).
     */
    spinlock_t          lock;

    /**
     * @rq:
     *
     * Runqueue on which this entity is currently scheduled.
     *
     * FIXME: Locking is very unclear for this. Writers are protected by
     * @lock, but readers are generally lockless and seem to just race with
     * not even a READ_ONCE.
     */
    struct drm_sched_rq     *rq;

    /**
     * @sched_list:
     *
     * A list of schedulers (struct drm_gpu_scheduler).  Jobs from this entity can
     * be scheduled on any scheduler on this list.
     *
     * This can be modified by calling drm_sched_entity_modify_sched().
     * Locking is entirely up to the driver, see the above function for more
     * details.
     *
     * This will be set to NULL if &num_sched_list equals 1 and @rq has been
     * set already.
     *
     * FIXME: This means priority changes through
     * drm_sched_entity_set_priority() will be lost henceforth in this case.
     */
    struct drm_gpu_scheduler        **sched_list;

    /**
     * @num_sched_list:
     *
     * Number of drm_gpu_schedulers in the @sched_list.
     */
    unsigned int                    num_sched_list;

    /**
     * @priority:
     *
     * Priority of the entity. This can be modified by calling
     * drm_sched_entity_set_priority(). Protected by @lock.
     */
    enum drm_sched_priority         priority;

    /**
     * @job_queue: the list of jobs of this entity.
     */
    struct spsc_queue       job_queue;

    /**
     * @fence_seq:
     *
     * A linearly increasing seqno incremented with each new
     * &drm_sched_fence which is part of the entity.
     *
     * FIXME: Callers of drm_sched_job_arm() need to ensure correct locking,
     * this doesn't need to be atomic.
     */
    atomic_t            fence_seq;

    /**
     * @fence_context:
     *
     * A unique context for all the fences which belong to this entity.  The
     * &drm_sched_fence.scheduled uses the fence_context but
     * &drm_sched_fence.finished uses fence_context + 1.
     */
    uint64_t            fence_context;

    /**
     * @dependency:
     *
     * The dependency fence of the job which is on the top of the job queue.
     */
    struct dma_fence        *dependency;

    /**
     * @cb:
     *
     * Callback for the dependency fence above.
     */
    struct dma_fence_cb     cb;

    /**
     * @guilty:
     *
     * Points to entities' guilty.
     */
    atomic_t            *guilty;

    /**
     * @last_scheduled:
     *
     * Points to the finished fence of the last scheduled job. Only written
     * by the scheduler thread, can be accessed locklessly from
     * drm_sched_job_arm() if the queue is empty.
     */
    struct dma_fence __rcu      *last_scheduled;

    /**
     * @last_user: last group leader pushing a job into the entity.
     */
    struct task_struct      *last_user;

    /**
     * @stopped:
     *
     * Marks the enity as removed from rq and destined for
     * termination. This is set by calling drm_sched_entity_flush() and by
     * drm_sched_fini().
     */
    bool                stopped;

    /**
     * @entity_idle:
     *
     * Signals when entity is not in use, used to sequence entity cleanup in
     * drm_sched_entity_fini().
     */
    struct completion       entity_idle;

    /**
     * @oldest_job_waiting:
     *
     * Marks earliest job waiting in SW queue
     */
    ktime_t             oldest_job_waiting;

    /**
     * @rb_tree_node:
     *
     * The node used to insert this entity into time based priority queue
     */
    struct rb_node          rb_tree_node;

};
```

```c
struct drm_sched_job {
    struct spsc_node        queue_node;
    struct list_head        list;

    /**
     * @sched:
     *
     * The scheduler this job is or will be scheduled on. Gets set by
     * drm_sched_job_arm(). Valid until drm_sched_backend_ops.free_job()
     * has finished.
     */
    struct drm_gpu_scheduler    *sched;
    struct drm_sched_fence      *s_fence;

    u32             credits;

    /*
     * work is used only after finish_cb has been used and will not be
     * accessed anymore.
     */
    union {
        struct dma_fence_cb     finish_cb;
        struct work_struct      work;
    };

    uint64_t            id;
    atomic_t            karma;
    enum drm_sched_priority     s_priority;
    struct drm_sched_entity         *entity;
    struct dma_fence_cb     cb;
    /**
     * @dependencies:
     *
     * Contains the dependencies as struct dma_fence for this job, see
     * drm_sched_job_add_dependency() and
     * drm_sched_job_add_implicit_dependencies().
     */
    struct xarray           dependencies;

    /** @last_dependency: tracks @dependencies as they signal */
    unsigned long           last_dependency;

    /**
     * @submit_ts:
     *
     * When the job was pushed into the entity queue.
     */
    ktime_t                         submit_ts;
};
```

```c
struct dma_fence {
    spinlock_t *lock;
    const struct dma_fence_ops *ops;
    /*
     * We clear the callback list on kref_put so that by the time we
     * release the fence it is unused. No one should be adding to the
     * cb_list that they don't themselves hold a reference for.
     *
     * The lifetime of the timestamp is similarly tied to both the
     * rcu freelist and the cb_list. The timestamp is only set upon
     * signaling while simultaneously notifying the cb_list. Ergo, we
     * only use either the cb_list of timestamp. Upon destruction,
     * neither are accessible, and so we can use the rcu. This means
     * that the cb_list is *only* valid until the signal bit is set,
     * and to read either you *must* hold a reference to the fence,
     * and not just the rcu_read_lock.
     *
     * Listed in chronological order.
     */
    union {
        struct list_head cb_list;
        /* @cb_list replaced by @timestamp on dma_fence_signal() */
        ktime_t timestamp;
        /* @timestamp replaced by @rcu on dma_fence_release() */
        struct rcu_head rcu;
    };
    u64 context;
    u64 seqno;
    unsigned long flags;
    struct kref refcount;
    int error;
};
```

```c
static __always_inline void spin_lock(spinlock_t *lock)
{
    raw_spin_lock(&lock->rlock);
}
```

```c
void drm_sched_rq_remove_entity(struct drm_sched_rq *rq,
                struct drm_sched_entity *entity)
{
    lockdep_assert_held(&entity->lock);

    if (list_empty(&entity->list))
        return;

    spin_lock(&rq->lock);

    atomic_dec(rq->sched->score);
    list_del_init(&entity->list);

    if (rq->current_entity == entity)
        rq->current_entity = NULL;

    if (drm_sched_policy == DRM_SCHED_POLICY_FIFO)
        drm_sched_rq_remove_fifo_locked(entity, rq);

    spin_unlock(&rq->lock);
}
```

```c
static __always_inline void spin_unlock(spinlock_t *lock)
{
    raw_spin_unlock(&lock->rlock);
}
```

```c
void __sched wait_for_completion(struct completion *x)
{
    wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE);
}
EXPORT_SYMBOL(wait_for_completion);
```

```c
#define rcu_dereference_check(p, c) \
    __rcu_dereference_check((p), __UNIQUE_ID(rcu), \
                (c) || rcu_read_lock_held(), __rcu)
```

```c
static inline struct dma_fence *dma_fence_get(struct dma_fence *fence)
{
    if (fence)
        kref_get(&fence->refcount);
    return fence;
}
```

```c
static inline struct spsc_node *spsc_queue_pop(struct spsc_queue *queue)
{
    struct spsc_node *next, *node;

    /* Verify reading from memory and not the cache */
    smp_rmb();

    node = READ_ONCE(queue->head);

    if (!node)
        return NULL;

    next = READ_ONCE(node->next);
    WRITE_ONCE(queue->head, next);

    if (unlikely(!next)) {
        /* slowpath for the last element in the queue */

        if (atomic_long_cmpxchg(&queue->tail,
                (long)&node->next, (long) &queue->head) != (long)&node->next) {
            /* Updating tail failed wait for new next to appear */
            do {
                smp_rmb();
            } while (unlikely(!(queue->head = READ_ONCE(node->next))));
        }
    }

    atomic_dec(&queue->job_count);
    return node;
}
```

```c
struct drm_sched_fence {
        /**
         * @scheduled: this fence is what will be signaled by the scheduler
         * when the job is scheduled.
         */
    struct dma_fence        scheduled;

        /**
         * @finished: this fence is what will be signaled by the scheduler
         * when the job is completed.
         *
         * When setting up an out fence for the job, you should use
         * this, since it's available immediately upon
         * drm_sched_job_init(), and the fence returned by the driver
         * from run_job() won't be created until the dependencies have
         * resolved.
         */
    struct dma_fence        finished;

    /**
     * @deadline: deadline set on &drm_sched_fence.finished which
     * potentially needs to be propagated to &drm_sched_fence.parent
     */
    ktime_t             deadline;

        /**
         * @parent: the fence returned by &drm_sched_backend_ops.run_job
         * when scheduling the job on hardware. We signal the
         * &drm_sched_fence.finished fence once parent is signalled.
         */
    struct dma_fence        *parent;
        /**
         * @sched: the scheduler instance to which the job having this struct
         * belongs to.
         */
    struct drm_gpu_scheduler    *sched;
        /**
         * @lock: the lock used by the scheduled and the finished fences.
         */
    spinlock_t          lock;
        /**
         * @owner: job owner for debugging
         */
    void                *owner;
};
```

```c
int dma_fence_add_callback(struct dma_fence *fence, struct dma_fence_cb *cb,
               dma_fence_func_t func)
{
    unsigned long flags;
    int ret = 0;

    if (WARN_ON(!fence || !func))
        return -EINVAL;

    if (test_bit(DMA_FENCE_FLAG_SIGNALED_BIT, &fence->flags)) {
        INIT_LIST_HEAD(&cb->node);
        return -ENOENT;
    }

    spin_lock_irqsave(fence->lock, flags);

    if (__dma_fence_enable_signaling(fence)) {
        cb->func = func;
        list_add_tail(&cb->node, &fence->cb_list);
    } else {
        INIT_LIST_HEAD(&cb->node);
        ret = -ENOENT;
    }

    spin_unlock_irqrestore(fence->lock, flags);

    return ret;
}
EXPORT_SYMBOL(dma_fence_add_callback);
```

```c
static void drm_sched_entity_kill_jobs_cb(struct dma_fence *f,
                      struct dma_fence_cb *cb)
{
    struct drm_sched_job *job = container_of(cb, struct drm_sched_job,
                         finish_cb);
    unsigned long index;

    dma_fence_put(f);

    /* Wait for all dependencies to avoid data corruptions */
    xa_for_each(&job->dependencies, index, f) {
        struct drm_sched_fence *s_fence = to_drm_sched_fence(f);

        if (s_fence && f == &s_fence->scheduled) {
            /* The dependencies array had a reference on the scheduled
             * fence, and the finished fence refcount might have
             * dropped to zero. Use dma_fence_get_rcu() so we get
             * a NULL fence in that case.
             */
            f = dma_fence_get_rcu(&s_fence->finished);

            /* Now that we have a reference on the finished fence,
             * we can release the reference the dependencies array
             * had on the scheduled fence.
             */
            dma_fence_put(&s_fence->scheduled);
        }

        xa_erase(&job->dependencies, index);
        if (f && !dma_fence_add_callback(f, &job->finish_cb,
                         drm_sched_entity_kill_jobs_cb))
            return;

        dma_fence_put(f);
    }

    INIT_WORK(&job->work, drm_sched_entity_kill_jobs_work);
    schedule_work(&job->work);
}
```

```c
static inline void dma_fence_put(struct dma_fence *fence)
{
    if (fence)
        kref_put(&fence->refcount, dma_fence_release);
}
```
