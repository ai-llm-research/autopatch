```c
struct v3d_dev {
    struct drm_device drm;

    /* Short representation (e.g. 33, 41) of the V3D tech version */
    int ver;

    /* Short representation (e.g. 5, 6) of the V3D tech revision */
    int rev;

    bool single_irq_line;

    struct v3d_perfmon_info perfmon_info;

    void __iomem *hub_regs;
    void __iomem *core_regs[3];
    void __iomem *bridge_regs;
    void __iomem *gca_regs;
    struct clk *clk;
    struct reset_control *reset;

    /* Virtual and DMA addresses of the single shared page table. */
    volatile u32 *pt;
    dma_addr_t pt_paddr;

    /* Virtual and DMA addresses of the MMU's scratch page.  When
     * a read or write is invalid in the MMU, it will be
     * redirected here.
     */
    void *mmu_scratch;
    dma_addr_t mmu_scratch_paddr;
    /* virtual address bits from V3D to the MMU. */
    int va_width;

    /* Number of V3D cores. */
    u32 cores;

    /* Allocator managing the address space.  All units are in
     * number of pages.
     */
    struct drm_mm mm;
    spinlock_t mm_lock;

    /*
     * tmpfs instance used for shmem backed objects
     */
    struct vfsmount *gemfs;

    struct work_struct overflow_mem_work;

    struct v3d_bin_job *bin_job;
    struct v3d_render_job *render_job;
    struct v3d_tfu_job *tfu_job;
    struct v3d_csd_job *csd_job;
    struct v3d_cpu_job *cpu_job;

    struct v3d_queue_state queue[V3D_MAX_QUEUES];

    /* Spinlock used to synchronize the overflow memory
     * management against bin job submission.
     */
    spinlock_t job_lock;

    /* Used to track the active perfmon if any. */
    struct v3d_perfmon *active_perfmon;

    /* Protects bo_stats */
    struct mutex bo_lock;

    /* Lock taken when resetting the GPU, to keep multiple
     * processes from trying to park the scheduler threads and
     * reset at once.
     */
    struct mutex reset_lock;

    /* Lock taken when creating and pushing the GPU scheduler
     * jobs, to keep the sched-fence seqnos in order.
     */
    struct mutex sched_lock;

    /* Lock taken during a cache clean and when initiating an L2
     * flush, to keep L2 flushes from interfering with the
     * synchronous L2 cleans.
     */
    struct mutex cache_clean_lock;

    struct {
        u32 num_allocated;
        u32 pages_allocated;
    } bo_stats;
};
```

```c
#define V3D_CORE_READ(core, offset) readl(v3d->core_regs[core] + offset)
```

```c
#define V3D_CORE_WRITE(core, offset, val) writel(val, v3d->core_regs[core] + offset)
```

```c
static inline bool schedule_work(struct work_struct *work)
{
    return queue_work(system_wq, work);
}
```

```c
struct v3d_fence {
    struct dma_fence base;
    struct drm_device *dev;
    /* v3d seqno for signaled() test */
    u64 seqno;
    enum v3d_queue queue;
};
```

```c
static inline struct v3d_fence *
to_v3d_fence(struct dma_fence *fence)
{
    return (struct v3d_fence *)fence;
}
```

```c
void
v3d_job_update_stats(struct v3d_job *job, enum v3d_queue queue)
{
    struct v3d_dev *v3d = job->v3d;
    struct v3d_file_priv *file = job->file->driver_priv;
    struct v3d_stats *global_stats = &v3d->queue[queue].stats;
    struct v3d_stats *local_stats = &file->stats[queue];
    u64 now = local_clock();
    unsigned long flags;

    /* See comment in v3d_job_start_stats() */
    if (IS_ENABLED(CONFIG_LOCKDEP))
        local_irq_save(flags);
    else
        preempt_disable();

    v3d_stats_update(local_stats, now);
    v3d_stats_update(global_stats, now);

    if (IS_ENABLED(CONFIG_LOCKDEP))
        local_irq_restore(flags);
    else
        preempt_enable();
}
```

```c
TRACE_EVENT(v3d_bcl_irq,
        TP_PROTO(struct drm_device *dev,
             uint64_t seqno),
        TP_ARGS(dev, seqno),

        TP_STRUCT__entry(
                 __field(u32, dev)
                 __field(u64, seqno)
                 ),

        TP_fast_assign(
               __entry->dev = dev->primary->index;
               __entry->seqno = seqno;
               ),

        TP_printk("dev=%u, seqno=%llu",
              __entry->dev,
              __entry->seqno)
);
```
```c
TRACE_EVENT(v3d_rcl_irq,
        TP_PROTO(struct drm_device *dev,
             uint64_t seqno),
        TP_ARGS(dev, seqno),

        TP_STRUCT__entry(
                 __field(u32, dev)
                 __field(u64, seqno)
                 ),

        TP_fast_assign(
               __entry->dev = dev->primary->index;
               __entry->seqno = seqno;
               ),

        TP_printk("dev=%u, seqno=%llu",
              __entry->dev,
              __entry->seqno)
);
```

```c
TRACE_EVENT(v3d_csd_irq,
        TP_PROTO(struct drm_device *dev,
             uint64_t seqno),
        TP_ARGS(dev, seqno),

        TP_STRUCT__entry(
                 __field(u32, dev)
                 __field(u64, seqno)
                 ),

        TP_fast_assign(
               __entry->dev = dev->primary->index;
               __entry->seqno = seqno;
               ),

        TP_printk("dev=%u, seqno=%llu",
              __entry->dev,
              __entry->seqno)
);
```

```c
static irqreturn_t
v3d_hub_irq(int irq, void *arg)
{
    struct v3d_dev *v3d = arg;
    u32 intsts;
    irqreturn_t status = IRQ_NONE;

    intsts = V3D_READ(V3D_HUB_INT_STS);

    /* Acknowledge the interrupts we're handling here. */
    V3D_WRITE(V3D_HUB_INT_CLR, intsts);

    if (intsts & V3D_HUB_INT_TFUC) {
        struct v3d_fence *fence =
            to_v3d_fence(v3d->tfu_job->base.irq_fence);

        v3d_job_update_stats(&v3d->tfu_job->base, V3D_TFU);
        trace_v3d_tfu_irq(&v3d->drm, fence->seqno);
        dma_fence_signal(&fence->base);
        status = IRQ_HANDLED;
    }

    if (intsts & (V3D_HUB_INT_MMU_WRV |
              V3D_HUB_INT_MMU_PTI |
              V3D_HUB_INT_MMU_CAP)) {
        u32 axi_id = V3D_READ(V3D_MMU_VIO_ID);
        u64 vio_addr = ((u64)V3D_READ(V3D_MMU_VIO_ADDR) <<
                (v3d->va_width - 32));
        static const char *const v3d41_axi_ids[] = {
            "L2T",
            "PTB",
            "PSE",
            "TLB",
            "CLE",
            "TFU",
            "MMU",
            "GMP",
        };
        const char *client = "?";

        V3D_WRITE(V3D_MMU_CTL, V3D_READ(V3D_MMU_CTL));

        if (v3d->ver >= 41) {
            axi_id = axi_id >> 5;
            if (axi_id < ARRAY_SIZE(v3d41_axi_ids))
                client = v3d41_axi_ids[axi_id];
        }

        dev_err(v3d->drm.dev, "MMU error from client %s (%d) at 0x%llx%s%s%s\n",
            client, axi_id, (long long)vio_addr,
            ((intsts & V3D_HUB_INT_MMU_WRV) ?
             ", write violation" : ""),
            ((intsts & V3D_HUB_INT_MMU_PTI) ?
             ", pte invalid" : ""),
            ((intsts & V3D_HUB_INT_MMU_CAP) ?
             ", cap exceeded" : ""));
        status = IRQ_HANDLED;
    }

    if (v3d->ver >= 71 && (intsts & V3D_V7_HUB_INT_GMPV)) {
        dev_err(v3d->drm.dev, "GMP Violation\n");
        status = IRQ_HANDLED;
    }

    return status;
}
```

```c
int dma_fence_signal(struct dma_fence *fence)
{
    unsigned long flags;
    int ret;
    bool tmp;

    if (WARN_ON(!fence))
        return -EINVAL;

    tmp = dma_fence_begin_signalling();

    spin_lock_irqsave(fence->lock, flags);
    ret = dma_fence_signal_timestamp_locked(fence, ktime_get());
    spin_unlock_irqrestore(fence->lock, flags);

    dma_fence_end_signalling(tmp);

    return ret;
}
EXPORT_SYMBOL(dma_fence_signal);
```
