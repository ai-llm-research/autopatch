{
  "cwe_type": "Expired Pointer Dereference",
  "cve_id": "CVE-2025-21697",
  "supplementary_code": "```c\nstruct v3d_dev {\nstruct drm_device drm;\n/* Short representation (e.g. 33, 41) of the V3D tech version */\nint ver;\n/* Short representation (e.g. 5, 6) of the V3D tech revision */\nint rev;\nbool single_irq_line;\nstruct v3d_perfmon_info perfmon_info;\nvoid __iomem *hub_regs;\nvoid __iomem *core_regs[3];\nvoid __iomem *bridge_regs;\nvoid __iomem *gca_regs;\nstruct clk *clk;\nstruct reset_control *reset;\n/* Virtual and DMA addresses of the single shared page table. */\nvolatile u32 *pt;\ndma_addr_t pt_paddr;\n/* Virtual and DMA addresses of the MMU's scratch page. When\n* a read or write is invalid in the MMU, it will be\n* redirected here.\n*/\nvoid *mmu_scratch;\ndma_addr_t mmu_scratch_paddr;\n/* virtual address bits from V3D to the MMU. */\nint va_width;\n/* Number of V3D cores. */\nu32 cores;\n/* Allocator managing the address space. All units are in\n* number of pages.\n*/\nstruct drm_mm mm;\nspinlock_t mm_lock;\n/*\n* tmpfs instance used for shmem backed objects\n*/\nstruct vfsmount *gemfs;\nstruct work_struct overflow_mem_work;\nstruct v3d_bin_job *bin_job;\nstruct v3d_render_job *render_job;\nstruct v3d_tfu_job *tfu_job;\nstruct v3d_csd_job *csd_job;\nstruct v3d_cpu_job *cpu_job;\nstruct v3d_queue_state queue[V3D_MAX_QUEUES];\n/* Spinlock used to synchronize the overflow memory\n* management against bin job submission.\n*/\nspinlock_t job_lock;\n/* Used to track the active perfmon if any. */\nstruct v3d_perfmon *active_perfmon;\n/* Protects bo_stats */\nstruct mutex bo_lock;\n/* Lock taken when resetting the GPU, to keep multiple\n* processes from trying to park the scheduler threads and\n* reset at once.\n*/\nstruct mutex reset_lock;\n/* Lock taken when creating and pushing the GPU scheduler\n* jobs, to keep the sched-fence seqnos in order.\n*/\nstruct mutex sched_lock;\n/* Lock taken during a cache clean and when initiating an L2\n* flush, to keep L2 flushes from interfering with the\n* synchronous L2 cleans.\n*/\nstruct mutex cache_clean_lock;\nstruct {\nu32 num_allocated;\nu32 pages_allocated;\n} bo_stats;\n};\n```\n```c\n#define V3D_CORE_READ(core, offset) readl(v3d->core_regs[core] + offset)\n```\n```c\n#define V3D_CORE_WRITE(core, offset, val) writel(val, v3d->core_regs[core] + offset)\n```\n```c\nstatic inline bool schedule_work(struct work_struct *work)\n{\nreturn queue_work(system_wq, work);\n}\n```\n```c\nstruct v3d_fence {\nstruct dma_fence base;\nstruct drm_device *dev;\n/* v3d seqno for signaled() test */\nu64 seqno;\nenum v3d_queue queue;\n};\n```\n```c\nstatic inline struct v3d_fence *\nto_v3d_fence(struct dma_fence *fence)\n{\nreturn (struct v3d_fence *)fence;\n}\n```\n```c\nvoid\nv3d_job_update_stats(struct v3d_job *job, enum v3d_queue queue)\n{\nstruct v3d_dev *v3d = job->v3d;\nstruct v3d_file_priv *file = job->file->driver_priv;\nstruct v3d_stats *global_stats = &v3d->queue[queue].stats;\nstruct v3d_stats *local_stats = &file->stats[queue];\nu64 now = local_clock();\nunsigned long flags;\n/* See comment in v3d_job_start_stats() */\nif (IS_ENABLED(CONFIG_LOCKDEP))\nlocal_irq_save(flags);\nelse\npreempt_disable();\nv3d_stats_update(local_stats, now);\nv3d_stats_update(global_stats, now);\nif (IS_ENABLED(CONFIG_LOCKDEP))\nlocal_irq_restore(flags);\nelse\npreempt_enable();\n}\n```\n```c\nTRACE_EVENT(v3d_bcl_irq,\nTP_PROTO(struct drm_device *dev,\nuint64_t seqno),\nTP_ARGS(dev, seqno),\nTP_STRUCT__entry(\n__field(u32, dev)\n__field(u64, seqno)\n),\nTP_fast_assign(\n__entry->dev = dev->primary->index;\n__entry->seqno = seqno;\n),\nTP_printk(\"dev=%u, seqno=%llu\",\n__entry->dev,\n__entry->seqno)\n);\n```\n```c\nTRACE_EVENT(v3d_rcl_irq,\nTP_PROTO(struct drm_device *dev,\nuint64_t seqno),\nTP_ARGS(dev, seqno),\nTP_STRUCT__entry(\n__field(u32, dev)\n__field(u64, seqno)\n),\nTP_fast_assign(\n__entry->dev = dev->primary->index;\n__entry->seqno = seqno;\n),\nTP_printk(\"dev=%u, seqno=%llu\",\n__entry->dev,\n__entry->seqno)\n);\n```\n```c\nTRACE_EVENT(v3d_csd_irq,\nTP_PROTO(struct drm_device *dev,\nuint64_t seqno),\nTP_ARGS(dev, seqno),\nTP_STRUCT__entry(\n__field(u32, dev)\n__field(u64, seqno)\n),\nTP_fast_assign(\n__entry->dev = dev->primary->index;\n__entry->seqno = seqno;\n),\nTP_printk(\"dev=%u, seqno=%llu\",\n__entry->dev,\n__entry->seqno)\n);\n```\n```c\nstatic irqreturn_t\nv3d_hub_irq(int irq, void *arg)\n{\nstruct v3d_dev *v3d = arg;\nu32 intsts;\nirqreturn_t status = IRQ_NONE;\nintsts = V3D_READ(V3D_HUB_INT_STS);\n/* Acknowledge the interrupts we're handling here. */\nV3D_WRITE(V3D_HUB_INT_CLR, intsts);\nif (intsts & V3D_HUB_INT_TFUC) {\nstruct v3d_fence *fence =\nto_v3d_fence(v3d->tfu_job->base.irq_fence);\nv3d_job_update_stats(&v3d->tfu_job->base, V3D_TFU);\ntrace_v3d_tfu_irq(&v3d->drm, fence->seqno);\ndma_fence_signal(&fence->base);\nstatus = IRQ_HANDLED;\n}\nif (intsts & (V3D_HUB_INT_MMU_WRV |\nV3D_HUB_INT_MMU_PTI |\nV3D_HUB_INT_MMU_CAP)) {\nu32 axi_id = V3D_READ(V3D_MMU_VIO_ID);\nu64 vio_addr = ((u64)V3D_READ(V3D_MMU_VIO_ADDR) <<\n(v3d->va_width - 32));\nstatic const char *const v3d41_axi_ids[] = {\n\"L2T\",\n\"PTB\",\n\"PSE\",\n\"TLB\",\n\"CLE\",\n\"TFU\",\n\"MMU\",\n\"GMP\",\n};\nconst char *client = \"?\";\nV3D_WRITE(V3D_MMU_CTL, V3D_READ(V3D_MMU_CTL));\nif (v3d->ver >= 41) {\naxi_id = axi_id >> 5;\nif (axi_id < ARRAY_SIZE(v3d41_axi_ids))\nclient = v3d41_axi_ids[axi_id];\n}\ndev_err(v3d->drm.dev, \"MMU error from client %s (%d) at 0x%llx%s%s%s\\n\",\nclient, axi_id, (long long)vio_addr,\n((intsts & V3D_HUB_INT_MMU_WRV) ?\n\", write violation\" : \"\"),\n((intsts & V3D_HUB_INT_MMU_PTI) ?\n\", pte invalid\" : \"\"),\n((intsts & V3D_HUB_INT_MMU_CAP) ?\n\", cap exceeded\" : \"\"));\nstatus = IRQ_HANDLED;\n}\nif (v3d->ver >= 71 && (intsts & V3D_V7_HUB_INT_GMPV)) {\ndev_err(v3d->drm.dev, \"GMP Violation\\n\");\nstatus = IRQ_HANDLED;\n}\nreturn status;\n}\n```\n```c\nint dma_fence_signal(struct dma_fence *fence)\n{\nunsigned long flags;\nint ret;\nbool tmp;\nif (WARN_ON(!fence))\nreturn -EINVAL;\ntmp = dma_fence_begin_signalling();\nspin_lock_irqsave(fence->lock, flags);\nret = dma_fence_signal_timestamp_locked(fence, ktime_get());\nspin_unlock_irqrestore(fence->lock, flags);\ndma_fence_end_signalling(tmp);\nreturn ret;\n}\nEXPORT_SYMBOL(dma_fence_signal);\n```",
  "original_code": "```c\nstatic irqreturn_t v3d_irq(int irq, void *arg)\n{\nstruct v3d_dev *v3d = arg;\nu32 intsts;\nirqreturn_t status = IRQ_NONE;\nintsts = V3D_CORE_READ(0, V3D_CTL_INT_STS);\n/* Acknowledge the interrupts we're handling here. */\nV3D_CORE_WRITE(0, V3D_CTL_INT_CLR, intsts);\nif (intsts & V3D_INT_OUTOMEM) {\n/* Note that the OOM status is edge signaled, so the\n* interrupt won't happen again until the we actually\n* add more memory. Also, as of V3D 4.1, FLDONE won't\n* be reported until any OOM state has been cleared.\n*/\nschedule_work(&v3d->overflow_mem_work);\nstatus = IRQ_HANDLED;\n}\nif (intsts & V3D_INT_FLDONE) {\nstruct v3d_fence *fence =\nto_v3d_fence(v3d->bin_job->base.irq_fence);\ntrace_v3d_bcl_irq(&v3d->drm, fence->seqno);\ndma_fence_signal(&fence->base);\nstatus = IRQ_HANDLED;\n}\nif (intsts & V3D_INT_FRDONE) {\nstruct v3d_fence *fence =\nto_v3d_fence(v3d->render_job->base.irq_fence);\ntrace_v3d_rcl_irq(&v3d->drm, fence->seqno);\ndma_fence_signal(&fence->base);\nstatus = IRQ_HANDLED;\n}\nif (intsts & V3D_INT_CSDDONE) {\nstruct v3d_fence *fence =\nto_v3d_fence(v3d->csd_job->base.irq_fence);\ntrace_v3d_csd_irq(&v3d->drm, fence->seqno);\ndma_fence_signal(&fence->base);\nstatus = IRQ_HANDLED;\n}\n/* We shouldn't be triggering these if we have GMP in\n* always-allowed mode.\n*/\nif (intsts & V3D_INT_GMPV)\ndev_err(v3d->drm.dev, \"GMP violation\\n\");\n/* V3D 4.2 wires the hub and core IRQs together, so if we &\n* didn't see the common one then check hub for MMU IRQs.\n*/\nif (v3d->single_irq_line && status == IRQ_NONE)\nreturn v3d_hub_irq(irq, arg);\nreturn status;\n}\n```",
  "vuln_patch": "```c\nstatic irqreturn_t v3d_irq(int irq, void *arg)\n{\nstruct v3d_dev *v3d = arg;\nu32 intsts;\nirqreturn_t status = IRQ_NONE;\nintsts = V3D_CORE_READ(0, V3D_CTL_INT_STS);\n/* Acknowledge the interrupts we're handling here. */\nV3D_CORE_WRITE(0, V3D_CTL_INT_CLR, intsts);\nif (intsts & V3D_INT_OUTOMEM) {\n/* Note that the OOM status is edge signaled, so the\n* interrupt won't happen again until the we actually\n* add more memory. Also, as of V3D 4.1, FLDONE won't\n* be reported until any OOM state has been cleared.\n*/\nschedule_work(&v3d->overflow_mem_work);\nstatus = IRQ_HANDLED;\n}\nif (intsts & V3D_INT_FLDONE) {\nstruct v3d_fence *fence =\nto_v3d_fence(v3d->bin_job->base.irq_fence);\ntrace_v3d_bcl_irq(&v3d->drm, fence->seqno);\ndma_fence_signal(&fence->base);\nv3d->bin_job = NULL;\nstatus = IRQ_HANDLED;\n}\nif (intsts & V3D_INT_FRDONE) {\nstruct v3d_fence *fence =\nto_v3d_fence(v3d->render_job->base.irq_fence);\ntrace_v3d_rcl_irq(&v3d->drm, fence->seqno);\ndma_fence_signal(&fence->base);\nv3d->render_job = NULL;\nstatus = IRQ_HANDLED;\n}\nif (intsts & V3D_INT_CSDDONE) {\nstruct v3d_fence *fence =\nto_v3d_fence(v3d->csd_job->base.irq_fence);\ntrace_v3d_csd_irq(&v3d->drm, fence->seqno);\ndma_fence_signal(&fence->base);\nv3d->csd_job = NULL;\nstatus = IRQ_HANDLED;\n}\n/* We shouldn't be triggering these if we have GMP in\n* always-allowed mode.\n*/\nif (intsts & V3D_INT_GMPV)\ndev_err(v3d->drm.dev, \"GMP violation\\n\");\n/* V3D 4.2 wires the hub and core IRQs together, so if we &\n* didn't see the common one then check hub for MMU IRQs.\n*/\nif (v3d->single_irq_line && status == IRQ_NONE)\nreturn v3d_hub_irq(irq, arg);\nreturn status;\n}\n```",
  "function_name": "v3d_irq",
  "function_prototype": "static irqreturn_t v3d_irq(int irq, void *arg)",
  "code_semantics": "The function handles interrupts for a device by reading the interrupt status, acknowledging the interrupts, and performing actions based on the type of interrupt. It checks for specific interrupt types like memory overflow, job completions, and violations, and executes corresponding actions such as scheduling work, signaling completion, or logging errors. If certain conditions are met, it delegates to another function for additional interrupt handling.",
  "safe_verification_cot": "The function dma_fence_signal is called to signal the completion of a job. After signaling, the job pointers (v3d->bin_job, v3d->render_job, v3d->csd_job) are explicitly set to NULL. This ensures that any subsequent dereference of these pointers will not occur, as they are now NULL, preventing expired pointer dereference.",
  "verification_cot": "The function dma_fence_signal is called to signal the completion of a job. After signaling, the job pointers (v3d->bin_job, v3d->render_job, v3d->csd_job) are not set to NULL. This omission means that the pointers can still be dereferenced later, even though the job has been completed, leading to expired pointer dereference.",
  "vulnerability_related_variables": {
    "v3d->bin_job": "This entity is responsible for managing a specific type of task related to organizing data into bins. It is involved in the process of handling signals and recording events for these tasks.",
    "v3d->render_job": "This entity is responsible for managing tasks related to generating images or frames. It is involved in the process of handling signals and recording events for these tasks.",
    "v3d->csd_job": "This entity is responsible for managing tasks related to executing compute operations. It is involved in the process of handling signals and recording events for these tasks."
  },
  "vulnerability_related_functions": {
    "dma_fence_signal": "The function checks if the input is valid. It then begins a signaling process, locks a resource to ensure exclusive access, updates a timestamp for the resource, and unlocks the resource. Finally, it ends the signaling process and returns the result of the update operation.",
    "to_v3d_fence": "The function takes an input and converts it to a specific data structure type, then returns the converted data structure."
  },
  "root_cause": "Expired pointer dereference due to not setting job pointers to NULL after job completion.",
  "patch_cot": "First, identify where the dma_fence_signal function is called for each job type (bin_job, render_job, csd_job). After each call to dma_fence_signal, set the corresponding job pointer to NULL. This ensures that the pointer does not point to an invalid memory location after the job is completed. Specifically, after handling V3D_INT_FLDONE, set v3d->bin_job = NULL;. After handling V3D_INT_FRDONE, set v3d->render_job = NULL;. After handling V3D_INT_CSDDONE, set v3d->csd_job = NULL;."
}