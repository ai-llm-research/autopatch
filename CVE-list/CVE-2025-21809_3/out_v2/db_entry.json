{
  "cwe_type": "Improper Locking",
  "cve_id": "CVE-2025-21809",
  "supplementary_code": "```c\nstruct rxrpc_local {\nstruct rcu_head rcu;\natomic_t active_users; /* Number of users of the local endpoint */\nrefcount_t ref; /* Number of references to the structure */\nstruct net *net; /* The network namespace */\nstruct rxrpc_net *rxnet; /* Our bits in the network namespace */\nstruct hlist_node link;\nstruct socket *socket; /* my UDP socket */\nstruct task_struct *io_thread;\nstruct completion io_thread_ready; /* Indication that the I/O thread started */\nstruct page_frag_cache tx_alloc; /* Tx control packet allocation (I/O thread only) */\nstruct rxrpc_sock *service; /* Service(s) listening on this endpoint */\n#ifdef CONFIG_AF_RXRPC_INJECT_RX_DELAY\nstruct sk_buff_head rx_delay_queue; /* Delay injection queue */\n#endif\nstruct sk_buff_head rx_queue; /* Received packets */\nstruct list_head conn_attend_q; /* Conns requiring immediate attention */\nstruct list_head call_attend_q; /* Calls requiring immediate attention */\nstruct rb_root client_bundles; /* Client connection bundles by socket params */\nspinlock_t client_bundles_lock; /* Lock for client_bundles */\nbool kill_all_client_conns;\nstruct list_head idle_client_conns;\nstruct timer_list client_conn_reap_timer;\nunsigned long client_conn_flags;\n#define RXRPC_CLIENT_CONN_REAP_TIMER 0 /* The client conn reap timer expired */\nspinlock_t lock; /* access lock */\nrwlock_t services_lock; /* lock for services list */\nint debug_id; /* debug ID for printks */\nbool dead;\nbool service_closed; /* Service socket closed */\nstruct idr conn_ids; /* List of connection IDs */\nstruct list_head new_client_calls; /* Newly created client calls need connection */\nspinlock_t client_call_lock; /* Lock for ->new_client_calls */\nstruct sockaddr_rxrpc srx; /* local address */\n};\n```\n```c\nstruct rxrpc_peer {\nstruct rcu_head rcu; /* This must be first */\nrefcount_t ref;\nunsigned long hash_key;\nstruct hlist_node hash_link;\nstruct rxrpc_local *local;\nstruct hlist_head error_targets; /* targets for net error distribution */\nstruct rb_root service_conns; /* Service connections */\nstruct list_head keepalive_link; /* Link in net->peer_keepalive[] */\ntime64_t last_tx_at; /* Last time packet sent here */\nseqlock_t service_conn_lock;\nspinlock_t lock; /* access lock */\nunsigned int if_mtu; /* interface MTU for this peer */\nunsigned int mtu; /* network MTU for this peer */\nunsigned int maxdata; /* data size (MTU - hdrsize) */\nunsigned short hdrsize; /* header size (IP + UDP + RxRPC) */\nint debug_id; /* debug ID for printks */\nstruct sockaddr_rxrpc srx; /* remote address */\n/* calculated RTT cache */\n#define RXRPC_RTT_CACHE_SIZE 32\nspinlock_t rtt_input_lock; /* RTT lock for input routine */\nktime_t rtt_last_req; /* Time of last RTT request */\nunsigned int rtt_count; /* Number of samples we've got */\nu32 srtt_us; /* smoothed round trip time << 3 in usecs */\nu32 mdev_us; /* medium deviation */\nu32 mdev_max_us; /* maximal mdev for the last rtt period */\nu32 rttvar_us; /* smoothed mdev_max */\nu32 rto_us; /* Retransmission timeout in usec */\nu8 backoff; /* Backoff timeout (as shift) */\nu8 cong_ssthresh; /* Congestion slow-start threshold */\n};\n```\n```c\nstruct rxrpc_net {\nstruct proc_dir_entry *proc_net; /* Subdir in /proc/net */\nu32 epoch; /* Local epoch for detecting local-end reset */\nstruct list_head calls; /* List of calls active in this namespace */\nspinlock_t call_lock; /* Lock for ->calls */\natomic_t nr_calls; /* Count of allocated calls */\natomic_t nr_conns;\nstruct list_head bundle_proc_list; /* List of bundles for proc */\nstruct list_head conn_proc_list; /* List of conns in this namespace for proc */\nstruct list_head service_conns; /* Service conns in this namespace */\nrwlock_t conn_lock; /* Lock for ->conn_proc_list, ->service_conns */\nstruct work_struct service_conn_reaper;\nstruct timer_list service_conn_reap_timer;\nbool live;\natomic_t nr_client_conns;\nstruct hlist_head local_endpoints;\nstruct mutex local_mutex; /* Lock for ->local_endpoints */\nDECLARE_HASHTABLE (peer_hash, 10);\nspinlock_t peer_hash_lock; /* Lock for ->peer_hash */\n#define RXRPC_KEEPALIVE_TIME 20 /* NAT keepalive time in seconds */\nu8 peer_keepalive_cursor;\ntime64_t peer_keepalive_base;\nstruct list_head peer_keepalive[32];\nstruct list_head peer_keepalive_new;\nstruct timer_list peer_keepalive_timer;\nstruct work_struct peer_keepalive_work;\natomic_t stat_tx_data;\natomic_t stat_tx_data_retrans;\natomic_t stat_tx_data_send;\natomic_t stat_tx_data_send_frag;\natomic_t stat_tx_data_send_fail;\natomic_t stat_tx_data_underflow;\natomic_t stat_tx_data_cwnd_reset;\natomic_t stat_rx_data;\natomic_t stat_rx_data_reqack;\natomic_t stat_rx_data_jumbo;\natomic_t stat_tx_ack_fill;\natomic_t stat_tx_ack_send;\natomic_t stat_tx_ack_skip;\natomic_t stat_tx_acks[256];\natomic_t stat_rx_acks[256];\natomic_t stat_why_req_ack[8];\natomic_t stat_io_loop;\n};\n```\n```c\nstatic unsigned long rxrpc_peer_hash_key(struct rxrpc_local *local,\nconst struct sockaddr_rxrpc *srx)\n{\nconst u16 *p;\nunsigned int i, size;\nunsigned long hash_key;\n_enter(\"\");\nhash_key = (unsigned long)local / __alignof__(*local);\nhash_key += srx->transport_type;\nhash_key += srx->transport_len;\nhash_key += srx->transport.family;\nswitch (srx->transport.family) {\ncase AF_INET:\nhash_key += (u16 __force)srx->transport.sin.sin_port;\nsize = sizeof(srx->transport.sin.sin_addr);\np = (u16 *)&srx->transport.sin.sin_addr;\nbreak;\n#ifdef CONFIG_AF_RXRPC_IPV6\ncase AF_INET6:\nhash_key += (u16 __force)srx->transport.sin.sin_port;\nsize = sizeof(srx->transport.sin6.sin6_addr);\np = (u16 *)&srx->transport.sin6.sin6_addr;\nbreak;\n#endif\ndefault:\nWARN(1, \"AF_RXRPC: Unsupported transport address family\\n\");\nreturn 0;\n}\n/* Step through the peer address in 16-bit portions for speed */\nfor (i = 0; i < size; i += sizeof(*p), p++)\nhash_key += *p;\n_leave(\" 0x%lx\", hash_key);\nreturn hash_key;\n}\n```\n```c\nstatic void rxrpc_init_peer(struct rxrpc_local *local, struct rxrpc_peer *peer, unsigned long hash_key)\n{\npeer->hash_key = hash_key;\nrxrpc_assess_MTU_size(local, peer);\npeer->mtu = peer->if_mtu;\npeer->rtt_last_req = ktime_get_real();\nswitch (peer->srx.transport.family) {\ncase AF_INET:\npeer->hdrsize = sizeof(struct iphdr);\nbreak;\n#ifdef CONFIG_AF_RXRPC_IPV6\ncase AF_INET6:\npeer->hdrsize = sizeof(struct ipv6hdr);\nbreak;\n#endif\ndefault:\nBUG();\n}\nswitch (peer->srx.transport_type) {\ncase SOCK_DGRAM:\npeer->hdrsize += sizeof(struct udphdr);\nbreak;\ndefault:\nBUG();\n}\npeer->hdrsize += sizeof(struct rxrpc_wire_header);\npeer->maxdata = peer->mtu - peer->hdrsize;\n}\n```\n```c\nstatic inline void spin_lock(spinlock_t *lock)\n{\nint ret = pthread_spin_lock(lock);\nassert(!ret);\n}\n```\n```c\n#define hash_add_rcu(hashtable, node, key) \\\nhlist_add_head_rcu(node, &hashtable[hash_min(key, HASH_BITS(hashtable))])\n```\n```c\nstatic inline void list_add_tail(struct list_head *new, struct list_head *head)\n{\n__list_add(new, head->prev, head);\n}\n```\n```c\nstatic inline void spin_unlock(spinlock_t *lock)\n{\nint ret = pthread_spin_unlock(lock);\nassert(!ret);\n}\n```",
  "original_code": "```c\nvoid rxrpc_new_incoming_peer(struct rxrpc_local *local, struct rxrpc_peer *peer)\n{\nstruct rxrpc_net *rxnet = local->rxnet;\nunsigned long hash_key;\nhash_key = rxrpc_peer_hash_key(local, &peer->srx);\nrxrpc_init_peer(local, peer, hash_key);\nspin_lock(&rxnet->peer_hash_lock);\nhash_add_rcu(rxnet->peer_hash, &peer->hash_link, hash_key);\nlist_add_tail(&peer->keepalive_link, &rxnet->peer_keepalive_new);\nspin_unlock(&rxnet->peer_hash_lock);\n}\n```",
  "vuln_patch": "```c\nvoid rxrpc_new_incoming_peer(struct rxrpc_local *local, struct rxrpc_peer *peer)\n{\nstruct rxrpc_net *rxnet = local->rxnet;\nunsigned long hash_key;\nhash_key = rxrpc_peer_hash_key(local, &peer->srx);\nrxrpc_init_peer(local, peer, hash_key);\nspin_lock_bh(&rxnet->peer_hash_lock);\nhash_add_rcu(rxnet->peer_hash, &peer->hash_link, hash_key);\nlist_add_tail(&peer->keepalive_link, &rxnet->peer_keepalive_new);\nspin_unlock_bh(&rxnet->peer_hash_lock);\n}\n```",
  "function_name": "rxrpc_new_incoming_peer",
  "function_prototype": "void rxrpc_new_incoming_peer(struct rxrpc_local *local, struct rxrpc_peer *peer)",
  "code_semantics": "The function manages a new network participant by computing a unique identifier based on its network address, setting up communication parameters like data packet size and header information, ensuring thread-safe operations using a locking mechanism, registering the participant in a system-wide directory for quick lookup, adding it to a list for periodic maintenance to keep it active, and finally releasing the lock to allow other operations.",
  "safe_verification_cot": "The function spin_lock_bh is used to lock rxnet->peer_hash_lock, which disables bottom halves. The function spin_unlock_bh is used to unlock rxnet->peer_hash_lock, which re-enables bottom halves. By disabling bottom halves, the patched code prevents race conditions, ensuring that rxnet->peer_hash and rxnet->peer_keepalive_new are safely modified without interference from bottom halves.",
  "verification_cot": "The function spin_lock is used to lock rxnet->peer_hash_lock, which does not disable bottom halves. The function spin_unlock is used to unlock rxnet->peer_hash_lock, which does not re-enable bottom halves. Since bottom halves are not disabled, there is a potential for race conditions if a bottom half accesses rxnet->peer_hash or rxnet->peer_keepalive_new while they are being modified.",
  "vulnerability_related_variables": {
    "rxnet->peer_hash_lock": "This variable is a synchronization primitive used to ensure exclusive access to shared resources. It is acquired before accessing or modifying shared data structures and released afterward to prevent concurrent access issues.",
    "rxnet->peer_hash": "This variable is a data structure used to store and organize elements for efficient retrieval. It uses a computed key to determine the storage location of an element, allowing for quick access based on the key.",
    "rxnet->peer_keepalive_new": "This variable is a sequential collection used to maintain a list of elements. New elements are added to the end of the collection, allowing for ordered tracking and processing of the elements."
  },
  "vulnerability_related_functions": {
    "spin_lock": "Acquires a lock to ensure exclusive access to a shared resource, preventing concurrent access by other threads.",
    "spin_unlock": "Releases a previously acquired lock, allowing other threads to access the shared resource.",
    "spin_lock_bh": "Acquires a lock in contexts where bottom halves (softirqs) are disabled, ensuring exclusive access in such scenarios.",
    "spin_unlock_bh": "Releases a lock in contexts where bottom halves (softirqs) are disabled, allowing access to the resource once released.",
    "hash_add_rcu": "Adds an element to a hash table in a manner that is safe for concurrent read-copy-update operations, allowing readers to access the table without locks.",
    "list_add_tail": "Adds a new element to the end of a linked list, maintaining the order of elements."
  },
  "root_cause": "Improper use of spinlocks without disabling bottom halves, leading to potential race conditions.",
  "patch_cot": "First, identify the critical section in the function rxrpc_new_incoming_peer where the shared resources rxnet->peer_hash and rxnet->peer_keepalive_new are accessed. Replace the spin_lock call with spin_lock_bh to ensure that the bottom halves are disabled when acquiring the lock rxnet->peer_hash_lock. Replace the spin_unlock call with spin_unlock_bh to ensure that the bottom halves are enabled when releasing the lock rxnet->peer_hash_lock. This change will prevent race conditions by ensuring that the critical section is not interrupted by bottom halves, thus maintaining data integrity."
}