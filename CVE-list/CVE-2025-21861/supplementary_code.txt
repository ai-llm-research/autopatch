```c
struct folio {
       struct page page;
};
```

```c
struct page {
    unsigned long long dummy;
};
```

```c
static inline struct page *migrate_pfn_to_page(unsigned long mpfn)
{
    if (!(mpfn & MIGRATE_PFN_VALID))
        return NULL;
    return pfn_to_page(mpfn >> MIGRATE_PFN_SHIFT);
}
```

```c
#define page_folio(p)       (_Generic((p),              \
    const struct page *:    (const struct folio *)_compound_head(p), \
    struct page *:      (struct folio *)_compound_head(p)))
```

```c
void folio_unlock(struct folio *folio)
{
    /* Bit 7 allows x86 to check the byte's sign bit */
    BUILD_BUG_ON(PG_waiters != 7);
    BUILD_BUG_ON(PG_locked > 7);
    VM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);
    if (folio_xor_flags_has_waiters(folio, 1 << PG_locked))
        folio_wake_bit(folio, PG_locked);
}
EXPORT_SYMBOL(folio_unlock);
```

```c
static inline void folio_put(struct folio *folio)
{
    if (folio_put_testzero(folio))
        __folio_put(folio);
}
```

```c
void remove_migration_ptes(struct folio *src, struct folio *dst, int flags)
{
    struct rmap_walk_arg rmap_walk_arg = {
        .folio = src,
        .map_unused_to_zeropage = flags & RMP_USE_SHARED_ZEROPAGE,
    };

    struct rmap_walk_control rwc = {
        .rmap_one = remove_migration_pte,
        .arg = &rmap_walk_arg,
    };

    VM_BUG_ON_FOLIO((flags & RMP_USE_SHARED_ZEROPAGE) && (src != dst), src);

    if (flags & RMP_LOCKED)
        rmap_walk_locked(dst, &rwc);
    else
        rmap_walk(dst, &rwc);
}
```

```c
static inline bool folio_is_zone_device(const struct folio *folio)
{
    return is_zone_device_page(&folio->page);
}
```

```c
void folio_putback_lru(struct folio *folio)
{
    folio_add_lru(folio);
    folio_put(folio);       /* drop ref from isolate */
}
```
