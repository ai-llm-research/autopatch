{
 "supplementary_code": "```c\n/**\n* struct perf_event - performance event kernel representation:\n*/\nstruct perf_event {\n#ifdef CONFIG_PERF_EVENTS\n/*\n* entry onto perf_event_context::event_list;\n* modifications require ctx->lock\n* RCU safe iterations.\n*/\nstruct list_head\tevent_entry;\n/*\n* Locked for modification by both ctx->mutex and ctx->lock; holding\n* either sufficies for read.\n*/\nstruct list_head\tsibling_list;\nstruct list_head\tactive_list;\n/*\n* Node on the pinned or flexible tree located at the event context;\n*/\nstruct rb_node\tgroup_node;\nu64\tgroup_index;\n/*\n* We need storage to track the entries in perf_pmu_migrate_context; we\n* cannot use the event_entry because of RCU and we want to keep the\n* group in tact which avoids us using the other two entries.\n*/\nstruct list_head\tmigrate_entry;\nstruct hlist_node\thlist_entry;\nstruct list_head\tactive_entry;\nint\tnr_siblings;\n/* Not serialized. Only written during event initialization. */\nint\tevent_caps;\n/* The cumulative AND of all event_caps for events in this group. */\nint\tgroup_caps;\nunsigned int\tgroup_generation;\nstruct perf_event\t*group_leader;\n/*\n* event->pmu will always point to pmu in which this event belongs.\n* Whereas event->pmu_ctx->pmu may point to other pmu when group of\n* different pmu events is created.\n*/\nstruct pmu\t*pmu;\nvoid\t*pmu_private;\nenum perf_event_state\tstate;\nunsigned int\tattach_state;\nlocal64_t\tcount;\natomic64_t\tchild_count;\n/*\n* These are the total time in nanoseconds that the event\n* has been enabled (i.e. eligible to run, and the task has\n* been scheduled in, if this is a per-task event)\n* and running (scheduled onto the CPU), respectively.\n*/\nu64\ttotal_time_enabled;\nu64\ttotal_time_running;\nu64\ttstamp;\nstruct perf_event_attr\tattr;\nu16\theader_size;\nu16\tid_header_size;\nu16\tread_size;\nstruct hw_perf_event\thw;\nstruct perf_event_context\t*ctx;\n/*\n* event->pmu_ctx points to perf_event_pmu_context in which the event\n* is added. This pmu_ctx can be of other pmu for sw event when that\n* sw event is part of a group which also contains non-sw events.\n*/\nstruct perf_event_pmu_context\t*pmu_ctx;\natomic_long_t\trefcount;\n/*\n* These accumulate total time (in nanoseconds) that children\n* events have been enabled and running, respectively.\n*/\natomic64_t\tchild_total_time_enabled;\natomic64_t\tchild_total_time_running;\n/*\n* Protect attach/detach and child_list:\n*/\nstruct mutex\tchild_mutex;\nstruct list_head\tchild_list;\nstruct perf_event\t*parent;\nint\toncpu;\nint\tcpu;\nstruct list_head\towner_entry;\nstruct task_struct\t*owner;\n/* mmap bits */\nstruct mutex\tmmap_mutex;\natomic_t\tmmap_count;\nstruct perf_buffer\t*rb;\nstruct list_head\trb_entry;\nunsigned long\trcu_batches;\nint\trcu_pending;\n/* poll related */\nwait_queue_head_t\twaitq;\nstruct fasync_struct\t*fasync;\n/* delayed work for NMIs and such */\nunsigned int\tpending_wakeup;\nunsigned int\tpending_kill;\nunsigned int\tpending_disable;\nunsigned long\tpending_addr;\t/* SIGTRAP */\nstruct irq_work\tpending_irq;\nstruct irq_work\tpending_disable_irq;\nstruct callback_head\tpending_task;\nunsigned int\tpending_work;\nstruct rcuwait\tpending_work_wait;\natomic_t\tevent_limit;\n/* address range filters */\nstruct perf_addr_filters_head\taddr_filters;\n/* vma address array for file-based filders */\nstruct perf_addr_filter_range\t*addr_filter_ranges;\nunsigned long\taddr_filters_gen;\n/* for aux_output events */\nstruct perf_event\t*aux_event;\nvoid (*destroy)(struct perf_event *);\nstruct rcu_head\trcu_head;\nstruct pid_namespace\t*ns;\nu64\tid;\natomic64_t\tlost_samples;\nu64\t(*clock)(void);\nperf_overflow_handler_t\toverflow_handler;\nvoid\t*overflow_handler_context;\nstruct bpf_prog\t*prog;\nu64\tbpf_cookie;\n#ifdef CONFIG_EVENT_TRACING\nstruct trace_event_call\t*tp_event;\nstruct event_filter\t*filter;\n#ifdef CONFIG_FUNCTION_TRACER\nstruct ftrace_ops ftrace_ops;\n#endif\n#endif\n#ifdef CONFIG_CGROUP_PERF\nstruct perf_cgroup\t*cgrp; /* cgroup event is attach to */\n#endif\n#ifdef CONFIG_SECURITY\nvoid *security;\n#endif\nstruct list_head\tsb_list;\n/*\n* Certain events gets forwarded to another pmu internally by over-\n* writing kernel copy of event->attr.type without user being aware\n* of it. event->orig_type contains original 'type' requested by\n* user.\n*/\n__u32\torig_type;\n#endif /* CONFIG_PERF_EVENTS */\n};\n```\n```c\nstruct bpf_prog_array {\nstruct rcu_head rcu;\nstruct bpf_prog_array_item items[];\n};\n```\n```c\nvoid mutex_lock(struct mutex *mtx)\nNO_THREAD_SAFETY_ANALYSIS\n{\nCHECK_ERR(pthread_mutex_lock(&mtx->lock));\n}\n```\n```c\n#define bpf_event_rcu_dereference(p)\t\\\nrcu_dereference_protected(p, lockdep_is_held(&bpf_event_mutex))\n/**\n* rcu_dereference_protected() - fetch RCU pointer when updates prevented\n* @p: The pointer to read, prior to dereferencing\n* @c: The conditions under which the dereference will take place\n*\n* Return the value of the specified RCU-protected pointer, but omit\n* the READ_ONCE(). This is useful in cases where update-side locks\n* prevent the value of the pointer from changing. Please note that this\n* primitive does *not* prevent the compiler from repeating this reference\n* or combining it with other references, so it should not be used without\n* protection of appropriate locks.\n*\n* This function is only for update-side use. Using this function\n* when protected only by rcu_read_lock() will result in infrequent\n* but very ugly failures.\n*/\n#define rcu_dereference_protected(p, c) \\\n__rcu_dereference_protected((p), __UNIQUE_ID(rcu), (c), __rcu)\n```\n```c\nint bpf_prog_array_copy(struct bpf_prog_array *old_array,\nstruct bpf_prog *exclude_prog,\nstruct bpf_prog *include_prog,\nu64 bpf_cookie,\nstruct bpf_prog_array **new_array)\n{\nint new_prog_cnt, carry_prog_cnt = 0;\nstruct bpf_prog_array_item *existing, *new;\nstruct bpf_prog_array *array;\nbool found_exclude = false;\n/* Figure out how many existing progs we need to carry over to\n* the new array.\n*/\nif (old_array) {\nexisting = old_array->items;\nfor (; existing->prog; existing++) {\nif (existing->prog == exclude_prog) {\nfound_exclude = true;\ncontinue;\n}\nif (existing->prog != &dummy_bpf_prog.prog)\ncarry_prog_cnt++;\nif (existing->prog == include_prog)\nreturn -EEXIST;\n}\n}\nif (exclude_prog && !found_exclude)\nreturn -ENOENT;\n/* How many progs (not NULL) will be in the new array? */\nnew_prog_cnt = carry_prog_cnt;\nif (include_prog)\nnew_prog_cnt += 1;\n/* Do we have any prog (not NULL) in the new array? */\nif (!new_prog_cnt) {\n*new_array = NULL;\nreturn 0;\n}\n/* +1 as the end of prog_array is marked with NULL */\narray = bpf_prog_array_alloc(new_prog_cnt + 1, GFP_KERNEL);\nif (!array)\nreturn -ENOMEM;\nnew = array->items;\n/* Fill in the new prog array */\nif (carry_prog_cnt) {\nexisting = old_array->items;\nfor (; existing->prog; existing++) {\nif (existing->prog == exclude_prog ||\nexisting->prog == &dummy_bpf_prog.prog)\ncontinue;\nnew->prog = existing->prog;\nnew->bpf_cookie = existing->bpf_cookie;\nnew++;\n}\n}\nif (include_prog) {\nnew->prog = include_prog;\nnew->bpf_cookie = bpf_cookie;\nnew++;\n}\nnew->prog = NULL;\n*new_array = array;\nreturn 0;\n}\n```\n```c\nvoid bpf_prog_array_delete_safe(struct bpf_prog_array *array,\nstruct bpf_prog *old_prog)\n{\nstruct bpf_prog_array_item *item;\nfor (item = array->items; item->prog; item++)\nif (item->prog == old_prog) {\nWRITE_ONCE(item->prog, &dummy_bpf_prog.prog);\nbreak;\n}\n}\n```\n```c\n/**\n* rcu_assign_pointer() - assign to RCU-protected pointer\n* @p: pointer to assign to\n* @v: value to assign (publish)\n*\n* Assigns the specified value to the specified RCU-protected\n* pointer, ensuring that any concurrent RCU readers will see\n* any prior initialization.\n*\n* Inserts memory barriers on architectures that require them\n* (which is most of them), and also prevents the compiler from\n* reordering the code that initializes the structure after the pointer\n* assignment. More importantly, this call documents which pointers\n* will be dereferenced by RCU read-side code.\n*\n* In some special cases, you may use RCU_INIT_POINTER() instead\n* of rcu_assign_pointer(). RCU_INIT_POINTER() is a bit faster due\n* to the fact that it does not constrain either the CPU or the compiler.\n* That said, using RCU_INIT_POINTER() when you should have used\n* rcu_assign_pointer() is a very bad thing that results in\n* impossible-to-diagnose memory corruption. So please be careful.\n* See the RCU_INIT_POINTER() comment header for details.\n*\n* Note that rcu_assign_pointer() evaluates each of its arguments only\n* once, appearances notwithstanding. One of the \"extra\" evaluations\n* is in typeof() and the other visible only to sparse (__CHECKER__),\n* neither of which actually execute the argument. As with most cpp\n* macros, this execute-arguments-only-once property is important, so\n* please be careful when making changes to rcu_assign_pointer() and the\n* other macros that it invokes.\n*/\n#define rcu_assign_pointer(p, v)\t\\\ndo {\t\\\nuintptr_t _r_a_p__v = (uintptr_t)(v);\t\\\nrcu_check_sparse(p, __rcu);\t\\\n\\\nif (__builtin_constant_p(v) && (_r_a_p__v) == (uintptr_t)NULL)\t\\\nWRITE_ONCE((p), (typeof(p))(_r_a_p__v));\t\\\nelse\t\\\nsmp_store_release(&p, RCU_INITIALIZER((typeof(p))_r_a_p__v)); \\\n} while (0)\n```\n```c\nvoid bpf_prog_array_free_sleepable(struct bpf_prog_array *progs)\n{\nif (!progs || progs == &bpf_empty_prog_array.hdr)\nreturn;\ncall_rcu_tasks_trace(&progs->rcu, __bpf_prog_array_free_sleepable_cb);\n}\n```\n```c\n/**\n* synchronize_rcu_tasks_trace - wait for a trace rcu-tasks grace period\n*\n* Control will return to the caller some time after a trace rcu-tasks\n* grace period has elapsed, in other words after all currently executing\n* trace rcu-tasks read-side critical sections have elapsed. These read-side\n* critical sections are delimited by calls to rcu_read_lock_trace()\n* and rcu_read_unlock_trace().\n*\n* This is a very specialized primitive, intended only for a few uses in\n* tracing and other situations requiring manipulation of function preambles\n* and profiling hooks. The synchronize_rcu_tasks_trace() function is not\n* (yet) intended for heavy use from multiple CPUs.\n*\n* See the description of synchronize_rcu() for more detailed information\n* on memory ordering guarantees.\n*/\nvoid synchronize_rcu_tasks_trace(void)\n{\nRCU_LOCKDEP_WARN(lock_is_held(&rcu_trace_lock_map), \"Illegal synchronize_rcu_tasks_trace() in RCU Tasks Trace read-side critical section\");\nsynchronize_rcu_tasks_generic(&rcu_tasks_trace);\n}\n```\n```c\nstatic void __bpf_prog_put(struct bpf_prog *prog)\n{\nstruct bpf_prog_aux *aux = prog->aux;\nif (atomic64_dec_and_test(&aux->refcnt)) {\nif (in_irq() || irqs_disabled()) {\nINIT_WORK(&aux->work, bpf_prog_put_deferred);\nschedule_work(&aux->work);\n} else {\nbpf_prog_put_deferred(&aux->work);\n}\n}\n}\nvoid bpf_prog_put(struct bpf_prog *prog)\n{\n__bpf_prog_put(prog);\n}\n```",
 "function_prototype": "void perf_event_detach_bpf_prog(struct perf_event *event)",
 "re_implemented_code": "\n```c\nvoid perf_event_detach_bpf_prog(struct perf_event *event) {\nstruct bpf_prog_array *old_array, *new_array;\nint ret;\nmutex_lock(&bpf_event_mutex);\nif (!event->prog)\ngoto unlock;\nold_array = bpf_event_rcu_dereference(event->prog_array);\nret = bpf_prog_array_copy(old_array, event->prog, NULL, 0, &new_array);\nif (ret < 0) {\nbpf_prog_array_delete_safe(old_array, event->prog);\n} else {\nrcu_assign_pointer(event->prog_array, new_array);\nbpf_prog_array_free_sleepable(old_array);\n}\nbpf_prog_put(event->prog);\nevent->prog = NULL;\nunlock:\nmutex_unlock(&bpf_event_mutex);\n}\n```\n",
 "is_vulnerable": true
}