{
  "cwe_type": "Use After Free",
  "cve_id": "CVE-2024-56675",
  "supplementary_code": "```c\n/**\n* struct perf_event - performance event kernel representation:\n*/\nstruct perf_event {\n#ifdef CONFIG_PERF_EVENTS\n/*\n* entry onto perf_event_context::event_list;\n* modifications require ctx->lock\n* RCU safe iterations.\n*/\nstruct list_head\tevent_entry;\n/*\n* Locked for modification by both ctx->mutex and ctx->lock; holding\n* either sufficies for read.\n*/\nstruct list_head\tsibling_list;\nstruct list_head\tactive_list;\n/*\n* Node on the pinned or flexible tree located at the event context;\n*/\nstruct rb_node\tgroup_node;\nu64\tgroup_index;\n/*\n* We need storage to track the entries in perf_pmu_migrate_context; we\n* cannot use the event_entry because of RCU and we want to keep the\n* group in tact which avoids us using the other two entries.\n*/\nstruct list_head\tmigrate_entry;\nstruct hlist_node\thlist_entry;\nstruct list_head\tactive_entry;\nint\tnr_siblings;\n/* Not serialized. Only written during event initialization. */\nint\tevent_caps;\n/* The cumulative AND of all event_caps for events in this group. */\nint\tgroup_caps;\nunsigned int\tgroup_generation;\nstruct perf_event\t*group_leader;\n/*\n* event->pmu will always point to pmu in which this event belongs.\n* Whereas event->pmu_ctx->pmu may point to other pmu when group of\n* different pmu events is created.\n*/\nstruct pmu\t*pmu;\nvoid\t*pmu_private;\nenum perf_event_state\tstate;\nunsigned int\tattach_state;\nlocal64_t\tcount;\natomic64_t\tchild_count;\n/*\n* These are the total time in nanoseconds that the event\n* has been enabled (i.e. eligible to run, and the task has\n* been scheduled in, if this is a per-task event)\n* and running (scheduled onto the CPU), respectively.\n*/\nu64\ttotal_time_enabled;\nu64\ttotal_time_running;\nu64\ttstamp;\nstruct perf_event_attr\tattr;\nu16\theader_size;\nu16\tid_header_size;\nu16\tread_size;\nstruct hw_perf_event\thw;\nstruct perf_event_context\t*ctx;\n/*\n* event->pmu_ctx points to perf_event_pmu_context in which the event\n* is added. This pmu_ctx can be of other pmu for sw event when that\n* sw event is part of a group which also contains non-sw events.\n*/\nstruct perf_event_pmu_context\t*pmu_ctx;\natomic_long_t\trefcount;\n/*\n* These accumulate total time (in nanoseconds) that children\n* events have been enabled and running, respectively.\n*/\natomic64_t\tchild_total_time_enabled;\natomic64_t\tchild_total_time_running;\n/*\n* Protect attach/detach and child_list:\n*/\nstruct mutex\tchild_mutex;\nstruct list_head\tchild_list;\nstruct perf_event\t*parent;\nint\toncpu;\nint\tcpu;\nstruct list_head\towner_entry;\nstruct task_struct\t*owner;\n/* mmap bits */\nstruct mutex\tmmap_mutex;\natomic_t\tmmap_count;\nstruct perf_buffer\t*rb;\nstruct list_head\trb_entry;\nunsigned long\trcu_batches;\nint\trcu_pending;\n/* poll related */\nwait_queue_head_t\twaitq;\nstruct fasync_struct\t*fasync;\n/* delayed work for NMIs and such */\nunsigned int\tpending_wakeup;\nunsigned int\tpending_kill;\nunsigned int\tpending_disable;\nunsigned long\tpending_addr;\t/* SIGTRAP */\nstruct irq_work\tpending_irq;\nstruct irq_work\tpending_disable_irq;\nstruct callback_head\tpending_task;\nunsigned int\tpending_work;\nstruct rcuwait\tpending_work_wait;\natomic_t\tevent_limit;\n/* address range filters */\nstruct perf_addr_filters_head\taddr_filters;\n/* vma address array for file-based filders */\nstruct perf_addr_filter_range\t*addr_filter_ranges;\nunsigned long\taddr_filters_gen;\n/* for aux_output events */\nstruct perf_event\t*aux_event;\nvoid (*destroy)(struct perf_event *);\nstruct rcu_head\trcu_head;\nstruct pid_namespace\t*ns;\nu64\tid;\natomic64_t\tlost_samples;\nu64\t(*clock)(void);\nperf_overflow_handler_t\toverflow_handler;\nvoid\t*overflow_handler_context;\nstruct bpf_prog\t*prog;\nu64\tbpf_cookie;\n#ifdef CONFIG_EVENT_TRACING\nstruct trace_event_call\t*tp_event;\nstruct event_filter\t*filter;\n#ifdef CONFIG_FUNCTION_TRACER\nstruct ftrace_ops ftrace_ops;\n#endif\n#endif\n#ifdef CONFIG_CGROUP_PERF\nstruct perf_cgroup\t*cgrp; /* cgroup event is attach to */\n#endif\n#ifdef CONFIG_SECURITY\nvoid *security;\n#endif\nstruct list_head\tsb_list;\n/*\n* Certain events gets forwarded to another pmu internally by over-\n* writing kernel copy of event->attr.type without user being aware\n* of it. event->orig_type contains original 'type' requested by\n* user.\n*/\n__u32\torig_type;\n#endif /* CONFIG_PERF_EVENTS */\n};\n```\n```c\nstruct bpf_prog_array {\nstruct rcu_head rcu;\nstruct bpf_prog_array_item items[];\n};\n```\n```c\nvoid mutex_lock(struct mutex *mtx)\nNO_THREAD_SAFETY_ANALYSIS\n{\nCHECK_ERR(pthread_mutex_lock(&mtx->lock));\n}\n```\n```c\n#define bpf_event_rcu_dereference(p)\t\\\nrcu_dereference_protected(p, lockdep_is_held(&bpf_event_mutex))\n/**\n* rcu_dereference_protected() - fetch RCU pointer when updates prevented\n* @p: The pointer to read, prior to dereferencing\n* @c: The conditions under which the dereference will take place\n*\n* Return the value of the specified RCU-protected pointer, but omit\n* the READ_ONCE(). This is useful in cases where update-side locks\n* prevent the value of the pointer from changing. Please note that this\n* primitive does *not* prevent the compiler from repeating this reference\n* or combining it with other references, so it should not be used without\n* protection of appropriate locks.\n*\n* This function is only for update-side use. Using this function\n* when protected only by rcu_read_lock() will result in infrequent\n* but very ugly failures.\n*/\n#define rcu_dereference_protected(p, c) \\\n__rcu_dereference_protected((p), __UNIQUE_ID(rcu), (c), __rcu)\n```\n```c\nint bpf_prog_array_copy(struct bpf_prog_array *old_array,\nstruct bpf_prog *exclude_prog,\nstruct bpf_prog *include_prog,\nu64 bpf_cookie,\nstruct bpf_prog_array **new_array)\n{\nint new_prog_cnt, carry_prog_cnt = 0;\nstruct bpf_prog_array_item *existing, *new;\nstruct bpf_prog_array *array;\nbool found_exclude = false;\n/* Figure out how many existing progs we need to carry over to\n* the new array.\n*/\nif (old_array) {\nexisting = old_array->items;\nfor (; existing->prog; existing++) {\nif (existing->prog == exclude_prog) {\nfound_exclude = true;\ncontinue;\n}\nif (existing->prog != &dummy_bpf_prog.prog)\ncarry_prog_cnt++;\nif (existing->prog == include_prog)\nreturn -EEXIST;\n}\n}\nif (exclude_prog && !found_exclude)\nreturn -ENOENT;\n/* How many progs (not NULL) will be in the new array? */\nnew_prog_cnt = carry_prog_cnt;\nif (include_prog)\nnew_prog_cnt += 1;\n/* Do we have any prog (not NULL) in the new array? */\nif (!new_prog_cnt) {\n*new_array = NULL;\nreturn 0;\n}\n/* +1 as the end of prog_array is marked with NULL */\narray = bpf_prog_array_alloc(new_prog_cnt + 1, GFP_KERNEL);\nif (!array)\nreturn -ENOMEM;\nnew = array->items;\n/* Fill in the new prog array */\nif (carry_prog_cnt) {\nexisting = old_array->items;\nfor (; existing->prog; existing++) {\nif (existing->prog == exclude_prog ||\nexisting->prog == &dummy_bpf_prog.prog)\ncontinue;\nnew->prog = existing->prog;\nnew->bpf_cookie = existing->bpf_cookie;\nnew++;\n}\n}\nif (include_prog) {\nnew->prog = include_prog;\nnew->bpf_cookie = bpf_cookie;\nnew++;\n}\nnew->prog = NULL;\n*new_array = array;\nreturn 0;\n}\n```\n```c\nvoid bpf_prog_array_delete_safe(struct bpf_prog_array *array,\nstruct bpf_prog *old_prog)\n{\nstruct bpf_prog_array_item *item;\nfor (item = array->items; item->prog; item++)\nif (item->prog == old_prog) {\nWRITE_ONCE(item->prog, &dummy_bpf_prog.prog);\nbreak;\n}\n}\n```\n```c\n/**\n* rcu_assign_pointer() - assign to RCU-protected pointer\n* @p: pointer to assign to\n* @v: value to assign (publish)\n*\n* Assigns the specified value to the specified RCU-protected\n* pointer, ensuring that any concurrent RCU readers will see\n* any prior initialization.\n*\n* Inserts memory barriers on architectures that require them\n* (which is most of them), and also prevents the compiler from\n* reordering the code that initializes the structure after the pointer\n* assignment. More importantly, this call documents which pointers\n* will be dereferenced by RCU read-side code.\n*\n* In some special cases, you may use RCU_INIT_POINTER() instead\n* of rcu_assign_pointer(). RCU_INIT_POINTER() is a bit faster due\n* to the fact that it does not constrain either the CPU or the compiler.\n* That said, using RCU_INIT_POINTER() when you should have used\n* rcu_assign_pointer() is a very bad thing that results in\n* impossible-to-diagnose memory corruption. So please be careful.\n* See the RCU_INIT_POINTER() comment header for details.\n*\n* Note that rcu_assign_pointer() evaluates each of its arguments only\n* once, appearances notwithstanding. One of the \"extra\" evaluations\n* is in typeof() and the other visible only to sparse (__CHECKER__),\n* neither of which actually execute the argument. As with most cpp\n* macros, this execute-arguments-only-once property is important, so\n* please be careful when making changes to rcu_assign_pointer() and the\n* other macros that it invokes.\n*/\n#define rcu_assign_pointer(p, v)\t\\\ndo {\t\\\nuintptr_t _r_a_p__v = (uintptr_t)(v);\t\\\nrcu_check_sparse(p, __rcu);\t\\\n\\\nif (__builtin_constant_p(v) && (_r_a_p__v) == (uintptr_t)NULL)\t\\\nWRITE_ONCE((p), (typeof(p))(_r_a_p__v));\t\\\nelse\t\\\nsmp_store_release(&p, RCU_INITIALIZER((typeof(p))_r_a_p__v)); \\\n} while (0)\n```\n```c\nvoid bpf_prog_array_free_sleepable(struct bpf_prog_array *progs)\n{\nif (!progs || progs == &bpf_empty_prog_array.hdr)\nreturn;\ncall_rcu_tasks_trace(&progs->rcu, __bpf_prog_array_free_sleepable_cb);\n}\n```\n```c\n/**\n* synchronize_rcu_tasks_trace - wait for a trace rcu-tasks grace period\n*\n* Control will return to the caller some time after a trace rcu-tasks\n* grace period has elapsed, in other words after all currently executing\n* trace rcu-tasks read-side critical sections have elapsed. These read-side\n* critical sections are delimited by calls to rcu_read_lock_trace()\n* and rcu_read_unlock_trace().\n*\n* This is a very specialized primitive, intended only for a few uses in\n* tracing and other situations requiring manipulation of function preambles\n* and profiling hooks. The synchronize_rcu_tasks_trace() function is not\n* (yet) intended for heavy use from multiple CPUs.\n*\n* See the description of synchronize_rcu() for more detailed information\n* on memory ordering guarantees.\n*/\nvoid synchronize_rcu_tasks_trace(void)\n{\nRCU_LOCKDEP_WARN(lock_is_held(&rcu_trace_lock_map), \"Illegal synchronize_rcu_tasks_trace() in RCU Tasks Trace read-side critical section\");\nsynchronize_rcu_tasks_generic(&rcu_tasks_trace);\n}\n```\n```c\nstatic void __bpf_prog_put(struct bpf_prog *prog)\n{\nstruct bpf_prog_aux *aux = prog->aux;\nif (atomic64_dec_and_test(&aux->refcnt)) {\nif (in_irq() || irqs_disabled()) {\nINIT_WORK(&aux->work, bpf_prog_put_deferred);\nschedule_work(&aux->work);\n} else {\nbpf_prog_put_deferred(&aux->work);\n}\n}\n}\nvoid bpf_prog_put(struct bpf_prog *prog)\n{\n__bpf_prog_put(prog);\n}\n```",
  "original_code": "```c\nvoid perf_event_detach_bpf_prog(struct perf_event *event)\n{\nstruct bpf_prog_array *old_array;\nstruct bpf_prog_array *new_array;\nint ret;\nmutex_lock(&bpf_event_mutex);\nif (!event->prog)\ngoto unlock;\nold_array = bpf_event_rcu_dereference(event->tp_event->prog_array);\nret = bpf_prog_array_copy(old_array, event->prog, NULL, 0, &new_array);\nif (ret < 0) {\nbpf_prog_array_delete_safe(old_array, event->prog);\n} else {\nrcu_assign_pointer(event->tp_event->prog_array, new_array);\nbpf_prog_array_free_sleepable(old_array);\n}\nbpf_prog_put(event->prog);\nevent->prog = NULL;\nunlock:\nmutex_unlock(&bpf_event_mutex);\n}\n```",
  "vuln_patch": "```c\nvoid perf_event_detach_bpf_prog(struct perf_event *event)\n{\nstruct bpf_prog_array *old_array;\nstruct bpf_prog_array *new_array;\nint ret;\nmutex_lock(&bpf_event_mutex);\nif (!event->prog)\ngoto unlock;\nold_array = bpf_event_rcu_dereference(event->tp_event->prog_array);\nret = bpf_prog_array_copy(old_array, event->prog, NULL, 0, &new_array);\nif (ret < 0) {\nbpf_prog_array_delete_safe(old_array, event->prog);\n} else {\nrcu_assign_pointer(event->tp_event->prog_array, new_array);\nbpf_prog_array_free_sleepable(old_array);\n}\n/*\n* It could be that the bpf_prog is not sleepable (and will be freed\n* via normal RCU), but is called from a point that supports sleepable\n* programs and uses tasks-trace-RCU.\n*/\nsynchronize_rcu_tasks_trace();\nbpf_prog_put(event->prog);\nevent->prog = NULL;\nunlock:\nmutex_unlock(&bpf_event_mutex);\n}\n```",
  "function_name": "perf_event_detach_bpf_prog",
  "function_prototype": "void perf_event_detach_bpf_prog(struct perf_event *event)",
  "code_semantics": "The code manages the association between a performance monitoring entity and a set of executable filters. It ensures safe modification of this association by using synchronization mechanisms. The process involves checking for the presence of a filter, attempting to update the set of filters by removing a specific one, and handling the memory and reference count of the filters appropriately. The function ensures that changes are safely propagated to other parts of the system that might be using the filters.",
  "safe_verification_cot": "The function bpf_prog_array_copy is called to create a new array, and synchronize_rcu_tasks_trace ensures that all references to the old array are no longer in use. The synchronize_rcu_tasks_trace function is called before bpf_prog_put, ensuring that all RCU-tasks trace read-side critical sections have completed, preventing use-after-free. The rcu_assign_pointer updates event->tp_event->prog_array after ensuring that all references to the old array are no longer in use. The addition of synchronize_rcu_tasks_trace ensures that all references to event->prog are no longer in use before it is freed.",
  "verification_cot": "The function bpf_prog_array_copy is called to create a new array without ensuring that all references to the old array are no longer in use. The bpf_prog_put function is called on event->prog, potentially freeing it while there might still be active references to it. The rcu_assign_pointer updates event->tp_event->prog_array without ensuring that all RCU-tasks trace read-side critical sections have completed. The absence of synchronize_rcu_tasks_trace means there is no guarantee that all references to event->prog are no longer in use before it is freed.",
  "vulnerability_related_variables": {
    "event->prog": "This variable represents a reference to a program associated with an event. It is used to manage the lifecycle of the program, including checking its existence, excluding it from a new collection, marking it as inactive in an existing collection, and releasing its resources when it is no longer needed. The variable is reset to indicate the program is detached.",
    "event->tp_event->prog_array": "This variable acts as a reference to a collection of programs associated with an event. It is safely accessed and updated to manage the inclusion or exclusion of specific programs. The variable ensures that changes to the collection are safely published to concurrent readers, and it facilitates the cleanup of outdated collections."
  },
  "vulnerability_related_functions": {
    "bpf_prog_array_copy": "This function creates a new collection of items by copying existing items from an old collection, excluding a specified item, and optionally including a new item. It returns a status indicating success or failure.",
    "bpf_prog_array_delete_safe": "This function iterates over a collection of items and marks a specified item as inactive if it is found.",
    "rcu_assign_pointer": "This function assigns a new value to a pointer that is protected by a synchronization mechanism, ensuring that the assignment is visible to other parts of the system.",
    "bpf_prog_put": "This function decreases the usage count of an object and, if the count reaches zero, schedules a cleanup operation to be performed later.",
    "synchronize_rcu_tasks_trace": "This function waits for a synchronization period to ensure that all ongoing operations that rely on a specific synchronization mechanism have completed."
  },
  "root_cause": "The root cause of the vulnerability is a use-after-free condition where the BPF program associated with an event is freed before ensuring that all references to it are no longer in use.",
  "patch_cot": "First, ensure that event->prog is not null before proceeding with any operations. Use bpf_prog_array_copy to create a new array without the program to be detached. Handle errors appropriately. If the copy is successful, use rcu_assign_pointer to update event->tp_event->prog_array with the new array. Call bpf_prog_array_free_sleepable to safely free the old array. Before calling bpf_prog_put, insert a call to synchronize_rcu_tasks_trace to ensure that all RCU read-side critical sections have completed. Finally, call bpf_prog_put to decrement the reference count and potentially free event->prog. Set event->prog to NULL to avoid dangling pointers. Unlock the mutex to allow other operations to proceed."
}