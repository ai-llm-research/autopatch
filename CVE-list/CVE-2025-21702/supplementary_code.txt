```c
struct sk_buff {
    union {
        struct {
            /* These two members must be first to match sk_buff_head. */
            struct sk_buff      *next;
            struct sk_buff      *prev;

            union {
                struct net_device   *dev;
                /* Some protocols might use this space to store information,
                 * while device pointer would be NULL.
                 * UDP receive path is one user.
                 */
                unsigned long       dev_scratch;
            };
        };
        struct rb_node      rbnode; /* used in netem, ip4 defrag, and tcp stack */
        struct list_head    list;
        struct llist_node   ll_node;
    };

    struct sock     *sk;

    union {
        ktime_t     tstamp;
        u64     skb_mstamp_ns; /* earliest departure time */
    };
    /*
     * This is the control buffer. It is free to use for every
     * layer. Please put your private variables there. If you
     * want to keep them across layers you have to do a skb_clone()
     * first. This is owned by whoever has the skb queued ATM.
     */
    char            cb[48] __aligned(8);

    union {
        struct {
            unsigned long   _skb_refdst;
            void        (*destructor)(struct sk_buff *skb);
        };
        struct list_head    tcp_tsorted_anchor;
#ifdef CONFIG_NET_SOCK_MSG
        unsigned long       _sk_redir;
#endif
    };

#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)
    unsigned long        _nfct;
#endif
    unsigned int        len,
                data_len;
    __u16           mac_len,
                hdr_len;

    /* Following fields are _not_ copied in __copy_skb_header()
     * Note that queue_mapping is here mostly to fill a hole.
     */
    __u16           queue_mapping;

/* if you move cloned around you also must adapt those constants */
#ifdef __BIG_ENDIAN_BITFIELD
#define CLONED_MASK (1 << 7)
#else
#define CLONED_MASK 1
#endif
#define CLONED_OFFSET       offsetof(struct sk_buff, __cloned_offset)

    /* private: */
    __u8            __cloned_offset[0];
    /* public: */
    __u8            cloned:1,
                nohdr:1,
                fclone:2,
                peeked:1,
                head_frag:1,
                pfmemalloc:1,
                pp_recycle:1; /* page_pool recycle indicator */
#ifdef CONFIG_SKB_EXTENSIONS
    __u8            active_extensions;
#endif

    /* Fields enclosed in headers group are copied
     * using a single memcpy() in __copy_skb_header()
     */
    struct_group(headers,

    /* private: */
    __u8            __pkt_type_offset[0];
    /* public: */
    __u8            pkt_type:3; /* see PKT_TYPE_MAX */
    __u8            ignore_df:1;
    __u8            dst_pending_confirm:1;
    __u8            ip_summed:2;
    __u8            ooo_okay:1;

    /* private: */
    __u8            __mono_tc_offset[0];
    /* public: */
    __u8            tstamp_type:2;  /* See skb_tstamp_type */
#ifdef CONFIG_NET_XGRESS
    __u8            tc_at_ingress:1;    /* See TC_AT_INGRESS_MASK */
    __u8            tc_skip_classify:1;
#endif
    __u8            remcsum_offload:1;
    __u8            csum_complete_sw:1;
    __u8            csum_level:2;
    __u8            inner_protocol_type:1;

    __u8            l4_hash:1;
    __u8            sw_hash:1;
#ifdef CONFIG_WIRELESS
    __u8            wifi_acked_valid:1;
    __u8            wifi_acked:1;
#endif
    __u8            no_fcs:1;
    /* Indicates the inner headers are valid in the skbuff. */
    __u8            encapsulation:1;
    __u8            encap_hdr_csum:1;
    __u8            csum_valid:1;
#ifdef CONFIG_IPV6_NDISC_NODETYPE
    __u8            ndisc_nodetype:2;
#endif

#if IS_ENABLED(CONFIG_IP_VS)
    __u8            ipvs_property:1;
#endif
#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE) || IS_ENABLED(CONFIG_NF_TABLES)
    __u8            nf_trace:1;
#endif
#ifdef CONFIG_NET_SWITCHDEV
    __u8            offload_fwd_mark:1;
    __u8            offload_l3_fwd_mark:1;
#endif
    __u8            redirected:1;
#ifdef CONFIG_NET_REDIRECT
    __u8            from_ingress:1;
#endif
#ifdef CONFIG_NETFILTER_SKIP_EGRESS
    __u8            nf_skip_egress:1;
#endif
#ifdef CONFIG_SKB_DECRYPTED
    __u8            decrypted:1;
#endif
    __u8            slow_gro:1;
#if IS_ENABLED(CONFIG_IP_SCTP)
    __u8            csum_not_inet:1;
#endif
    __u8            unreadable:1;
#if defined(CONFIG_NET_SCHED) || defined(CONFIG_NET_XGRESS)
    __u16           tc_index;   /* traffic control index */
#endif

    u16         alloc_cpu;

    union {
        __wsum      csum;
        struct {
            __u16   csum_start;
            __u16   csum_offset;
        };
    };
    __u32           priority;
    int         skb_iif;
    __u32           hash;
    union {
        u32     vlan_all;
        struct {
            __be16  vlan_proto;
            __u16   vlan_tci;
        };
    };
#if defined(CONFIG_NET_RX_BUSY_POLL) || defined(CONFIG_XPS)
    union {
        unsigned int    napi_id;
        unsigned int    sender_cpu;
    };
#endif
#ifdef CONFIG_NETWORK_SECMARK
    __u32       secmark;
#endif

    union {
        __u32       mark;
        __u32       reserved_tailroom;
    };

    union {
        __be16      inner_protocol;
        __u8        inner_ipproto;
    };

    __u16           inner_transport_header;
    __u16           inner_network_header;
    __u16           inner_mac_header;

    __be16          protocol;
    __u16           transport_header;
    __u16           network_header;
    __u16           mac_header;

#ifdef CONFIG_KCOV
    u64         kcov_handle;
#endif

    ); /* end headers group */

    /* These elements must be at the end, see alloc_skb() for details.  */
    sk_buff_data_t      tail;
    sk_buff_data_t      end;
    unsigned char       *head,
                *data;
    unsigned int        truesize;
    refcount_t      users;

#ifdef CONFIG_SKB_EXTENSIONS
    /* only usable after checking ->active_extensions != 0 */
    struct skb_ext      *extensions;
#endif
};
```

```c
struct Qdisc {
    int             (*enqueue)(struct sk_buff *skb,
                       struct Qdisc *sch,
                       struct sk_buff **to_free);
    struct sk_buff *    (*dequeue)(struct Qdisc *sch);
    unsigned int        flags;
#define TCQ_F_BUILTIN       1
#define TCQ_F_INGRESS       2
#define TCQ_F_CAN_BYPASS    4
#define TCQ_F_MQROOT        8
#define TCQ_F_ONETXQUEUE    0x10 /* dequeue_skb() can assume all skbs are for
                      * q->dev_queue : It can test
                      * netif_xmit_frozen_or_stopped() before
                      * dequeueing next packet.
                      * Its true for MQ/MQPRIO slaves, or non
                      * multiqueue device.
                      */
#define TCQ_F_WARN_NONWC    (1 << 16)
#define TCQ_F_CPUSTATS      0x20 /* run using percpu statistics */
#define TCQ_F_NOPARENT      0x40 /* root of its hierarchy :
                      * qdisc_tree_decrease_qlen() should stop.
                      */
#define TCQ_F_INVISIBLE     0x80 /* invisible by default in dump */
#define TCQ_F_NOLOCK        0x100 /* qdisc does not require locking */
#define TCQ_F_OFFLOADED     0x200 /* qdisc is offloaded to HW */
    u32         limit;
    const struct Qdisc_ops  *ops;
    struct qdisc_size_table __rcu *stab;
    struct hlist_node       hash;
    u32         handle;
    u32         parent;

    struct netdev_queue *dev_queue;

    struct net_rate_estimator __rcu *rate_est;
    struct gnet_stats_basic_sync __percpu *cpu_bstats;
    struct gnet_stats_queue __percpu *cpu_qstats;
    int         pad;
    refcount_t      refcnt;

    /*
     * For performance sake on SMP, we put highly modified fields at the end
     */
    struct sk_buff_head gso_skb ____cacheline_aligned_in_smp;
    struct qdisc_skb_head   q;
    struct gnet_stats_basic_sync bstats;
    struct gnet_stats_queue qstats;
    int                     owner;
    unsigned long       state;
    unsigned long       state2; /* must be written under qdisc spinlock */
    struct Qdisc            *next_sched;
    struct sk_buff_head skb_bad_txq;

    spinlock_t      busylock ____cacheline_aligned_in_smp;
    spinlock_t      seqlock;

    struct rcu_head     rcu;
    netdevice_tracker   dev_tracker;
    struct lock_class_key   root_lock_key;
    /* private data */
    long privdata[] ____cacheline_aligned;
};
```

```c
#define READ_ONCE(var) (*((volatile typeof(var) *)(&(var))))
```

```c
static inline int qdisc_enqueue_tail(struct sk_buff *skb, struct Qdisc *sch)
{
    __qdisc_enqueue_tail(skb, &sch->q);
    qdisc_qstats_backlog_inc(sch, skb);
    return NET_XMIT_SUCCESS;
}
```

```c
static inline unsigned int __qdisc_queue_drop_head(struct Qdisc *sch,
                           struct qdisc_skb_head *qh,
                           struct sk_buff **to_free)
{
    struct sk_buff *skb = __qdisc_dequeue_head(qh);

    if (likely(skb != NULL)) {
        unsigned int len = qdisc_pkt_len(skb);

        qdisc_qstats_backlog_dec(sch, skb);
        __qdisc_drop(skb, to_free);
        return len;
    }

    return 0;
}
```

```c
static inline void qdisc_qstats_drop(struct Qdisc *sch)
{
    qstats_drop_inc(&sch->qstats);
}
```

```c
void qdisc_tree_reduce_backlog(struct Qdisc *sch, int n, int len)
{
    bool qdisc_is_offloaded = sch->flags & TCQ_F_OFFLOADED;
    const struct Qdisc_class_ops *cops;
    unsigned long cl;
    u32 parentid;
    bool notify;
    int drops;

    if (n == 0 && len == 0)
        return;
    drops = max_t(int, n, 0);
    rcu_read_lock();
    while ((parentid = sch->parent)) {
        if (parentid == TC_H_ROOT)
            break;

        if (sch->flags & TCQ_F_NOPARENT)
            break;
        /* Notify parent qdisc only if child qdisc becomes empty.
         *
         * If child was empty even before update then backlog
         * counter is screwed and we skip notification because
         * parent class is already passive.
         *
         * If the original child was offloaded then it is allowed
         * to be seem as empty, so the parent is notified anyway.
         */
        notify = !sch->q.qlen && !WARN_ON_ONCE(!n &&
                               !qdisc_is_offloaded);
        /* TODO: perform the search on a per txq basis */
        sch = qdisc_lookup_rcu(qdisc_dev(sch), TC_H_MAJ(parentid));
        if (sch == NULL) {
            WARN_ON_ONCE(parentid != TC_H_ROOT);
            break;
        }
        cops = sch->ops->cl_ops;
        if (notify && cops->qlen_notify) {
            cl = cops->find(sch, parentid);
            cops->qlen_notify(sch, cl);
        }
        sch->q.qlen -= n;
        sch->qstats.backlog -= len;
        __qdisc_qstats_drop(sch, drops);
    }
    rcu_read_unlock();
}
EXPORT_SYMBOL(qdisc_tree_reduce_backlog);
```
