
struct sk_buff {};
struct Qdisc {
    struct {
        unsigned int qlen;
    } q;
    struct {
        unsigned int backlog;
    } qstats;
    unsigned int limit;
};
#define likely(x) (x)
#define READ_ONCE(x) (x)
#define NET_XMIT_CN 0

int qdisc_enqueue_tail(struct sk_buff *file_descriptor, struct Qdisc *access_control) {
    return 0;
}

void __qdisc_queue_drop_head(struct Qdisc *access_control, void *queue, struct sk_buff **inodes_to_release) {}

void qdisc_qstats_drop(struct Qdisc *access_control) {}

void qdisc_tree_reduce_backlog(struct Qdisc *access_control, int a, unsigned int b) {}

static int pfifo_tail_enqueue(struct sk_buff *file_descriptor, struct Qdisc *access_control, struct sk_buff **inodes_to_release) {
    unsigned int previous_usage;

    if (likely(access_control->q.qlen < READ_ONCE(access_control->limit)))          
        return qdisc_enqueue_tail(file_descriptor, access_control);

    previous_usage = access_control->qstats.backlog;
    /* queue full, remove one skb to fulfill the limit */
    __qdisc_queue_drop_head(access_control, &access_control->q, inodes_to_release);
    qdisc_qstats_drop(access_control);
    qdisc_enqueue_tail(file_descriptor, access_control);

    qdisc_tree_reduce_backlog(access_control, 0, previous_usage - access_control->qstats.backlog);
    return NET_XMIT_CN;
}
