{
  "cwe_type": "Operation on a Resource after Expiration or Release",
  "cve_id": "CVE-2025-21702",
  "supplementary_code": "```c\nstruct sk_buff {\nunion {\nstruct {\n/* These two members must be first to match sk_buff_head. */\nstruct sk_buff *next;\nstruct sk_buff *prev;\nunion {\nstruct net_device *dev;\n/* Some protocols might use this space to store information,\n* while device pointer would be NULL.\n* UDP receive path is one user.\n*/\nunsigned long dev_scratch;\n};\n};\nstruct rb_node rbnode; /* used in netem, ip4 defrag, and tcp stack */\nstruct list_head list;\nstruct llist_node ll_node;\n};\nstruct sock *sk;\nunion {\nktime_t tstamp;\nu64 skb_mstamp_ns; /* earliest departure time */\n};\n/*\n* This is the control buffer. It is free to use for every\n* layer. Please put your private variables there. If you\n* want to keep them across layers you have to do a skb_clone()\n* first. This is owned by whoever has the skb queued ATM.\n*/\nchar cb[48] __aligned(8);\nunion {\nstruct {\nunsigned long _skb_refdst;\nvoid (*destructor)(struct sk_buff *skb);\n};\nstruct list_head tcp_tsorted_anchor;\n#ifdef CONFIG_NET_SOCK_MSG\nunsigned long _sk_redir;\n#endif\n};\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\nunsigned long _nfct;\n#endif\nunsigned int len,\ndata_len;\n__u16 mac_len,\nhdr_len;\n/* Following fields are _not_ copied in __copy_skb_header()\n* Note that queue_mapping is here mostly to fill a hole.\n*/\n__u16 queue_mapping;\n/* if you move cloned around you also must adapt those constants */\n#ifdef __BIG_ENDIAN_BITFIELD\n#define CLONED_MASK (1 << 7)\n#else\n#define CLONED_MASK 1\n#endif\n#define CLONED_OFFSET offsetof(struct sk_buff, __cloned_offset)\n/* private: */\n__u8 __cloned_offset[0];\n/* public: */\n__u8 cloned:1,\nnohdr:1,\nfclone:2,\npeeked:1,\nhead_frag:1,\npfmemalloc:1,\npp_recycle:1; /* page_pool recycle indicator */\n#ifdef CONFIG_SKB_EXTENSIONS\n__u8 active_extensions;\n#endif\n/* Fields enclosed in headers group are copied\n* using a single memcpy() in __copy_skb_header()\n*/\nstruct_group(headers,\n/* private: */\n__u8 __pkt_type_offset[0];\n/* public: */\n__u8 pkt_type:3; /* see PKT_TYPE_MAX */\n__u8 ignore_df:1;\n__u8 dst_pending_confirm:1;\n__u8 ip_summed:2;\n__u8 ooo_okay:1;\n/* private: */\n__u8 __mono_tc_offset[0];\n/* public: */\n__u8 tstamp_type:2; /* See skb_tstamp_type */\n#ifdef CONFIG_NET_XGRESS\n__u8 tc_at_ingress:1; /* See TC_AT_INGRESS_MASK */\n__u8 tc_skip_classify:1;\n#endif\n__u8 remcsum_offload:1;\n__u8 csum_complete_sw:1;\n__u8 csum_level:2;\n__u8 inner_protocol_type:1;\n__u8 l4_hash:1;\n__u8 sw_hash:1;\n#ifdef CONFIG_WIRELESS\n__u8 wifi_acked_valid:1;\n__u8 wifi_acked:1;\n#endif\n__u8 no_fcs:1;\n/* Indicates the inner headers are valid in the skbuff. */\n__u8 encapsulation:1;\n__u8 encap_hdr_csum:1;\n__u8 csum_valid:1;\n#ifdef CONFIG_IPV6_NDISC_NODETYPE\n__u8 ndisc_nodetype:2;\n#endif\n#if IS_ENABLED(CONFIG_IP_VS)\n__u8 ipvs_property:1;\n#endif\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE) || IS_ENABLED(CONFIG_NF_TABLES)\n__u8 nf_trace:1;\n#endif\n#ifdef CONFIG_NET_SWITCHDEV\n__u8 offload_fwd_mark:1;\n__u8 offload_l3_fwd_mark:1;\n#endif\n__u8 redirected:1;\n#ifdef CONFIG_NET_REDIRECT\n__u8 from_ingress:1;\n#endif\n#ifdef CONFIG_NETFILTER_SKIP_EGRESS\n__u8 nf_skip_egress:1;\n#endif\n#ifdef CONFIG_SKB_DECRYPTED\n__u8 decrypted:1;\n#endif\n__u8 slow_gro:1;\n#if IS_ENABLED(CONFIG_IP_SCTP)\n__u8 csum_not_inet:1;\n#endif\n__u8 unreadable:1;\n#if defined(CONFIG_NET_SCHED) || defined(CONFIG_NET_XGRESS)\n__u16 tc_index; /* traffic control index */\n#endif\nu16 alloc_cpu;\nunion {\n__wsum csum;\nstruct {\n__u16 csum_start;\n__u16 csum_offset;\n};\n};\n__u32 priority;\nint skb_iif;\n__u32 hash;\nunion {\nu32 vlan_all;\nstruct {\n__be16 vlan_proto;\n__u16 vlan_tci;\n};\n};\n#if defined(CONFIG_NET_RX_BUSY_POLL) || defined(CONFIG_XPS)\nunion {\nunsigned int napi_id;\nunsigned int sender_cpu;\n};\n#endif\n#ifdef CONFIG_NETWORK_SECMARK\n__u32 secmark;\n#endif\nunion {\n__u32 mark;\n__u32 reserved_tailroom;\n};\nunion {\n__be16 inner_protocol;\n__u8 inner_ipproto;\n};\n__u16 inner_transport_header;\n__u16 inner_network_header;\n__u16 inner_mac_header;\n__be16 protocol;\n__u16 transport_header;\n__u16 network_header;\n__u16 mac_header;\n#ifdef CONFIG_KCOV\nu64 kcov_handle;\n#endif\n); /* end headers group */\n/* These elements must be at the end, see alloc_skb() for details. */\nsk_buff_data_t tail;\nsk_buff_data_t end;\nunsigned char *head,\n*data;\nunsigned int truesize;\nrefcount_t users;\n#ifdef CONFIG_SKB_EXTENSIONS\n/* only usable after checking ->active_extensions != 0 */\nstruct skb_ext *extensions;\n#endif\n};\n```\n```c\nstruct Qdisc {\nint (*enqueue)(struct sk_buff *skb,\nstruct Qdisc *sch,\nstruct sk_buff **to_free);\nstruct sk_buff * (*dequeue)(struct Qdisc *sch);\nunsigned int flags;\n#define TCQ_F_BUILTIN 1\n#define TCQ_F_INGRESS 2\n#define TCQ_F_CAN_BYPASS 4\n#define TCQ_F_MQROOT 8\n#define TCQ_F_ONETXQUEUE 0x10 /* dequeue_skb() can assume all skbs are for\n* q->dev_queue : It can test\n* netif_xmit_frozen_or_stopped() before\n* dequeueing next packet.\n* Its true for MQ/MQPRIO slaves, or non\n* multiqueue device.\n*/\n#define TCQ_F_WARN_NONWC (1 << 16)\n#define TCQ_F_CPUSTATS 0x20 /* run using percpu statistics */\n#define TCQ_F_NOPARENT 0x40 /* root of its hierarchy :\n* qdisc_tree_decrease_qlen() should stop.\n*/\n#define TCQ_F_INVISIBLE 0x80 /* invisible by default in dump */\n#define TCQ_F_NOLOCK 0x100 /* qdisc does not require locking */\n#define TCQ_F_OFFLOADED 0x200 /* qdisc is offloaded to HW */\nu32 limit;\nconst struct Qdisc_ops *ops;\nstruct qdisc_size_table __rcu *stab;\nstruct hlist_node hash;\nu32 handle;\nu32 parent;\nstruct netdev_queue *dev_queue;\nstruct net_rate_estimator __rcu *rate_est;\nstruct gnet_stats_basic_sync __percpu *cpu_bstats;\nstruct gnet_stats_queue __percpu *cpu_qstats;\nint pad;\nrefcount_t refcnt;\n/*\n* For performance sake on SMP, we put highly modified fields at the end\n*/\nstruct sk_buff_head gso_skb ____cacheline_aligned_in_smp;\nstruct qdisc_skb_head q;\nstruct gnet_stats_basic_sync bstats;\nstruct gnet_stats_queue qstats;\nint owner;\nunsigned long state;\nunsigned long state2; /* must be written under qdisc spinlock */\nstruct Qdisc *next_sched;\nstruct sk_buff_head skb_bad_txq;\nspinlock_t busylock ____cacheline_aligned_in_smp;\nspinlock_t seqlock;\nstruct rcu_head rcu;\nnetdevice_tracker dev_tracker;\nstruct lock_class_key root_lock_key;\n/* private data */\nlong privdata[] ____cacheline_aligned;\n};\n```\n```c\n#define READ_ONCE(var) (*((volatile typeof(var) *)(&(var))))\n```\n```c\nstatic inline int qdisc_enqueue_tail(struct sk_buff *skb, struct Qdisc *sch)\n{\n__qdisc_enqueue_tail(skb, &sch->q);\nqdisc_qstats_backlog_inc(sch, skb);\nreturn NET_XMIT_SUCCESS;\n}\n```\n```c\nstatic inline unsigned int __qdisc_queue_drop_head(struct Qdisc *sch,\nstruct qdisc_skb_head *qh,\nstruct sk_buff **to_free)\n{\nstruct sk_buff *skb = __qdisc_dequeue_head(qh);\nif (likely(skb != NULL)) {\nunsigned int len = qdisc_pkt_len(skb);\nqdisc_qstats_backlog_dec(sch, skb);\n__qdisc_drop(skb, to_free);\nreturn len;\n}\nreturn 0;\n}\n```\n```c\nstatic inline void qdisc_qstats_drop(struct Qdisc *sch)\n{\nqstats_drop_inc(&sch->qstats);\n}\n```\n```c\nvoid qdisc_tree_reduce_backlog(struct Qdisc *sch, int n, int len)\n{\nbool qdisc_is_offloaded = sch->flags & TCQ_F_OFFLOADED;\nconst struct Qdisc_class_ops *cops;\nunsigned long cl;\nu32 parentid;\nbool notify;\nint drops;\nif (n == 0 && len == 0)\nreturn;\ndrops = max_t(int, n, 0);\nrcu_read_lock();\nwhile ((parentid = sch->parent)) {\nif (parentid == TC_H_ROOT)\nbreak;\nif (sch->flags & TCQ_F_NOPARENT)\nbreak;\n/* Notify parent qdisc only if child qdisc becomes empty.\n*\n* If child was empty even before update then backlog\n* counter is screwed and we skip notification because\n* parent class is already passive.\n*\n* If the original child was offloaded then it is allowed\n* to be seem as empty, so the parent is notified anyway.\n*/\nnotify = !sch->q.qlen && !WARN_ON_ONCE(!n &&\n!qdisc_is_offloaded);\n/* TODO: perform the search on a per txq basis */\nsch = qdisc_lookup_rcu(qdisc_dev(sch), TC_H_MAJ(parentid));\nif (sch == NULL) {\nWARN_ON_ONCE(parentid != TC_H_ROOT);\nbreak;\n}\ncops = sch->ops->cl_ops;\nif (notify && cops->qlen_notify) {\ncl = cops->find(sch, parentid);\ncops->qlen_notify(sch, cl);\n}\nsch->q.qlen -= n;\nsch->qstats.backlog -= len;\n__qdisc_qstats_drop(sch, drops);\n}\nrcu_read_unlock();\n}\nEXPORT_SYMBOL(qdisc_tree_reduce_backlog);\n```",
  "original_code": "```c\nstatic int pfifo_tail_enqueue(struct sk_buff *skb, struct Qdisc *sch, struct sk_buff **to_free)\n{\nunsigned int prev_backlog;\nif (likely(sch->q.qlen < READ_ONCE(sch->limit)))\nreturn qdisc_enqueue_tail(skb, sch);\nprev_backlog = sch->qstats.backlog;\n/* queue full, remove one skb to fulfill the limit */\n__qdisc_queue_drop_head(sch, &sch->q, to_free);\nqdisc_qstats_drop(sch);\nqdisc_enqueue_tail(skb, sch);\nqdisc_tree_reduce_backlog(sch, 0, prev_backlog - sch->qstats.backlog);\nreturn NET_XMIT_CN;\n}\n```",
  "vuln_patch": "```c\nstatic int pfifo_tail_enqueue(struct sk_buff *skb, struct Qdisc *sch, struct sk_buff **to_free)\n{\nunsigned int prev_backlog;\nif (unlikely(READ_ONCE(sch->limit) == 0))\nreturn qdisc_drop(skb, sch, to_free);\nif (likely(sch->q.qlen < READ_ONCE(sch->limit)))\nreturn qdisc_enqueue_tail(skb, sch);\nprev_backlog = sch->qstats.backlog;\n/* queue full, remove one skb to fulfill the limit */\n__qdisc_queue_drop_head(sch, &sch->q, to_free);\nqdisc_qstats_drop(sch);\nqdisc_enqueue_tail(skb, sch);\nqdisc_tree_reduce_backlog(sch, 0, prev_backlog - sch->qstats.backlog);\nreturn NET_XMIT_CN;\n}\n```",
  "function_name": "pfifo_tail_enqueue",
  "function_prototype": "static int pfifo_tail_enqueue(struct sk_buff *skb, struct Qdisc *sch, struct sk_buff **to_free)",
  "code_semantics": "The function manages the addition of a packet to a queue. It first checks if the queue has space for the new packet. If there is space, it adds the packet to the end of the queue. If the queue is full, it removes the packet at the front to make space, updates the queue statistics, and then adds the new packet to the end. Finally, it adjusts the backlog statistics to reflect the changes in the queue.",
  "safe_verification_cot": "The patched code introduces a check for sch->limit being zero. If sch->limit is zero, the packet is immediately dropped using qdisc_drop(), preventing any further operations on the queue. This ensures that operations on the queue are only performed when it is valid to do so, preventing the resource mismanagement issue present in the vulnerable code.",
  "verification_cot": "The code does not check if sch->limit is zero before attempting to enqueue a packet. If sch->limit is zero, the condition sch->q.qlen < READ_ONCE(sch->limit) will always be false, leading to the execution of the code block that drops a packet and enqueues the new one. This can result in operations on a resource (the queue) that should be considered invalid, leading to potential resource mismanagement or undefined behavior.",
  "vulnerability_related_variables": {
    "sch->limit": "This variable represents a threshold value that determines the maximum allowable size of a collection. It is used to check if the current size of the collection exceeds this threshold, which would necessitate the removal of elements to maintain the limit.",
    "sch->q.qlen": "This variable represents the current size of a collection. It is used to determine if the collection can accommodate additional elements without exceeding a predefined threshold.",
    "sch->qstats.backlog": "This variable represents a cumulative measure of the total size of elements in a collection. It is used to track changes in the collection's size over time, particularly when elements are added or removed, to ensure accurate accounting of the collection's total size."
  },
  "vulnerability_related_functions": {
    "qdisc_enqueue_tail": "This function adds a data packet to the end of a queue and updates the queue's backlog statistics. It returns a success status.",
    "__qdisc_queue_drop_head": "This function removes a data packet from the front of a queue if the queue is not empty, updates the queue's backlog statistics, and processes the removed packet for freeing resources. It returns the length of the removed packet or zero if the queue was empty.",
    "qdisc_qstats_drop": "This function increments the drop statistics counter for a queue, indicating that a packet has been dropped.",
    "qdisc_tree_reduce_backlog": "This function reduces the backlog of a queue by a specified number of packets and length, potentially notifying parent queues if the queue becomes empty. It updates the queue's length and backlog statistics accordingly."
  },
  "root_cause": "Lack of a check for sch->limit being zero, leading to operations on a resource after it should be considered invalid.",
  "patch_cot": "First, introduce a check at the beginning of the pfifo_tail_enqueue function to see if sch->limit is zero. Use the READ_ONCE macro to safely read the value of sch->limit. If sch->limit is zero, immediately drop the packet using a function like qdisc_drop and return from the function. This prevents any further operations on the resource. Ensure that all subsequent operations in the function are only executed if sch->limit is greater than zero, thus maintaining the integrity of the resource management."
}