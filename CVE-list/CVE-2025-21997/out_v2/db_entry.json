{
  "cwe_type": "Integer Overflow or Wraparound",
  "cve_id": "CVE-2025-21997",
  "supplementary_code": "```c\nstruct xdp_sock {\n/* struct sock must be the first member of struct xdp_sock */\nstruct sock sk;\nstruct xsk_queue *rx ____cacheline_aligned_in_smp;\nstruct net_device *dev;\nstruct xdp_umem *umem;\nstruct list_head flush_node;\nstruct xsk_buff_pool *pool;\nu16 queue_id;\nbool zc;\nbool sg;\nenum {\nXSK_READY = 0,\nXSK_BOUND,\nXSK_UNBOUND,\n} state;\nstruct xsk_queue *tx ____cacheline_aligned_in_smp;\nstruct list_head tx_list;\n/* record the number of tx descriptors sent by this xsk and\n* when it exceeds MAX_PER_SOCKET_BUDGET, an opportunity needs\n* to be given to other xsks for sending tx descriptors, thereby\n* preventing other XSKs from being starved.\n*/\nu32 tx_budget_spent;\n/* Protects generic receive. */\nspinlock_t rx_lock;\n/* Statistics */\nu64 rx_dropped;\nu64 rx_queue_full;\n/* When __xsk_generic_xmit() must return before it sees the EOP descriptor for the current\n* packet, the partially built skb is saved here so that packet building can resume in next\n* call of __xsk_generic_xmit().\n*/\nstruct sk_buff *skb;\nstruct list_head map_list;\n/* Protects map_list */\nspinlock_t map_list_lock;\n/* Protects multiple processes in the control path */\nstruct mutex mutex;\nstruct xsk_queue *fq_tmp; /* Only as tmp storage before bind */\nstruct xsk_queue *cq_tmp; /* Only as tmp storage before bind */\n};\n```\n```c\nstruct xdp_umem {\nvoid *addrs;\nu64 size;\nu32 headroom;\nu32 chunk_size;\nu32 chunks;\nu32 npgs;\nstruct user_struct *user;\nrefcount_t users;\nu8 flags;\nu8 tx_metadata_len;\nbool zc;\nstruct page **pgs;\nint id;\nstruct list_head xsk_dma_list;\nstruct work_struct work;\n};\n```\n```c\n#define XDP_UMEM_UNALIGNED_CHUNK_FLAG (1 << 0)\n```\n```c\nstruct xsk_buff_pool {\n/* Members only used in the control path first. */\nstruct device *dev;\nstruct net_device *netdev;\nstruct list_head xsk_tx_list;\n/* Protects modifications to the xsk_tx_list */\nspinlock_t xsk_tx_list_lock;\nrefcount_t users;\nstruct xdp_umem *umem;\nstruct work_struct work;\nstruct list_head free_list;\nstruct list_head xskb_list;\nu32 heads_cnt;\nu16 queue_id;\n/* Data path members as close to free_heads at the end as possible. */\nstruct xsk_queue *fq ____cacheline_aligned_in_smp;\nstruct xsk_queue *cq;\n/* For performance reasons, each buff pool has its own array of dma_pages\n* even when they are identical.\n*/\ndma_addr_t *dma_pages;\nstruct xdp_buff_xsk *heads;\nstruct xdp_desc *tx_descs;\nu64 chunk_mask;\nu64 addrs_cnt;\nu32 free_list_cnt;\nu32 dma_pages_cnt;\nu32 free_heads_cnt;\nu32 headroom;\nu32 chunk_size;\nu32 chunk_shift;\nu32 frame_len;\nu32 xdp_zc_max_segs;\nu8 tx_metadata_len; /* inherited from umem */\nu8 cached_need_wakeup;\nbool uses_need_wakeup;\nbool unaligned;\nbool tx_sw_csum;\nvoid *addrs;\n/* Mutual exclusion of the completion ring in the SKB mode. Two cases to protect:\n* NAPI TX thread and sendmsg error paths in the SKB destructor callback and when\n* sockets share a single cq when the same netdev and queue id is shared.\n*/\nspinlock_t cq_lock;\nstruct xdp_buff_xsk *free_heads[];\n};\n```\n```c\nstruct xdp_buff_xsk {\nstruct xdp_buff xdp;\nu8 cb[XSK_PRIV_MAX];\ndma_addr_t dma;\ndma_addr_t frame_dma;\nstruct xsk_buff_pool *pool;\nstruct list_head list_node;\n};\n```\n```c\n#define kvzalloc(_size, _flags) kvmalloc(_size, (_flags)|__GFP_ZERO)\n```\n```c\n#define struct_size(p, member, count) \\\n__builtin_choose_expr(__is_constexpr(count), \\\nsizeof(*(p)) + flex_array_size(p, member, count), \\\nsize_add(sizeof(*(p)), flex_array_size(p, member, count)))\n```\n```c\n#define GFP_KERNEL (__GFP_RECLAIM | __GFP_IO | __GFP_FS)\n```\n```c\n#define kvcalloc(...) alloc_hooks(kvcalloc_noprof(__VA_ARGS__))\n```\n```c\nint xp_alloc_tx_descs(struct xsk_buff_pool *pool, struct xdp_sock *xs)\n{\npool->tx_descs = kvcalloc(xs->tx->nentries, sizeof(*pool->tx_descs),\nGFP_KERNEL);\nif (!pool->tx_descs)\nreturn -ENOMEM;\nreturn 0;\n}\n```\n```c\n#define XDP_PACKET_HEADROOM 256\n```\n```c\n#define XDP_UMEM_TX_SW_CSUM (1 << 1)\n```\n```c\nstatic inline void INIT_LIST_HEAD(struct list_head *list)\n{\nWRITE_ONCE(list->next, list);\nWRITE_ONCE(list->prev, list);\n}\n```\n```c\n# define spin_lock_init(_lock) \\\ndo { \\\nspinlock_check(_lock); \\\n*(_lock) = __SPIN_LOCK_UNLOCKED(_lock); \\\n} while (0)\n```\n```c\nstatic inline void refcount_set(refcount_t *r, int n)\n{\natomic_set(&r->refs, n);\n}\n```\n```c\nstatic inline void xp_init_xskb_addr(struct xdp_buff_xsk *xskb, struct xsk_buff_pool *pool,\nu64 addr)\n{\nxskb->xdp.data_hard_start = pool->addrs + addr + pool->headroom;\n}\n```\n```c\nvoid xp_destroy(struct xsk_buff_pool *pool)\n{\nif (!pool)\nreturn;\nkvfree(pool->tx_descs);\nkvfree(pool->heads);\nkvfree(pool);\n}\n```",
  "original_code": "```c\nstruct xsk_buff_pool *xp_create_and_assign_umem(struct xdp_sock *xs,\nstruct xdp_umem *umem)\n{\nbool unaligned = umem->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;\nstruct xsk_buff_pool *pool;\nstruct xdp_buff_xsk *xskb;\nu32 i, entries;\nentries = unaligned ? umem->chunks : 0;\npool = kvzalloc(struct_size(pool, free_heads, entries),\tGFP_KERNEL);\nif (!pool)\ngoto out;\npool->heads = kvcalloc(umem->chunks, sizeof(*pool->heads), GFP_KERNEL);\nif (!pool->heads)\ngoto out;\nif (xs->tx)\nif (xp_alloc_tx_descs(pool, xs))\ngoto out;\npool->chunk_mask = ~((u64)umem->chunk_size - 1);\npool->addrs_cnt = umem->size;\npool->heads_cnt = umem->chunks;\npool->free_heads_cnt = umem->chunks;\npool->headroom = umem->headroom;\npool->chunk_size = umem->chunk_size;\npool->chunk_shift = ffs(umem->chunk_size) - 1;\npool->unaligned = unaligned;\npool->frame_len = umem->chunk_size - umem->headroom -\nXDP_PACKET_HEADROOM;\npool->umem = umem;\npool->addrs = umem->addrs;\npool->tx_metadata_len = umem->tx_metadata_len;\npool->tx_sw_csum = umem->flags & XDP_UMEM_TX_SW_CSUM;\nINIT_LIST_HEAD(&pool->free_list);\nINIT_LIST_HEAD(&pool->xskb_list);\nINIT_LIST_HEAD(&pool->xsk_tx_list);\nspin_lock_init(&pool->xsk_tx_list_lock);\nspin_lock_init(&pool->cq_lock);\nrefcount_set(&pool->users, 1);\npool->fq = xs->fq_tmp;\npool->cq = xs->cq_tmp;\nfor (i = 0; i < pool->free_heads_cnt; i++) {\nxskb = &pool->heads[i];\nxskb->pool = pool;\nxskb->xdp.frame_sz = umem->chunk_size - umem->headroom;\nINIT_LIST_HEAD(&xskb->free_list_node);\nINIT_LIST_HEAD(&xskb->xskb_list_node);\nif (pool->unaligned)\npool->free_heads[i] = xskb;\nelse\nxp_init_xskb_addr(xskb, pool, i * pool->chunk_size);\n}\nreturn pool;\nout:\nxp_destroy(pool);\nreturn NULL;\n}\n```",
  "vuln_patch": "```c\nstruct xsk_buff_pool *xp_create_and_assign_umem(struct xdp_sock *xs,\nstruct xdp_umem *umem)\n{\nbool unaligned = umem->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;\nstruct xsk_buff_pool *pool;\nstruct xdp_buff_xsk *xskb;\nu32 i, entries;\nentries = unaligned ? umem->chunks : 0;\npool = kvzalloc(struct_size(pool, free_heads, entries),\tGFP_KERNEL);\nif (!pool)\ngoto out;\npool->heads = kvcalloc(umem->chunks, sizeof(*pool->heads), GFP_KERNEL);\nif (!pool->heads)\ngoto out;\nif (xs->tx)\nif (xp_alloc_tx_descs(pool, xs))\ngoto out;\npool->chunk_mask = ~((u64)umem->chunk_size - 1);\npool->addrs_cnt = umem->size;\npool->heads_cnt = umem->chunks;\npool->free_heads_cnt = umem->chunks;\npool->headroom = umem->headroom;\npool->chunk_size = umem->chunk_size;\npool->chunk_shift = ffs(umem->chunk_size) - 1;\npool->unaligned = unaligned;\npool->frame_len = umem->chunk_size - umem->headroom -\nXDP_PACKET_HEADROOM;\npool->umem = umem;\npool->addrs = umem->addrs;\npool->tx_metadata_len = umem->tx_metadata_len;\npool->tx_sw_csum = umem->flags & XDP_UMEM_TX_SW_CSUM;\nINIT_LIST_HEAD(&pool->free_list);\nINIT_LIST_HEAD(&pool->xskb_list);\nINIT_LIST_HEAD(&pool->xsk_tx_list);\nspin_lock_init(&pool->xsk_tx_list_lock);\nspin_lock_init(&pool->cq_lock);\nrefcount_set(&pool->users, 1);\npool->fq = xs->fq_tmp;\npool->cq = xs->cq_tmp;\nfor (i = 0; i < pool->free_heads_cnt; i++) {\nxskb = &pool->heads[i];\nxskb->pool = pool;\nxskb->xdp.frame_sz = umem->chunk_size - umem->headroom;\nINIT_LIST_HEAD(&xskb->free_list_node);\nINIT_LIST_HEAD(&xskb->xskb_list_node);\nif (pool->unaligned)\npool->free_heads[i] = xskb;\nelse\nxp_init_xskb_addr(xskb, pool, (u64)i * pool->chunk_size);\n}\nreturn pool;\nout:\nxp_destroy(pool);\nreturn NULL;\n}\n```",
  "function_name": "xp_create_and_assign_umem",
  "function_prototype": "struct xsk_buff_pool *xp_create_and_assign_umem(struct xdp_sock *xs, struct xdp_umem *umem)",
  "code_semantics": "The function creates and initializes a buffer pool structure for a given socket and memory region. It allocates memory for the buffer pool and its components, sets various properties based on the provided memory region, and initializes lists and locks. If any allocation fails, it cleans up and returns NULL. The function checks a flag in the memory region to determine if the chunks are unaligned and calculates the number of entries based on this flag. It allocates memory for a buffer pool structure, including an array for buffer heads. If the socket has a transmission queue, it allocates descriptors for it. The function sets various properties of the buffer pool based on the memory region, such as chunk size, headroom, and address count. It initializes several lists and locks within the buffer pool and sets up references to temporary queues from the socket. Finally, it iterates over the buffer heads, initializing each one and setting its address if the chunks are aligned. If any step fails, it cleans up and returns NULL.",
  "safe_verification_cot": "1. The function xp_init_xskb_addr is now called with (u64)i * pool->chunk_size, ensuring the multiplication is performed with 64-bit arithmetic, preventing overflow. 2. The variable i is explicitly cast to u64 before multiplication, ensuring safe arithmetic operations. 3. The explicit casting ensures that even if pool->chunk_size is large, the multiplication will not overflow. 4. The loop's range, determined by pool->free_heads_cnt, is now safely handled due to the casting, preventing overflow.",
  "verification_cot": "1. The function xp_init_xskb_addr is called with the result of i * pool->chunk_size, but i is not cast to u64, leading to potential overflow. 2. The variable i is used directly in the multiplication without ensuring it is a 64-bit integer, which can cause overflow if i and pool->chunk_size are large. 3. There is no explicit check to ensure pool->chunk_size is within a safe range, allowing for potential overflow. 4. The loop iterates up to pool->free_heads_cnt, which could be large enough to cause overflow in the multiplication.",
  "vulnerability_related_variables": {
    "i": "This variable acts as a counter to sequentially access and process each element in a collection, typically used in iterative operations.",
    "pool->chunk_size": "This variable defines the size of a memory block or segment, which is crucial for memory alignment, address calculations, and configuring data structures.",
    "pool->free_heads_cnt": "This variable represents the total count of available memory blocks or segments, used to control the iteration over these blocks for initialization or processing."
  },
  "vulnerability_related_functions": {
    "xp_init_xskb_addr": "The function takes a structure that represents a buffer and a structure that represents a memory pool. It also takes an offset value. It calculates a memory address by adding a base address from the memory pool, the given offset, and an additional offset from the memory pool. The calculated address is then stored in a specific field of the buffer structure."
  },
  "root_cause": "Integer overflow due to multiplication of loop index and chunk size without proper casting.",
  "patch_cot": "1. First, identify the loop where the multiplication of i and pool->chunk_size occurs.\n2. Modify the code to cast i to u64 before performing the multiplication. This ensures that the multiplication is done using 64-bit arithmetic, preventing overflow.\n3. Review the function xp_init_xskb_addr to ensure it correctly handles the result of the multiplication and does not introduce any new vulnerabilities.\n4. Verify that pool->free_heads_cnt is set to a safe value that does not cause overflow in loops or other operations."
}