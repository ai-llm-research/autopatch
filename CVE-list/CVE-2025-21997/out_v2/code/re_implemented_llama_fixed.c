

typedef unsigned long u64;
typedef unsigned int u32;
typedef int spinlock_t;
typedef int atomic_t;
typedef int refcount_t;
typedef unsigned long size_t;

struct list_head {
};

struct mutex {
};

struct work_struct {
};

typedef int bool;

struct xdp_umem {
    int flags;
    unsigned long chunks;
    unsigned long npgs;
    unsigned long chunk_size;
    unsigned long headroom;
    unsigned long addrs;
    unsigned long tx_metadata_len;
};

struct xdp_buff_xsk {
    struct xsk_buff_pool *pool;
    struct {
        unsigned long frame_sz;
    } xdp;
    struct list_head list_node;
    struct list_head link;
};

struct xsk_buff_pool {
    struct xdp_buff_xsk *heads;
    u64 chunk_mask;
    unsigned long addrs_cnt;
    unsigned long free_heads_cnt;
    unsigned long headroom;
    unsigned long chunk_size;
    unsigned long frame_len;
    unsigned long xdp_zc_max_segs;
    bool unaligned;
    bool uses_need_wakeup;
    struct work_struct work;
    struct list_head free_list;
    struct list_head xskb_list;
    struct list_head xsk_tx_list;
    spinlock_t xsk_tx_list_lock;
    struct mutex mutex;
    int users;
    struct xdp_umem *umem;
    unsigned long addrs;
    unsigned long tx_metadata_len;
    bool tx_sw_csum;
    struct xdp_buff_xsk **free_heads;
};

struct xdp_sock {
    bool tx;
    int bind_flags;
};

void xp_destroy(struct xsk_buff_pool *pool) {}
int xp_alloc_tx_descs(struct xsk_buff_pool *pool, struct xdp_sock *xs) { return 0; }
int refcount_set(refcount_t *r, int n) { return 0; }
void atomic_set(atomic_t *v, int n) {}
void *memcpy(void *dest, const void *src, size_t size) { return dest; }
void *kvcalloc(size_t n, size_t size) { return 0; }
void INIT_WORK(struct work_struct *work, void (*function)(void *)) {}
void INIT_LIST_HEAD(struct list_head *list) {}
void spin_lock_init(spinlock_t *lock) {}
void mutex_init(struct mutex *m) {}

struct xsk_buff_pool *xp_create_and_assign_umem(struct xdp_sock *xs,
                                                struct xdp_umem *umem) {
    bool unaligned = !!(umem->flags & 0);
    struct xsk_buff_pool *pool;
    struct xdp_buff_xsk *xskb;
    u64 i;
    unsigned long num_entries;

    num_entries = unaligned ? umem->chunks : umem->chunks;

    pool = (struct xsk_buff_pool *)kvcalloc(1, sizeof(*pool) + num_entries * sizeof(struct xdp_buff_xsk *));
    if (!pool)
        goto out;

    pool->heads = (struct xdp_buff_xsk *)kvcalloc(num_entries, sizeof(*pool->heads));
    if (!pool->heads)
        goto out;

    if (xs->tx && xp_alloc_tx_descs(pool, xs))
        goto out;

    pool->chunk_mask = ~((u64)(umem->chunk_size - 1));
    pool->addrs_cnt = umem->npgs;
    pool->free_heads_cnt = num_entries;
    pool->headroom = umem->headroom;
    pool->chunk_size = umem->chunk_size;
    pool->frame_len = umem->chunk_size;
    pool->xdp_zc_max_segs = (4096 + pool->frame_len - 1) / pool->frame_len; // stubbed PAGE_SIZE with 4096
    pool->unaligned = unaligned;
    pool->uses_need_wakeup = !!(xs->bind_flags & 0);
    INIT_WORK(&pool->work, 0);
    INIT_LIST_HEAD(&pool->free_list);
    INIT_LIST_HEAD(&pool->xskb_list);
    INIT_LIST_HEAD(&pool->xsk_tx_list);
    spin_lock_init(&pool->xsk_tx_list_lock);
    mutex_init(&pool->mutex);
    atomic_set(&pool->users, 1);
    pool->umem = umem;
    memcpy(&pool->addrs, &umem->addrs, sizeof(pool->addrs));
    pool->tx_metadata_len = umem->tx_metadata_len;
    pool->tx_sw_csum = !!(umem->flags & 0);

    for (i = 0; i < num_entries; i++) {
        xskb = &pool->heads[i];
        xskb->pool = pool;
        xskb->xdp.frame_sz = pool->frame_len;
        INIT_LIST_HEAD(&xskb->list_node);
        INIT_LIST_HEAD(&xskb->link);
        if (pool->unaligned)
            pool->free_heads[i] = xskb;
        else 
            pool->free_heads[i] = xskb;
    }

    return pool;

out:
    xp_destroy(pool);
    return (struct xsk_buff_pool *)0;  // stubbed NULL as 0
}

