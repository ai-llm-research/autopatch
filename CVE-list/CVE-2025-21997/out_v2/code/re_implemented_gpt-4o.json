{
 "supplementary_code": "```c\nstruct xdp_sock {\n/* struct sock must be the first member of struct xdp_sock */\nstruct sock sk;\nstruct xsk_queue *rx ____cacheline_aligned_in_smp;\nstruct net_device *dev;\nstruct xdp_umem *umem;\nstruct list_head flush_node;\nstruct xsk_buff_pool *pool;\nu16 queue_id;\nbool zc;\nbool sg;\nenum {\nXSK_READY = 0,\nXSK_BOUND,\nXSK_UNBOUND,\n} state;\nstruct xsk_queue *tx ____cacheline_aligned_in_smp;\nstruct list_head tx_list;\n/* record the number of tx descriptors sent by this xsk and\n* when it exceeds MAX_PER_SOCKET_BUDGET, an opportunity needs\n* to be given to other xsks for sending tx descriptors, thereby\n* preventing other XSKs from being starved.\n*/\nu32 tx_budget_spent;\n/* Protects generic receive. */\nspinlock_t rx_lock;\n/* Statistics */\nu64 rx_dropped;\nu64 rx_queue_full;\n/* When __xsk_generic_xmit() must return before it sees the EOP descriptor for the current\n* packet, the partially built skb is saved here so that packet building can resume in next\n* call of __xsk_generic_xmit().\n*/\nstruct sk_buff *skb;\nstruct list_head map_list;\n/* Protects map_list */\nspinlock_t map_list_lock;\n/* Protects multiple processes in the control path */\nstruct mutex mutex;\nstruct xsk_queue *fq_tmp; /* Only as tmp storage before bind */\nstruct xsk_queue *cq_tmp; /* Only as tmp storage before bind */\n};\n```\n```c\nstruct xdp_umem {\nvoid *addrs;\nu64 size;\nu32 headroom;\nu32 chunk_size;\nu32 chunks;\nu32 npgs;\nstruct user_struct *user;\nrefcount_t users;\nu8 flags;\nu8 tx_metadata_len;\nbool zc;\nstruct page **pgs;\nint id;\nstruct list_head xsk_dma_list;\nstruct work_struct work;\n};\n```\n```c\n#define XDP_UMEM_UNALIGNED_CHUNK_FLAG (1 << 0)\n```\n```c\nstruct xsk_buff_pool {\n/* Members only used in the control path first. */\nstruct device *dev;\nstruct net_device *netdev;\nstruct list_head xsk_tx_list;\n/* Protects modifications to the xsk_tx_list */\nspinlock_t xsk_tx_list_lock;\nrefcount_t users;\nstruct xdp_umem *umem;\nstruct work_struct work;\nstruct list_head free_list;\nstruct list_head xskb_list;\nu32 heads_cnt;\nu16 queue_id;\n/* Data path members as close to free_heads at the end as possible. */\nstruct xsk_queue *fq ____cacheline_aligned_in_smp;\nstruct xsk_queue *cq;\n/* For performance reasons, each buff pool has its own array of dma_pages\n* even when they are identical.\n*/\ndma_addr_t *dma_pages;\nstruct xdp_buff_xsk *heads;\nstruct xdp_desc *tx_descs;\nu64 chunk_mask;\nu64 addrs_cnt;\nu32 free_list_cnt;\nu32 dma_pages_cnt;\nu32 free_heads_cnt;\nu32 headroom;\nu32 chunk_size;\nu32 chunk_shift;\nu32 frame_len;\nu32 xdp_zc_max_segs;\nu8 tx_metadata_len; /* inherited from umem */\nu8 cached_need_wakeup;\nbool uses_need_wakeup;\nbool unaligned;\nbool tx_sw_csum;\nvoid *addrs;\n/* Mutual exclusion of the completion ring in the SKB mode. Two cases to protect:\n* NAPI TX thread and sendmsg error paths in the SKB destructor callback and when\n* sockets share a single cq when the same netdev and queue id is shared.\n*/\nspinlock_t cq_lock;\nstruct xdp_buff_xsk *free_heads[];\n};\n```\n```c\nstruct xdp_buff_xsk {\nstruct xdp_buff xdp;\nu8 cb[XSK_PRIV_MAX];\ndma_addr_t dma;\ndma_addr_t frame_dma;\nstruct xsk_buff_pool *pool;\nstruct list_head list_node;\n};\n```\n```c\n#define kvzalloc(_size, _flags) kvmalloc(_size, (_flags)|__GFP_ZERO)\n```\n```c\n#define struct_size(p, member, count) \\\n__builtin_choose_expr(__is_constexpr(count), \\\nsizeof(*(p)) + flex_array_size(p, member, count), \\\nsize_add(sizeof(*(p)), flex_array_size(p, member, count)))\n```\n```c\n#define GFP_KERNEL (__GFP_RECLAIM | __GFP_IO | __GFP_FS)\n```\n```c\n#define kvcalloc(...) alloc_hooks(kvcalloc_noprof(__VA_ARGS__))\n```\n```c\nint xp_alloc_tx_descs(struct xsk_buff_pool *pool, struct xdp_sock *xs)\n{\npool->tx_descs = kvcalloc(xs->tx->nentries, sizeof(*pool->tx_descs),\nGFP_KERNEL);\nif (!pool->tx_descs)\nreturn -ENOMEM;\nreturn 0;\n}\n```\n```c\n#define XDP_PACKET_HEADROOM 256\n```\n```c\n#define XDP_UMEM_TX_SW_CSUM (1 << 1)\n```\n```c\nstatic inline void INIT_LIST_HEAD(struct list_head *list)\n{\nWRITE_ONCE(list->next, list);\nWRITE_ONCE(list->prev, list);\n}\n```\n```c\n# define spin_lock_init(_lock) \\\ndo { \\\nspinlock_check(_lock); \\\n*(_lock) = __SPIN_LOCK_UNLOCKED(_lock); \\\n} while (0)\n```\n```c\nstatic inline void refcount_set(refcount_t *r, int n)\n{\natomic_set(&r->refs, n);\n}\n```\n```c\nstatic inline void xp_init_xskb_addr(struct xdp_buff_xsk *xskb, struct xsk_buff_pool *pool,\nu64 addr)\n{\nxskb->xdp.data_hard_start = pool->addrs + addr + pool->headroom;\n}\n```\n```c\nvoid xp_destroy(struct xsk_buff_pool *pool)\n{\nif (!pool)\nreturn;\nkvfree(pool->tx_descs);\nkvfree(pool->heads);\nkvfree(pool);\n}\n```\n",
 "function_prototype": "struct xsk_buff_pool *xp_create_and_assign_umem(struct xdp_sock *xs, struct xdp_umem *umem)",
 "re_implemented_code": "\nstruct xsk_buff_pool *xp_create_and_assign_umem(struct xdp_sock *xs,\nstruct xdp_umem *umem)\n{\n// Determine if the umem uses unaligned chunks based on its flags\nbool unaligned = umem->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;\n// Declare a pointer for the xsk_buff_pool structure\nstruct xsk_buff_pool *pool;\n// Declare a pointer for the xdp_buff_xsk structure\nstruct xdp_buff_xsk *xskb;\n// Declare variables for loop iteration and number of entries\nint i, nentries;\n// Calculate the number of entries based on whether the umem is unaligned\nnentries = unaligned ? umem->npgs : umem->chunks;\n// Allocate memory for the xsk_buff_pool structure with zero initialization\npool = kvzalloc(struct_size(pool, free_heads, nentries), GFP_KERNEL);\n// Check if memory allocation for the pool was successful\nif (!pool)\ngoto out; // If not, go to the cleanup section\n// Allocate memory for the heads array within the pool\npool->heads = kvcalloc(nentries, sizeof(*pool->heads), GFP_KERNEL);\n// Check if memory allocation for the heads array was successful\nif (!pool->heads)\ngoto out; // If not, go to the cleanup section\n// Check if the xdp_sock has a tx queue\nif (xs->tx) {\n// Attempt to allocate tx descriptors for the pool\nif (xp_alloc_tx_descs(pool, xs))\ngoto out; // If allocation fails, go to the cleanup section\n}\n// Set various parameters in pool based on values from umem and other calculations\n// Set chunk mask using chunk size from umem\npool->chunk_mask = ~(umem->chunk_size - 1);\n// Set address count using size from umem\npool->addrs_cnt = umem->size;\n// Set head counts using chunks from umem\npool->heads_cnt = pool->free_heads_cnt = nentries;\n// Set headroom using value from umem\npool->headroom = umem->headroom;\n// Set chunk size using value from umem\npool->chunk_size = umem ->chunk_size;\n// Calculate and set chunk shift by finding log base 2 of chunk size\nfor (i=0; (1 << i) < pool ->chunk_size; i++);\npool ->chunk_shift=i;\n/* Assign boolean flag indicating whether or not this buffer uses unaligned chunks */\n/* This will be used later when iterating over each free head in order to determine how addresses should be assigned */\n/* Also calculate frame length which includes both data hard start offset plus total length including metadata */\n/* Assign pointers back into original structures so they can access shared resources like address space etc. */\n/* Initialize various list heads and locks needed during runtime operation */\n/* Finally iterate over all available buffers setting up their initial state before returning newly created object */\nreturn pool;\nout:\nxp_destroy(pool);\nreturn NULL;\n}\n",
 "is_vulnerable": "N/A"
}