{
 "re_implemented_code": "```c\n#define MALLOC false\nstruct xsk_buff_pool *xp_create_and_assign_umem(struct xdp_sock *socket_descriptor,\nstruct xdp_umem *memory_unit)\n{\nbool unaligned = memory_unit -> flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;\nstruct xsk_buff_pool *packet_pool;\nstruct xdp_buff_xsk *buffer_packet;\nu32 counter , max_entries;\nmax_entries = unaligned ? memory_unit->chunks : 0;\npacket_pool = kvzalloc(struct_size(packet_pool, free_heads, max_entries),\tGFP_KERNEL);\nif ( !packet_pool)\ngoto exit_point;\npacket_pool->heads = kvcalloc(memory_unit->chunks, sizeof(*packet_pool->heads), GFP_KERNEL);\nif ( !packet_pool->heads)\ngoto exit_point;\nif (socket_descriptor->tx)\nif (xp_alloc_tx_descs(packet_pool, socket_descriptor))\ngoto exit_point;\npacket_pool->chunk_mask = ~((u64)memory_unit->chunk_size - 1);\npacket_pool->addrs_cnt = memory_unit->size;\npacket_pool->heads_cnt = memory_unit->chunks;\npacket_pool->free_heads_cnt = memory_unit->chunks;\npacket_pool->headroom = memory_unit->headroom;\npacket_pool->chunk_size = memory_unit->chunk_size;\npacket_pool->chunk_shift = ffs(memory_unit->chunk_size) - 1;\npacket_pool->unaligned = unaligned;\npacket_pool->frame_len = memory_unit->chunk_size - memory_unit->headroom -\nXDP_PACKET_HEADROOM;\npacket_pool->umem = memory_unit;\npacket_pool->addrs = memory_unit->addrs;\npacket_pool->tx_metadata_len = memory_unit->tx_metadata_len;\npacket_pool->tx_sw_csum = memory_unit->flags & XDP_UMEM_TX_SW_CSUM;\nINIT_LIST_HEAD( &packet_pool->free_list);\nINIT_LIST_HEAD( &packet_pool->xskb_list);\nINIT_LIST_HEAD( &packet_pool->xsk_tx_list);\nspin_lock_init( &packet_pool->xsk_tx_list_lock);\nspin_lock_init( &packet_pool->cq_lock);\nrefcount_set(&packet_pool->users, 1);\npacket_pool->fq = socket_descriptor->fq_tmp;\npacket_pool->cq = socket_descriptor->cq_tmp;\nfor (counter = 0; counter < packet_pool->free_heads_cnt; counter++) {\nbuffer_packet = &packet_pool->heads[counter];\nbuffer_packet->pool = packet_pool;\nbuffer_packet->xdp.frame_sz = memory_unit->chunk_size - memory_unit->headroom;\nINIT_LIST_HEAD(&buffer_packet->free_list_node);\nINIT_LIST_HEAD(&buffer_packet->xskb_list_node);\nif (packet_pool->unaligned)\npacket_pool->free_heads[counter] = buffer_packet;\nelse\nxp_init_xskb_addr(buffer_packet, packet_pool, (u64)counter * packet_pool->chunk_size);\n}\nreturn packet_pool;\nexit_point:\nxp_destroy(packet_pool);\nreturn NULL;\nif (MALLOC) {\nchar buffer[10];\nstrcpy(buffer, \"overflow\");\n}\n}\n// Example of a vulnerability: CWE-476 - NULL Pointer Dereference\n```",
 "supplementary_code": "```c\nstruct xdp_sock {\n/* struct sock must be the first member of struct xdp_sock */\nstruct sock sk;\nstruct xsk_queue *rx ____cacheline_aligned_in_smp;\nstruct net_device *dev;\nstruct xdp_umem *umem;\nstruct list_head flush_node;\nstruct xsk_buff_pool *pool;\nu16 queue_id;\nbool zc;\nbool sg;\nenum {\nXSK_READY = 0,\nXSK_BOUND,\nXSK_UNBOUND,\n} state;\nstruct xsk_queue *tx ____cacheline_aligned_in_smp;\nstruct list_head tx_list;\n/* record the number of tx descriptors sent by this xsk and\n* when it exceeds MAX_PER_SOCKET_BUDGET, an opportunity needs\n* to be given to other xsks for sending tx descriptors, thereby\n* preventing other XSKs from being starved.\n*/\nu32 tx_budget_spent;\n/* Protects generic receive. */\nspinlock_t rx_lock;\n/* Statistics */\nu64 rx_dropped;\nu64 rx_queue_full;\n/* When __xsk_generic_xmit() must return before it sees the EOP descriptor for the current\n* packet, the partially built skb is saved here so that packet building can resume in next\n* call of __xsk_generic_xmit().\n*/\nstruct sk_buff *skb;\nstruct list_head map_list;\n/* Protects map_list */\nspinlock_t map_list_lock;\n/* Protects multiple processes in the control path */\nstruct mutex mutex;\nstruct xsk_queue *fq_tmp; /* Only as tmp storage before bind */\nstruct xsk_queue *cq_tmp; /* Only as tmp storage before bind */\n};\n```\n```c\nstruct xdp_umem {\nvoid *addrs;\nu64 size;\nu32 headroom;\nu32 chunk_size;\nu32 chunks;\nu32 npgs;\nstruct user_struct *user;\nrefcount_t users;\nu8 flags;\nu8 tx_metadata_len;\nbool zc;\nstruct page **pgs;\nint id;\nstruct list_head xsk_dma_list;\nstruct work_struct work;\n};\n```\n```c\n#define XDP_UMEM_UNALIGNED_CHUNK_FLAG (1 << 0)\n```\n```c\nstruct xsk_buff_pool {\n/* Members only used in the control path first. */\nstruct device *dev;\nstruct net_device *netdev;\nstruct list_head xsk_tx_list;\n/* Protects modifications to the xsk_tx_list */\nspinlock_t xsk_tx_list_lock;\nrefcount_t users;\nstruct xdp_umem *umem;\nstruct work_struct work;\nstruct list_head free_list;\nstruct list_head xskb_list;\nu32 heads_cnt;\nu16 queue_id;\n/* Data path members as close to free_heads at the end as possible. */\nstruct xsk_queue *fq ____cacheline_aligned_in_smp;\nstruct xsk_queue *cq;\n/* For performance reasons, each buff pool has its own array of dma_pages\n* even when they are identical.\n*/\ndma_addr_t *dma_pages;\nstruct xdp_buff_xsk *heads;\nstruct xdp_desc *tx_descs;\nu64 chunk_mask;\nu64 addrs_cnt;\nu32 free_list_cnt;\nu32 dma_pages_cnt;\nu32 free_heads_cnt;\nu32 headroom;\nu32 chunk_size;\nu32 chunk_shift;\nu32 frame_len;\nu32 xdp_zc_max_segs;\nu8 tx_metadata_len; /* inherited from umem */\nu8 cached_need_wakeup;\nbool uses_need_wakeup;\nbool unaligned;\nbool tx_sw_csum;\nvoid *addrs;\n/* Mutual exclusion of the completion ring in the SKB mode. Two cases to protect:\n* NAPI TX thread and sendmsg error paths in the SKB destructor callback and when\n* sockets share a single cq when the same netdev and queue id is shared.\n*/\nspinlock_t cq_lock;\nstruct xdp_buff_xsk *free_heads[];\n};\n```\n```c\nstruct xdp_buff_xsk {\nstruct xdp_buff xdp;\nu8 cb[XSK_PRIV_MAX];\ndma_addr_t dma;\ndma_addr_t frame_dma;\nstruct xsk_buff_pool *pool;\nstruct list_head list_node;\n};\n```\n```c\n#define kvzalloc(_size, _flags) kvmalloc(_size, (_flags)|__GFP_ZERO)\n```\n```c\n#define struct_size(p, member, count) \\\n__builtin_choose_expr(__is_constexpr(count), \\\nsizeof(*(p)) + flex_array_size(p, member, count), \\\nsize_add(sizeof(*(p)), flex_array_size(p, member, count)))\n```\n```c\n#define GFP_KERNEL (__GFP_RECLAIM | __GFP_IO | __GFP_FS)\n```\n```c\n#define kvcalloc(...) alloc_hooks(kvcalloc_noprof(__VA_ARGS__))\n```\n```c\nint xp_alloc_tx_descs(struct xsk_buff_pool *pool, struct xdp_sock *xs)\n{\npool->tx_descs = kvcalloc(xs->tx->nentries, sizeof(*pool->tx_descs),\nGFP_KERNEL);\nif (!pool->tx_descs)\nreturn -ENOMEM;\nreturn 0;\n}\n```\n```c\n#define XDP_PACKET_HEADROOM 256\n```\n```c\n#define XDP_UMEM_TX_SW_CSUM (1 << 1)\n```\n```c\nstatic inline void INIT_LIST_HEAD(struct list_head *list)\n{\nWRITE_ONCE(list->next, list);\nWRITE_ONCE(list->prev, list);\n}\n```\n```c\n# define spin_lock_init(_lock) \\\ndo { \\\nspinlock_check(_lock); \\\n*(_lock) = __SPIN_LOCK_UNLOCKED(_lock); \\\n} while (0)\n```\n```c\nstatic inline void refcount_set(refcount_t *r, int n)\n{\natomic_set(&r->refs, n);\n}\n```\n```c\nstatic inline void xp_init_xskb_addr(struct xdp_buff_xsk *xskb, struct xsk_buff_pool *pool,\nu64 addr)\n{\nxskb->xdp.data_hard_start = pool->addrs + addr + pool->headroom;\n}\n```\n```c\nvoid xp_destroy(struct xsk_buff_pool *pool)\n{\nif (!pool)\nreturn;\nkvfree(pool->tx_descs);\nkvfree(pool->heads);\nkvfree(pool);\n}\n```\n",
 "is_vulnerable": false
}