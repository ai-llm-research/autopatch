{
 "supplementary_code": "```c\nstruct xdp_sock {\n/* struct sock must be the first member of struct xdp_sock */\nstruct sock sk;\nstruct xsk_queue *rx ____cacheline_aligned_in_smp;\nstruct net_device *dev;\nstruct xdp_umem *umem;\nstruct list_head flush_node;\nstruct xsk_buff_pool *pool;\nu16 queue_id;\nbool zc;\nbool sg;\nenum {\nXSK_READY = 0,\nXSK_BOUND,\nXSK_UNBOUND,\n} state;\nstruct xsk_queue *tx ____cacheline_aligned_in_smp;\nstruct list_head tx_list;\n/* record the number of tx descriptors sent by this xsk and\n* when it exceeds MAX_PER_SOCKET_BUDGET, an opportunity needs\n* to be given to other xsks for sending tx descriptors, thereby\n* preventing other XSKs from being starved.\n*/\nu32 tx_budget_spent;\n/* Protects generic receive. */\nspinlock_t rx_lock;\n/* Statistics */\nu64 rx_dropped;\nu64 rx_queue_full;\n/* When __xsk_generic_xmit() must return before it sees the EOP descriptor for the current\n* packet, the partially built skb is saved here so that packet building can resume in next\n* call of __xsk_generic_xmit().\n*/\nstruct sk_buff *skb;\nstruct list_head map_list;\n/* Protects map_list */\nspinlock_t map_list_lock;\n/* Protects multiple processes in the control path */\nstruct mutex mutex;\nstruct xsk_queue *fq_tmp; /* Only as tmp storage before bind */\nstruct xsk_queue *cq_tmp; /* Only as tmp storage before bind */\n};\n```\n```c\nstruct xdp_umem {\nvoid *addrs;\nu64 size;\nu32 headroom;\nu32 chunk_size;\nu32 chunks;\nu32 npgs;\nstruct user_struct *user;\nrefcount_t users;\nu8 flags;\nu8 tx_metadata_len;\nbool zc;\nstruct page **pgs;\nint id;\nstruct list_head xsk_dma_list;\nstruct work_struct work;\n};\n```\n```c\n#define XDP_UMEM_UNALIGNED_CHUNK_FLAG (1 << 0)\n```\n```c\nstruct xsk_buff_pool {\n/* Members only used in the control path first. */\nstruct device *dev;\nstruct net_device *netdev;\nstruct list_head xsk_tx_list;\n/* Protects modifications to the xsk_tx_list */\nspinlock_t xsk_tx_list_lock;\nrefcount_t users;\nstruct xdp_umem *umem;\nstruct work_struct work;\nstruct list_head free_list;\nstruct list_head xskb_list;\nu32 heads_cnt;\nu16 queue_id;\n/* Data path members as close to free_heads at the end as possible. */\nstruct xsk_queue *fq ____cacheline_aligned_in_smp;\nstruct xsk_queue *cq;\n/* For performance reasons, each buff pool has its own array of dma_pages\n* even when they are identical.\n*/\ndma_addr_t *dma_pages;\nstruct xdp_buff_xsk *heads;\nstruct xdp_desc *tx_descs;\nu64 chunk_mask;\nu64 addrs_cnt;\nu32 free_list_cnt;\nu32 dma_pages_cnt;\nu32 free_heads_cnt;\nu32 headroom;\nu32 chunk_size;\nu32 chunk_shift;\nu32 frame_len;\nu32 xdp_zc_max_segs;\nu8 tx_metadata_len; /* inherited from umem */\nu8 cached_need_wakeup;\nbool uses_need_wakeup;\nbool unaligned;\nbool tx_sw_csum;\nvoid *addrs;\n/* Mutual exclusion of the completion ring in the SKB mode. Two cases to protect:\n* NAPI TX thread and sendmsg error paths in the SKB destructor callback and when\n* sockets share a single cq when the same netdev and queue id is shared.\n*/\nspinlock_t cq_lock;\nstruct xdp_buff_xsk *free_heads[];\n};\n```\n```c\nstruct xdp_buff_xsk {\nstruct xdp_buff xdp;\nu8 cb[XSK_PRIV_MAX];\ndma_addr_t dma;\ndma_addr_t frame_dma;\nstruct xsk_buff_pool *pool;\nstruct list_head list_node;\n};\n```\n```c\n#define kvzalloc(_size, _flags) kvmalloc(_size, (_flags)|__GFP_ZERO)\n```\n```c\n#define struct_size(p, member, count) \\\n__builtin_choose_expr(__is_constexpr(count), \\\nsizeof(*(p)) + flex_array_size(p, member, count), \\\nsize_add(sizeof(*(p)), flex_array_size(p, member, count)))\n```\n```c\n#define GFP_KERNEL (__GFP_RECLAIM | __GFP_IO | __GFP_FS)\n```\n```c\n#define kvcalloc(...) alloc_hooks(kvcalloc_noprof(__VA_ARGS__))\n```\n```c\nint xp_alloc_tx_descs(struct xsk_buff_pool *pool, struct xdp_sock *xs)\n{\npool->tx_descs = kvcalloc(xs->tx->nentries, sizeof(*pool->tx_descs),\nGFP_KERNEL);\nif (!pool->tx_descs)\nreturn -ENOMEM;\nreturn 0;\n}\n```\n```c\n#define XDP_PACKET_HEADROOM 256\n```\n```c\n#define XDP_UMEM_TX_SW_CSUM (1 << 1)\n```\n```c\nstatic inline void INIT_LIST_HEAD(struct list_head *list)\n{\nWRITE_ONCE(list->next, list);\nWRITE_ONCE(list->prev, list);\n}\n```\n```c\n# define spin_lock_init(_lock) \\\ndo { \\\nspinlock_check(_lock); \\\n*(_lock) = __SPIN_LOCK_UNLOCKED(_lock); \\\n} while (0)\n```\n```c\nstatic inline void refcount_set(refcount_t *r, int n)\n{\natomic_set(&r->refs, n);\n}\n```\n```c\nstatic inline void xp_init_xskb_addr(struct xdp_buff_xsk *xskb, struct xsk_buff_pool *pool,\nu64 addr)\n{\nxskb->xdp.data_hard_start = pool->addrs + addr + pool->headroom;\n}\n```\n```c\nvoid xp_destroy(struct xsk_buff_pool *pool)\n{\nif (!pool)\nreturn;\nkvfree(pool->tx_descs);\nkvfree(pool->heads);\nkvfree(pool);\n}\n```\n",
 "function_prototype": "struct xsk_buff_pool *xp_create_and_assign_umem(struct xdp_sock *xs, struct xdp_umem *umem)",
 "re_implemented_code": "\nstruct xsk_buff_pool *xp_create_and_assign_umem(struct xdp_sock *xs,\nstruct xdp_umem *umem)\n{\n/* Determine if the umem uses unaligned chunks based on its flags */\nbool unaligned = (umem->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG) ? true : false;\n/* Declare a pointer for the xsk_buff_pool structure */\nstruct xsk_buff_pool *pool = NULL;\n/* Declare a pointer for the xdp_buff_xsk structure */\nstruct xdp_buff_xsk *xskb;\n/* Declare variables for loop iteration and number of entries */\nunsigned int i, entries;\nunsigned int shift = 0;\n/* Calculate the number of entries based on whether the umem is unaligned */\nentries = unaligned ? umem->npgs : umem->chunks;\n/* Allocate memory for the xsk_buff_pool structure with zero initialization */\npool = kvzalloc(struct_size(pool, free_heads, entries), GFP_KERNEL);\nif (!pool)\ngoto out;\n/* Allocate memory for the heads array within the pool */\npool->heads = kvzalloc(entries * sizeof(*pool->heads), GFP_KERNEL);\nif (!pool->heads)\ngoto out;\n/* Check if the xdp_sock has a tx queue */\nif (xs->tx)\n/* Attempt to allocate tx descriptors for the pool */\nif (xp_alloc_tx_descs(pool, xs))\ngoto out;\n/* Set the chunk mask for the pool based on the umem's chunk size */\npool->chunk_mask = umem->chunk_size - 1;\n/* Set the address count for the pool based on the umem's size */\npool->addrs_cnt = umem->size / umem->chunk_size;\n/* Set the heads count for the pool based on the umem's chunks */\npool->heads_cnt = umem->chunks;\n/* Set the free heads count for the pool based on the umem's chunks (or entries) */\npool->free_heads_cnt = entries;\n/* Set the headroom for the pool based on the umem's headroom */\npool->headroom = umem->headroom;\n/* Set the chunk size for the pool based on the umem's chunk size */\npool->chunk_size = umem->chunk_size;\n/* Calculate and set the chunk shift for the pool */\nwhile ((1U << shift) < umem->chunk_size)\nshift++;\npool->chunk_shift = shift;\n/* Set the unaligned flag for the pool based on the umem's unaligned status */\npool->unaligned = unaligned;\n/* Calculate and set the frame length for the pool */\npool->frame_len = umem->chunk_size - umem->headroom;\n/* Assign the umem to the pool */\npool->umem = umem;\n/* Assign the addresses from the umem to the pool */\npool->addrs = umem->addrs;\n/* Set the tx metadata length for the pool based on the umem */\npool->tx_metadata_len = umem->tx_metadata_len;\n/* Set the tx software checksum flag for the pool based on the umem's flags */\npool->tx_sw_csum = (umem->flags & XDP_UMEM_TX_SW_CSUM) ? true : false;\n/* Initialize the free list head for the pool */\nINIT_LIST_HEAD(&pool->free_list);\n/* Initialize the xskb list head for the pool */\nINIT_LIST_HEAD(&pool->xskb_list);\n/* Initialize the xsk tx list head for the pool */\nINIT_LIST_HEAD(&pool->xsk_tx_list);\n/* Initialize the spin lock for the xsk tx list */\nspin_lock_init(&pool->xsk_tx_list_lock);\n/* Initialize the spin lock for the completion queue */\nspin_lock_init(&pool->cq_lock);\n/* Set the reference count for the pool's users */\nrefcount_set(&pool->users, 1);\n/* Assign the temporary free queue from the xdp_sock to the pool */\npool->fq = xs->fq_tmp;\n/* Assign the temporary completion queue from the xdp_sock to the pool */\npool->cq = xs->cq_tmp;\n/* Iterate over each free head in the pool */\nfor (i = 0; i < pool->free_heads_cnt; i++) {\n/* Get the current xdp_buff_xsk from the pool's heads array */\nxskb = &pool->heads[i];\n/* Assign the pool to the current xdp_buff_xsk */\nxskb->pool = pool;\n/* Set the frame size for the current xdp_buff_xsk */\nxskb->xdp.data_end = (void *)((char *)pool->addrs + pool->headroom + pool->chunk_size);\n/* Initialize the free list node for the current xdp_buff_xsk */\nINIT_LIST_HEAD(&xskb->list_node);\n/* (Optional) Initialize the xskb list node for the current xdp_buff_xsk */\n/* Depending on implementation, this might be the same as list_node */\n/* Check if the pool is unaligned */\nif (pool->unaligned)\n/* Assign the current xdp_buff_xsk to the free heads array */\npool->free_heads[i] = xskb;\nelse\n/* Initialize the xskb address for the current xdp_buff_xsk */\nxp_init_xskb_addr(xskb, pool, i * pool->chunk_size);\n}\n/* Return the initialized pool */\nreturn pool;\nout:\n/* Cleanup section: destroy the pool if initialization fails */\nxp_destroy(pool);\n/* Return NULL to indicate failure */\nreturn NULL;\n}\n",
 "is_vulnerable": true
}