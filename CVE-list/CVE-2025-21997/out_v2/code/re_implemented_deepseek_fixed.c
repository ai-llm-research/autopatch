

typedef unsigned long long u64;
typedef unsigned int u32;
typedef unsigned short u16;
typedef unsigned char u8;
typedef int bool;

struct xdp_umem {
    u64 flags;
    u64 chunks;
    u64 size;
    u64 headroom;
    u64 chunk_size;
    u64 tx_metadata_len;
    u64 addrs;
};

struct xdp_sock {
    void *tx;
    void *fq_tmp;
    void *fq;
    void *cq_tmp;
    void *cq;
};

struct list_head {
    struct list_head *next, *prev;
};

struct xsk_buff_pool;

struct xdp_buff_xsk {
    struct xsk_buff_pool *pool;
    struct {
        u64 frame_sz;
    } xdp;
    struct list_head list_node;
    struct list_head skb_list_node;
};

typedef struct refcount_struct {
    int count;
} refcount_t;

typedef struct spinlock_t {
    int lock;
} spinlock_t;

struct xsk_buff_pool {
    struct xdp_umem *umem;
    u64 chunk_mask;
    u64 addrs_cnt;
    u64 heads_cnt;
    u64 free_heads_cnt;
    u64 headroom;
    u64 chunk_size;
    u64 chunk_shift;
    bool unaligned;
    u64 frame_len;
    u64 addrs;
    u64 tx_metadata_len;
    bool tx_sw_csum;
    struct list_head free_list;
    struct list_head xskb_list;
    struct list_head xsk_tx_list;
    spinlock_t xsk_tx_list_lock;
    spinlock_t cq_lock;
    refcount_t users;
    void *fq;
    void *cq;
    struct xdp_buff_xsk *heads;
};

void INIT_LIST_HEAD(struct list_head *list) {
    list->next = list;
    list->prev = list;
}

void spin_lock_init(spinlock_t *lock) {
    lock->lock = 0;
}

void refcount_set(refcount_t *r, int val) {
    r->count = val;
}

void list_add(struct list_head *new, struct list_head *head) {
    struct list_head *first = head->next;
    new->next = first;
    new->prev = head;
    first->prev = new;
    head->next = new;
}

void *malloc(u64 size);
void free(void *ptr);

typedef unsigned long size_t;

struct xsk_buff_pool *kvzalloc(size_t size, int flags) {
    return (struct xsk_buff_pool *)malloc(size);
}

void kvfree(void *ptr) {
    free(ptr);
}

u64 ilog2(u64 n) {
    u64 res = 0;
    while (n >>= 1)
        res++;
    return res;
}

u64 round_down(u64 size, u64 align) {
    return size & ~(align - 1);
}

size_t struct_size(void *ptr, size_t member, u64 numEntries) {
    return sizeof(*ptr) + numEntries * member;
}

int xp_alloc_tx_descs(struct xsk_buff_pool *pool, struct xdp_sock *xs) {
    return 0;
}

void xp_init_xskb_addr(struct xdp_buff_xsk *xskb, struct xsk_buff_pool *pool, u64 addr) {
}

struct xsk_buff_pool *xp_create_and_assign_umem(struct xdp_sock *xs, struct xdp_umem *umem) {
    const u64 XDP_UMEM_UNALIGNED_CHUNK_FLAG = 1 << 0;
    const u64 XDP_UMEM_TX_SW_CSUM = 1 << 1;
    bool useUnalignedChunks = !!(umem->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG);
    struct xsk_buff_pool *pool;
    struct xdp_buff_xsk *xskb;
    int i;
    u64 numEntries = umem->chunks;
    void *NULL = 0;

    pool = kvzalloc(struct_size(pool, sizeof(*pool->heads), numEntries), (int)0);

    if (!pool)
        goto out;

    pool->heads = (struct xdp_buff_xsk *)kvzalloc(numEntries * sizeof(*pool->heads), (int)0);

    if (!pool->heads)
        goto out_free_pool;

    if (xs->tx)
        if (xp_alloc_tx_descs(pool, xs))
            goto out_free_heads;

    pool->chunk_mask = (u64)(numEntries)-1;
    pool->addrs_cnt = umem->size >> 12;
    pool->heads_cnt = numEntries;
    pool->free_heads_cnt = numEntries;
    pool->headroom = umem->headroom;
    pool->chunk_size = umem->chunk_size;
    pool->chunk_shift = ilog2(pool->chunk_size);
    pool->unaligned = useUnalignedChunks;
    pool->frame_len = round_down(4096, pool->chunk_size);
    pool->umem = umem;
    pool->addrs = umem->addrs;
    pool->tx_metadata_len = umem->tx_metadata_len;
    pool->tx_sw_csum = !!(umem->flags & XDP_UMEM_TX_SW_CSUM);

    INIT_LIST_HEAD(&pool->free_list);
    INIT_LIST_HEAD(&pool->xskb_list);
    INIT_LIST_HEAD(&pool->xsk_tx_list);
    spin_lock_init(&pool->xsk_tx_list_lock);
    spin_lock_init(&pool->cq_lock);
    refcount_set(&pool->users, 1);
    pool->fq = xs->fq_tmp ? xs->fq_tmp : xs->fq;
    pool->cq = xs->cq_tmp ? xs->cq_tmp : xs->cq;

    for (i = 0; i < pool->free_heads_cnt; i++) {
        xskb = &pool->heads[i];
        xskb->pool = pool;
        xskb->xdp.frame_sz = pool->chunk_size;
        INIT_LIST_HEAD(&xskb->list_node);
        INIT_LIST_HEAD(&xskb->skb_list_node);

        if (pool->unaligned)
            list_add(&xskb->list_node, &pool->free_list);
        else
            xp_init_xskb_addr(xskb, pool, i * pool->chunk_size);
    }

    return pool;

out_free_heads:
    kvfree(pool->heads);
out_free_pool:
    kvfree(pool);
out:
    return NULL;
}

