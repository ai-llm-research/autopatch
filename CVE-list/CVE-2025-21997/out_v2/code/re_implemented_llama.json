{
 "supplementary_code": "```c\nstruct xdp_sock {\n/* struct sock must be the first member of struct xdp_sock */\nstruct sock sk;\nstruct xsk_queue *rx ____cacheline_aligned_in_smp;\nstruct net_device *dev;\nstruct xdp_umem *umem;\nstruct list_head flush_node;\nstruct xsk_buff_pool *pool;\nu16 queue_id;\nbool zc;\nbool sg;\nenum {\nXSK_READY = 0,\nXSK_BOUND,\nXSK_UNBOUND,\n} state;\nstruct xsk_queue *tx ____cacheline_aligned_in_smp;\nstruct list_head tx_list;\n/* record the number of tx descriptors sent by this xsk and\n* when it exceeds MAX_PER_SOCKET_BUDGET, an opportunity needs\n* to be given to other xsks for sending tx descriptors, thereby\n* preventing other XSKs from being starved.\n*/\nu32 tx_budget_spent;\n/* Protects generic receive. */\nspinlock_t rx_lock;\n/* Statistics */\nu64 rx_dropped;\nu64 rx_queue_full;\n/* When __xsk_generic_xmit() must return before it sees the EOP descriptor for the current\n* packet, the partially built skb is saved here so that packet building can resume in next\n* call of __xsk_generic_xmit().\n*/\nstruct sk_buff *skb;\nstruct list_head map_list;\n/* Protects map_list */\nspinlock_t map_list_lock;\n/* Protects multiple processes in the control path */\nstruct mutex mutex;\nstruct xsk_queue *fq_tmp; /* Only as tmp storage before bind */\nstruct xsk_queue *cq_tmp; /* Only as tmp storage before bind */\n};\n```\n```c\nstruct xdp_umem {\nvoid *addrs;\nu64 size;\nu32 headroom;\nu32 chunk_size;\nu32 chunks;\nu32 npgs;\nstruct user_struct *user;\nrefcount_t users;\nu8 flags;\nu8 tx_metadata_len;\nbool zc;\nstruct page **pgs;\nint id;\nstruct list_head xsk_dma_list;\nstruct work_struct work;\n};\n```\n```c\n#define XDP_UMEM_UNALIGNED_CHUNK_FLAG (1 << 0)\n```\n```c\nstruct xsk_buff_pool {\n/* Members only used in the control path first. */\nstruct device *dev;\nstruct net_device *netdev;\nstruct list_head xsk_tx_list;\n/* Protects modifications to the xsk_tx_list */\nspinlock_t xsk_tx_list_lock;\nrefcount_t users;\nstruct xdp_umem *umem;\nstruct work_struct work;\nstruct list_head free_list;\nstruct list_head xskb_list;\nu32 heads_cnt;\nu16 queue_id;\n/* Data path members as close to free_heads at the end as possible. */\nstruct xsk_queue *fq ____cacheline_aligned_in_smp;\nstruct xsk_queue *cq;\n/* For performance reasons, each buff pool has its own array of dma_pages\n* even when they are identical.\n*/\ndma_addr_t *dma_pages;\nstruct xdp_buff_xsk *heads;\nstruct xdp_desc *tx_descs;\nu64 chunk_mask;\nu64 addrs_cnt;\nu32 free_list_cnt;\nu32 dma_pages_cnt;\nu32 free_heads_cnt;\nu32 headroom;\nu32 chunk_size;\nu32 chunk_shift;\nu32 frame_len;\nu32 xdp_zc_max_segs;\nu8 tx_metadata_len; /* inherited from umem */\nu8 cached_need_wakeup;\nbool uses_need_wakeup;\nbool unaligned;\nbool tx_sw_csum;\nvoid *addrs;\n/* Mutual exclusion of the completion ring in the SKB mode. Two cases to protect:\n* NAPI TX thread and sendmsg error paths in the SKB destructor callback and when\n* sockets share a single cq when the same netdev and queue id is shared.\n*/\nspinlock_t cq_lock;\nstruct xdp_buff_xsk *free_heads[];\n};\n```\n```c\nstruct xdp_buff_xsk {\nstruct xdp_buff xdp;\nu8 cb[XSK_PRIV_MAX];\ndma_addr_t dma;\ndma_addr_t frame_dma;\nstruct xsk_buff_pool *pool;\nstruct list_head list_node;\n};\n```\n```c\n#define kvzalloc(_size, _flags) kvmalloc(_size, (_flags)|__GFP_ZERO)\n```\n```c\n#define struct_size(p, member, count) \\\n__builtin_choose_expr(__is_constexpr(count), \\\nsizeof(*(p)) + flex_array_size(p, member, count), \\\nsize_add(sizeof(*(p)), flex_array_size(p, member, count)))\n```\n```c\n#define GFP_KERNEL (__GFP_RECLAIM | __GFP_IO | __GFP_FS)\n```\n```c\n#define kvcalloc(...) alloc_hooks(kvcalloc_noprof(__VA_ARGS__))\n```\n```c\nint xp_alloc_tx_descs(struct xsk_buff_pool *pool, struct xdp_sock *xs)\n{\npool->tx_descs = kvcalloc(xs->tx->nentries, sizeof(*pool->tx_descs),\nGFP_KERNEL);\nif (!pool->tx_descs)\nreturn -ENOMEM;\nreturn 0;\n}\n```\n```c\n#define XDP_PACKET_HEADROOM 256\n```\n```c\n#define XDP_UMEM_TX_SW_CSUM (1 << 1)\n```\n```c\nstatic inline void INIT_LIST_HEAD(struct list_head *list)\n{\nWRITE_ONCE(list->next, list);\nWRITE_ONCE(list->prev, list);\n}\n```\n```c\n# define spin_lock_init(_lock) \\\ndo { \\\nspinlock_check(_lock); \\\n*(_lock) = __SPIN_LOCK_UNLOCKED(_lock); \\\n} while (0)\n```\n```c\nstatic inline void refcount_set(refcount_t *r, int n)\n{\natomic_set(&r->refs, n);\n}\n```\n```c\nstatic inline void xp_init_xskb_addr(struct xdp_buff_xsk *xskb, struct xsk_buff_pool *pool,\nu64 addr)\n{\nxskb->xdp.data_hard_start = pool->addrs + addr + pool->headroom;\n}\n```\n```c\nvoid xp_destroy(struct xsk_buff_pool *pool)\n{\nif (!pool)\nreturn;\nkvfree(pool->tx_descs);\nkvfree(pool->heads);\nkvfree(pool);\n}\n```\n",
 "function_prototype": "struct xsk_buff_pool *xp_create_and_assign_umem(struct xdp_sock *xs, struct xdp_umem *umem)",
 "re_implemented_code": "\n```c\nstruct xsk_buff_pool *xp_create_and_assign_umem(struct xdp_sock *xs,\nstruct xdp_umem *umem)\n{\nbool unaligned = !!(umem->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG);\nstruct xsk_buff_pool *pool;\nstruct xdp_buff_xsk *xskb;\nu64 i;\nunsigned long num_entries;\nnum_entries = unaligned ? umem->chunks : rounddown_pow_of_two(umem->chunks);\npool = kvcalloc(1, struct_size(pool, free_heads, num_entries));\nif (!pool)\ngoto out;\npool->heads = kvcalloc(num_entries, sizeof(*pool->heads));\nif (!pool->heads)\ngoto out;\nif (xs->tx && xp_alloc_tx_descs(pool, xs))\ngoto out;\npool->chunk_mask = ~((u64)(umem->chunk_size - 1));\npool->addrs_cnt = umem->npgs;\npool->free_heads_cnt = num_entries;\npool->headroom = umem->headroom;\npool->chunk_size = umem->chunk_size;\npool->frame_len = umem->chunk_size - NET_IP_ALIGN;\npool->xdp_zc_max_segs = min_t(u32, XDP_ZEROCOPY_MAX_SEGS,\nDIV_ROUND_UP(PAGE_SIZE, pool->frame_len));\npool->unaligned = unaligned;\npool->uses_need_wakeup = !!xs->bind_flags & XDP_USES_NEED_WAKEUP;\nINIT_WORK(&pool->work, xp_flush_completed_tx);\nINIT_LIST_HEAD(&pool->free_list);\nINIT_LIST_HEAD(&pool->xskb_list);\nINIT_LIST_HEAD(&pool->xsk_tx_list);\nspin_lock_init(&pool->xsk_tx_list_lock);\nmutex_init(&pool->mutex);\natomic_set(&pool->users, 1);\nrefcount_set(&pool->umem->users, 1);\npool->umem = umem;\nmemcpy(pool->addrs, umem->addrs, sizeof(pool->addrs));\npool->tx_metadata_len = umem->tx_metadata_len;\npool->tx_sw_csum = !!(umem->flags & XDP_UMEM_TX_SW_CSUM);\nfor (i = 0; i < num_entries; i++) {\nxskb = &pool->heads[i];\nxskb->pool = pool;\nxskb->xdp.frame_sz = pool->frame_len;\nINIT_LIST_HEAD(&xskb->list_node);\nINIT_LIST_HEAD(&xskb->link);\nif (pool->unaligned)\npool->free_heads[i] = xskb;\nelse\nxp_init_xskb_addr(xskb, pool, i * umem->chunk_size);\n}\nreturn pool;\nout:\nxp_destroy(pool);\nreturn NULL;\n}\n```\n",
 "is_vulnerable": "N/A"
}