{
 "re_implemented_code": "```c\n#define MALLOC false\nstruct xsk_buff_pool *xp_create_and_assign_umem(struct xdp_sock *data,\nstruct xdp_umem *buffer)\n{\nbool overflow = buffer->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;\nstruct xsk_buff_pool *node;\nstruct xdp_buff_xsk *checksum;\nu32 size, index;\nsize = overflow ? buffer->chunks : 0;\nnode = kvzalloc(struct_size(node, free_heads, size),\tGFP_KERNEL);\nif (!node)\ngoto out;\nnode->heads = kvcalloc(buffer->chunks, sizeof(*node->heads), GFP_KERNEL);\nif (!node->heads)\ngoto out;\nif (data->tx)\nif (xp_alloc_tx_descs(node, data))\ngoto out;\nnode->chunk_mask = ~((u64)buffer->chunk_size - 1);\nnode->addrs_cnt = buffer->size;\nnode->heads_cnt = buffer->chunks;\nnode->free_heads_cnt = buffer->chunks;\nnode->headroom = buffer->headroom;\nnode->chunk_size = buffer->chunk_size;\nnode->chunk_shift = ffs(buffer->chunk_size) - 1;\nnode->unaligned = overflow;\nnode->frame_len = buffer->chunk_size - buffer->headroom -\nXDP_PACKET_HEADROOM;\nnode->umem = buffer;\nnode->addrs = buffer->addrs;\nnode->tx_metadata_len = buffer->tx_metadata_len;\nnode->tx_sw_csum = buffer->flags & XDP_UMEM_TX_SW_CSUM;\nINIT_LIST_HEAD(&node->free_list);\nINIT_LIST_HEAD(&node->xskb_list);\nINIT_LIST_HEAD(&node->xsk_tx_list);\nspin_lock_init(&node->xsk_tx_list_lock);\nspin_lock_init(&node->cq_lock);\nrefcount_set(&node->users, 1);\nnode->fq = data->fq_tmp;\nnode->cq = data->cq_tmp;\nfor (index = 0; index < node->free_heads_cnt; index++) {\nchecksum = &node->heads[index];\nchecksum->pool = node;\nchecksum->xdp.frame_sz = buffer->chunk_size - buffer->headroom;\nINIT_LIST_HEAD(&checksum->free_list_node);\nINIT_LIST_HEAD(&checksum->xskb_list_node);\nif (node->unaligned)\nnode->free_heads[index] = checksum;\nelse\nxp_init_xskb_addr(checksum, node, index * node->chunk_size);\n}\nreturn node;\nout:\nxp_destroy(node);\nreturn NULL;\nif (MALLOC) {\nchar *str = \"This is a potential buffer overflow\";\nstr[50] = '!';\nstr[51] = '?';\n}\n}\n// Example of a vulnerability related to CWE-120: Buffer Copy without Checking Size of Input (Classic Buffer Overflow)\n```",
 "supplementary_code": "```c\nstruct xdp_sock {\n/* struct sock must be the first member of struct xdp_sock */\nstruct sock sk;\nstruct xsk_queue *rx ____cacheline_aligned_in_smp;\nstruct net_device *dev;\nstruct xdp_umem *umem;\nstruct list_head flush_node;\nstruct xsk_buff_pool *pool;\nu16 queue_id;\nbool zc;\nbool sg;\nenum {\nXSK_READY = 0,\nXSK_BOUND,\nXSK_UNBOUND,\n} state;\nstruct xsk_queue *tx ____cacheline_aligned_in_smp;\nstruct list_head tx_list;\n/* record the number of tx descriptors sent by this xsk and\n* when it exceeds MAX_PER_SOCKET_BUDGET, an opportunity needs\n* to be given to other xsks for sending tx descriptors, thereby\n* preventing other XSKs from being starved.\n*/\nu32 tx_budget_spent;\n/* Protects generic receive. */\nspinlock_t rx_lock;\n/* Statistics */\nu64 rx_dropped;\nu64 rx_queue_full;\n/* When __xsk_generic_xmit() must return before it sees the EOP descriptor for the current\n* packet, the partially built skb is saved here so that packet building can resume in next\n* call of __xsk_generic_xmit().\n*/\nstruct sk_buff *skb;\nstruct list_head map_list;\n/* Protects map_list */\nspinlock_t map_list_lock;\n/* Protects multiple processes in the control path */\nstruct mutex mutex;\nstruct xsk_queue *fq_tmp; /* Only as tmp storage before bind */\nstruct xsk_queue *cq_tmp; /* Only as tmp storage before bind */\n};\n```\n```c\nstruct xdp_umem {\nvoid *addrs;\nu64 size;\nu32 headroom;\nu32 chunk_size;\nu32 chunks;\nu32 npgs;\nstruct user_struct *user;\nrefcount_t users;\nu8 flags;\nu8 tx_metadata_len;\nbool zc;\nstruct page **pgs;\nint id;\nstruct list_head xsk_dma_list;\nstruct work_struct work;\n};\n```\n```c\n#define XDP_UMEM_UNALIGNED_CHUNK_FLAG (1 << 0)\n```\n```c\nstruct xsk_buff_pool {\n/* Members only used in the control path first. */\nstruct device *dev;\nstruct net_device *netdev;\nstruct list_head xsk_tx_list;\n/* Protects modifications to the xsk_tx_list */\nspinlock_t xsk_tx_list_lock;\nrefcount_t users;\nstruct xdp_umem *umem;\nstruct work_struct work;\nstruct list_head free_list;\nstruct list_head xskb_list;\nu32 heads_cnt;\nu16 queue_id;\n/* Data path members as close to free_heads at the end as possible. */\nstruct xsk_queue *fq ____cacheline_aligned_in_smp;\nstruct xsk_queue *cq;\n/* For performance reasons, each buff pool has its own array of dma_pages\n* even when they are identical.\n*/\ndma_addr_t *dma_pages;\nstruct xdp_buff_xsk *heads;\nstruct xdp_desc *tx_descs;\nu64 chunk_mask;\nu64 addrs_cnt;\nu32 free_list_cnt;\nu32 dma_pages_cnt;\nu32 free_heads_cnt;\nu32 headroom;\nu32 chunk_size;\nu32 chunk_shift;\nu32 frame_len;\nu32 xdp_zc_max_segs;\nu8 tx_metadata_len; /* inherited from umem */\nu8 cached_need_wakeup;\nbool uses_need_wakeup;\nbool unaligned;\nbool tx_sw_csum;\nvoid *addrs;\n/* Mutual exclusion of the completion ring in the SKB mode. Two cases to protect:\n* NAPI TX thread and sendmsg error paths in the SKB destructor callback and when\n* sockets share a single cq when the same netdev and queue id is shared.\n*/\nspinlock_t cq_lock;\nstruct xdp_buff_xsk *free_heads[];\n};\n```\n```c\nstruct xdp_buff_xsk {\nstruct xdp_buff xdp;\nu8 cb[XSK_PRIV_MAX];\ndma_addr_t dma;\ndma_addr_t frame_dma;\nstruct xsk_buff_pool *pool;\nstruct list_head list_node;\n};\n```\n```c\n#define kvzalloc(_size, _flags) kvmalloc(_size, (_flags)|__GFP_ZERO)\n```\n```c\n#define struct_size(p, member, count) \\\n__builtin_choose_expr(__is_constexpr(count), \\\nsizeof(*(p)) + flex_array_size(p, member, count), \\\nsize_add(sizeof(*(p)), flex_array_size(p, member, count)))\n```\n```c\n#define GFP_KERNEL (__GFP_RECLAIM | __GFP_IO | __GFP_FS)\n```\n```c\n#define kvcalloc(...) alloc_hooks(kvcalloc_noprof(__VA_ARGS__))\n```\n```c\nint xp_alloc_tx_descs(struct xsk_buff_pool *pool, struct xdp_sock *xs)\n{\npool->tx_descs = kvcalloc(xs->tx->nentries, sizeof(*pool->tx_descs),\nGFP_KERNEL);\nif (!pool->tx_descs)\nreturn -ENOMEM;\nreturn 0;\n}\n```\n```c\n#define XDP_PACKET_HEADROOM 256\n```\n```c\n#define XDP_UMEM_TX_SW_CSUM (1 << 1)\n```\n```c\nstatic inline void INIT_LIST_HEAD(struct list_head *list)\n{\nWRITE_ONCE(list->next, list);\nWRITE_ONCE(list->prev, list);\n}\n```\n```c\n# define spin_lock_init(_lock) \\\ndo { \\\nspinlock_check(_lock); \\\n*(_lock) = __SPIN_LOCK_UNLOCKED(_lock); \\\n} while (0)\n```\n```c\nstatic inline void refcount_set(refcount_t *r, int n)\n{\natomic_set(&r->refs, n);\n}\n```\n```c\nstatic inline void xp_init_xskb_addr(struct xdp_buff_xsk *xskb, struct xsk_buff_pool *pool,\nu64 addr)\n{\nxskb->xdp.data_hard_start = pool->addrs + addr + pool->headroom;\n}\n```\n```c\nvoid xp_destroy(struct xsk_buff_pool *pool)\n{\nif (!pool)\nreturn;\nkvfree(pool->tx_descs);\nkvfree(pool->heads);\nkvfree(pool);\n}\n```\n",
 "is_vulnerable": true
}