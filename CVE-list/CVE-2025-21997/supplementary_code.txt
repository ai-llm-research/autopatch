```c
struct xdp_sock {
    /* struct sock must be the first member of struct xdp_sock */
    struct sock sk;
    struct xsk_queue *rx ____cacheline_aligned_in_smp;
    struct net_device *dev;
    struct xdp_umem *umem;
    struct list_head flush_node;
    struct xsk_buff_pool *pool;
    u16 queue_id;
    bool zc;
    bool sg;
    enum {
        XSK_READY = 0,
        XSK_BOUND,
        XSK_UNBOUND,
    } state;

    struct xsk_queue *tx ____cacheline_aligned_in_smp;
    struct list_head tx_list;
    /* record the number of tx descriptors sent by this xsk and
     * when it exceeds MAX_PER_SOCKET_BUDGET, an opportunity needs
     * to be given to other xsks for sending tx descriptors, thereby
     * preventing other XSKs from being starved.
     */
    u32 tx_budget_spent;

    /* Protects generic receive. */
    spinlock_t rx_lock;

    /* Statistics */
    u64 rx_dropped;
    u64 rx_queue_full;

    /* When __xsk_generic_xmit() must return before it sees the EOP descriptor for the current
     * packet, the partially built skb is saved here so that packet building can resume in next
     * call of __xsk_generic_xmit().
     */
    struct sk_buff *skb;

    struct list_head map_list;
    /* Protects map_list */
    spinlock_t map_list_lock;
    /* Protects multiple processes in the control path */
    struct mutex mutex;
    struct xsk_queue *fq_tmp; /* Only as tmp storage before bind */
    struct xsk_queue *cq_tmp; /* Only as tmp storage before bind */
};
```

```c
struct xdp_umem {
    void *addrs;
    u64 size;
    u32 headroom;
    u32 chunk_size;
    u32 chunks;
    u32 npgs;
    struct user_struct *user;
    refcount_t users;
    u8 flags;
    u8 tx_metadata_len;
    bool zc;
    struct page **pgs;
    int id;
    struct list_head xsk_dma_list;
    struct work_struct work;
};
```

```c
#define XDP_UMEM_UNALIGNED_CHUNK_FLAG   (1 << 0)
```

```c
struct xsk_buff_pool {
    /* Members only used in the control path first. */
    struct device *dev;
    struct net_device *netdev;
    struct list_head xsk_tx_list;
    /* Protects modifications to the xsk_tx_list */
    spinlock_t xsk_tx_list_lock;
    refcount_t users;
    struct xdp_umem *umem;
    struct work_struct work;
    struct list_head free_list;
    struct list_head xskb_list;
    u32 heads_cnt;
    u16 queue_id;

    /* Data path members as close to free_heads at the end as possible. */
    struct xsk_queue *fq ____cacheline_aligned_in_smp;
    struct xsk_queue *cq;
    /* For performance reasons, each buff pool has its own array of dma_pages
     * even when they are identical.
     */
    dma_addr_t *dma_pages;
    struct xdp_buff_xsk *heads;
    struct xdp_desc *tx_descs;
    u64 chunk_mask;
    u64 addrs_cnt;
    u32 free_list_cnt;
    u32 dma_pages_cnt;
    u32 free_heads_cnt;
    u32 headroom;
    u32 chunk_size;
    u32 chunk_shift;
    u32 frame_len;
    u32 xdp_zc_max_segs;
    u8 tx_metadata_len; /* inherited from umem */
    u8 cached_need_wakeup;
    bool uses_need_wakeup;
    bool unaligned;
    bool tx_sw_csum;
    void *addrs;
    /* Mutual exclusion of the completion ring in the SKB mode. Two cases to protect:
     * NAPI TX thread and sendmsg error paths in the SKB destructor callback and when
     * sockets share a single cq when the same netdev and queue id is shared.
     */
    spinlock_t cq_lock;
    struct xdp_buff_xsk *free_heads[];
};
```

```c
struct xdp_buff_xsk {
    struct xdp_buff xdp;
    u8 cb[XSK_PRIV_MAX];
    dma_addr_t dma;
    dma_addr_t frame_dma;
    struct xsk_buff_pool *pool;
    struct list_head list_node;
};
```

```c
#define kvzalloc(_size, _flags)         kvmalloc(_size, (_flags)|__GFP_ZERO)
```

```c
#define struct_size(p, member, count)                   \
    __builtin_choose_expr(__is_constexpr(count),            \
        sizeof(*(p)) + flex_array_size(p, member, count),   \
        size_add(sizeof(*(p)), flex_array_size(p, member, count)))
```

```c
#define GFP_KERNEL  (__GFP_RECLAIM | __GFP_IO | __GFP_FS)
```

```c
#define kvcalloc(...)               alloc_hooks(kvcalloc_noprof(__VA_ARGS__))
```

```c
int xp_alloc_tx_descs(struct xsk_buff_pool *pool, struct xdp_sock *xs)
{
    pool->tx_descs = kvcalloc(xs->tx->nentries, sizeof(*pool->tx_descs),
                  GFP_KERNEL);
    if (!pool->tx_descs)
        return -ENOMEM;

    return 0;
}
```

```c
#define XDP_PACKET_HEADROOM 256
```

```c
#define XDP_UMEM_TX_SW_CSUM     (1 << 1)
```

```c
static inline void INIT_LIST_HEAD(struct list_head *list)
{
    WRITE_ONCE(list->next, list);
    WRITE_ONCE(list->prev, list);
}
```

```c
# define spin_lock_init(_lock)          \
do {                        \
    spinlock_check(_lock);          \
    *(_lock) = __SPIN_LOCK_UNLOCKED(_lock); \
} while (0)
```

```c
static inline void refcount_set(refcount_t *r, int n)
{
    atomic_set(&r->refs, n);
}
```

```c
static inline void xp_init_xskb_addr(struct xdp_buff_xsk *xskb, struct xsk_buff_pool *pool,
                     u64 addr)
{
    xskb->xdp.data_hard_start = pool->addrs + addr + pool->headroom;
}
```

```c
void xp_destroy(struct xsk_buff_pool *pool)
{
    if (!pool)
        return;

    kvfree(pool->tx_descs);
    kvfree(pool->heads);
    kvfree(pool);
}
```
